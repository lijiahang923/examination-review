

这份复习指南是基于南京理工大学自动化学院黄琦龙老师的《人工智能基础》课程课件整理而成。指南涵盖了从第一章到第十章的核心考点、关键算法和重要公式。

---

# 《人工智能基础》期末复习指南

## 第一部分：人工智能概论 (Chapter 1)

**核心概念**
*   **人工智能 (AI) 定义：** 用人工的方法在机器（计算机）上实现的智能。
*   **图灵测试 (Turing Test)：** 1950年阿兰·图灵提出，用于判定机器是否具有智能。如果测试者无法区分回答是由人还是机器做出的，则认为机器具有智能。
*   **中文屋实验 (Chinese Room)：** 约翰·塞尔提出，反驳图灵测试，论证了“语法不等于语义”，机器可能只是在操作符号而弗理解其含义。
*   **AI 的诞生：** 1956年达特茅斯会议 (Dartmouth Conference)，麦卡锡等人正式提出“Artificial Intelligence”一词。
*   **三大流派：**
    1.  **符号主义 (Symbolism)：** 基于逻辑推理（如专家系统）。
    2.  **连接主义 (Connectionism)：** 基于神经网络（如深度学习）。
    3.  **行为主义 (Behaviorism)：** 基于感知-行动（如进化计算、机器人控制）。

---

## 第二部分：知识表示 (Chapter 2)

**1. 一阶谓词逻辑**
*   **构成：** 连接词 ($\neg, \land, \lor, \rightarrow, \leftrightarrow$)，量词 ($\forall, \exists$)。
*   **表示步骤：** 定义谓词 -> 变元赋值 -> 用连接词连接。
*   **特点：** 严密性强，但效率低，难以表示不确定性知识。

**2. 产生式表示法 (Production Systems)**
*   **结构：** `IF P THEN Q` (或者 `IF P THEN Q (置信度)`)。
*   **组成：** 规则库、综合数据库（事实库）、控制系统（推理机）。
*   **优点：** 模块性好，符合人类思维，易于模块化。

**3. 框架表示法 (Frame)**
*   **结构：** 框架名 -> 槽 (Slot) -> 侧面 (Facet) -> 值。
*   **特点：** 结构性强，支持继承 (Inheritance)，适合表示对象的属性和关系。

**4. 知识图谱 (Knowledge Graph)**
*   **本质：** 互联网环境下的知识表示，本质是语义网络。
*   **基本单元：** 三元组 (实体-关系-实体) 或 (实体-属性-属性值)。
*   **架构：** 模式层（核心）+ 数据层。

---

## 第三部分：确定性推理 (Chapter 3)

**1. 推理分类**
*   **按逻辑基础：** 演绎推理（一般到个别）、归纳推理（个别到一般）、默认推理（缺省）。
*   **按方向：** 正向推理（数据驱动）、逆向推理（目标驱动）、混合推理。

**2. 鲁宾逊归结原理 (Resolution Principle)**
*   **核心思想：** 反证法。要证明 $P \rightarrow Q$，即证明 $P \land \neg Q$ 是不可满足的（永假的）。
*   **步骤：**
    1.  将前提和结论的否定转化为**子句集**（去除量词，化为合取范式）。
    2.  对子句集进行归结（消去互补文字，如 $P$ 和 $\neg P$），直到推导出**空子句 (NIL)**。
    3.  若得出 NIL，则原结论成立。

---

## 第四部分：不确定性推理 (Chapter 4) ★重点★

**1. 可信度方法 (C-F 模型)**
*   **表示：** $IF\ E\ THEN\ H\ (CF(H,E))$
    *   $CF(H,E)$: 知识的静态强度，范围 $[-1, 1]$。
*   **计算公式：**
    *   **证据的不确定性：**
        *   合取 (AND): $CF(E) = \min\{CF(E_1), CF(E_2), ...\}$
        *   析取 (OR): $CF(E) = \max\{CF(E_1), CF(E_2), ...\}$
    *   **结论的不确定性 (传递算法)：**
        *   $CF(H) = CF(H,E) \times \max\{0, CF(E)\}$
    *   **结论的合成 (多条规则推出同一结论)：**
        *   若 $CF_1, CF_2 > 0$: $CF_{1,2} = CF_1 + CF_2 - CF_1 \times CF_2$
        *   若异号: $CF_{1,2} = (CF_1 + CF_2) / (1 - \min(|CF_1|, |CF_2|))$

**2. 证据理论 (D-S 理论)**
*   **基本概念：** 识别框架 $\Theta$，概率分配函数 $M(A)$ (满足 $\sum M(A)=1, M(\Phi)=0$)。
*   **信任函数 (Bel)：** $Bel(A) = \sum_{B \subseteq A} M(B)$ (下限)。
*   **似然函数 (Pl)：** $Pl(A) = 1 - Bel(\neg A)$ (上限)。
*   **Dempster 合成规则 (正交和)：** 两个证据 $M_1$和 $M_2$ 的组合。
    *   $K = 1 - \sum_{X \cap Y = \Phi} M_1(X)M_2(Y)$ (冲突因子)
    *   $M(A) = \frac{1}{K} \sum_{X \cap Y = A} M_1(X)M_2(Y)$

**3. 模糊推理**
*   **模糊集合：** 由隶属函数 $\mu_A(x)$ 描述，范围 $[0, 1]$。
*   **运算：**
    *   交 (Intersection): $\mu_{A \cap B}(x) = \min(\mu_A(x), \mu_B(x))$
    *   并 (Union): $\mu_{A \cup B}(x) = \max(\mu_A(x), \mu_B(x))$
    *   补 (Complement): $\mu_{\neg A}(x) = 1 - \mu_A(x)$
*   **模糊关系合成：** 最大-最小合成法 (Max-Min Composition)。

---

## 第五部分：搜索求解策略 (Chapter 5)

**1. 状态空间表示**
*   四元组 $(S, O, S_0, G)$：状态集合、操作算子、初始状态、目标状态。

**2. 盲目搜索**
*   **宽度优先搜索 (BFS)：** 逐层扩展，使用队列 (Open表先进先出)，**完备且能找到最优解**，但空间复杂度高。
*   **深度优先搜索 (DFS)：** 一条路走到黑，使用栈 (Open表后进先出)，可能陷入死循环，不保证最优解。

**3. 启发式搜索**
*   **估价函数：** $f(n) = g(n) + h(n)$
    *   $g(n)$: 从初始节点到 $n$ 的实际代价。
    *   $h(n)$: 从 $n$ 到目标节点的估计代价（启发函数）。
*   **A* 算法：**
    *   条件：$h(n) \le h^*(n)$ （$h^*(n)$ 为实际最小代价）。
    *   特性：**可采纳性**（一定能找到最优解）。

---

## 第六部分：智能计算 (Chapter 6)

**1. 遗传算法 (GA)**
*   **流程：** 编码 -> 初始化种群 -> 评价适应度 -> 选择 -> 交叉 -> 变异 -> 终止。
*   **关键算子：**
    *   **选择：** 轮盘赌法 (Roulette Wheel)、锦标赛法。
    *   **交叉：** 单点交叉 (主要产生新个体)。
    *   **变异：** 小概率翻转位 (维持多样性，防早熟)。
*   **应用：** 旅行商问题 (TSP)、函数优化。

**2. 群智能算法**
*   **粒子群优化 (PSO)：** 模拟鸟群。
    *   核心公式：速度更新 $v_{id} = w v_{id} + c_1 r_1 (p_{id} - x_{id}) + c_2 r_2 (p_{gd} - x_{id})$
    *   $p_{id}$: 个体历史最优；$p_{gd}$: 全局历史最优。
*   **蚁群算法 (ACO)：** 模拟蚂蚁寻找食物。
    *   核心机制：信息素 (Pheromone) 的正反馈。
    *   要素：信息素挥发、按概率选择路径（与信息素浓度成正比）。

---

## 第七部分：专家系统 (Chapter 7)

*   **定义：** 包含知识和推理的智能计算机程序。
*   **核心组成：** **知识库** (存储知识) + **推理机** (运用知识)。
*   **与传统程序区别：** 知识与推理分离、处理符号、具有解释功能。
*   **开发工具：** 骨架系统 (如EMYCIN, KAS)。

---

## 第八部分：神经网络与深度学习 (Chapter 8) ★重点★

**1. 神经元模型 (M-P)**
*   $y = f(\sum w_i x_i - \theta)$
*   激活函数：Sigmoid, Tanh, ReLU, Step.

**2. BP 神经网络 (Back Propagation)**
*   **结构：** 多层前馈网络（输入层-隐层-输出层）。
*   **算法思想：**
    1.  **正向传播：** 信号从输入到输出。
    2.  **反向传播：** 计算误差，按梯度下降法修正权值。
*   **特点：** 逼近能力强，但易陷入局部极小值，收敛慢。

**3. Hopfield 神经网络**
*   **类型：** 反馈型网络（全连接）。
*   **特性：** 联想记忆。具有能量函数，网络稳定时能量最小（对应记忆的模式）。
*   **应用：** 优化计算 (TSP问题)、字符识别。

**4. 深度学习 (Deep Learning)**
*   **卷积神经网络 (CNN)：**
    *   核心层：卷积层 (提取特征)、池化层 (降维/下采样)、全连接层。
    *   权值共享：减少了参数数量。
    *   应用：图像识别 (LeNet, AlexNet)。
*   **生成对抗网络 (GAN)：**
    *   组成：生成器 (Generator) vs 判别器 (Discriminator)。
    *   原理：零和博弈。

---

## 第十部分：自然语言处理 (Chapter 10)

*   **层次：** 语音分析 -> 词法分析 -> 句法分析 -> 语义分析。
*   **应用：**
    *   **机器翻译：** 规则法 -> 统计法 (SMT) -> 神经机器翻译 (NMT, Seq2Seq, Attention)。
    *   **语音识别：** 隐马尔可夫模型 (HMM)。

---

# 复习建议与题型预测

1.  **计算题（必考）：**
    *   **C-F 模型计算：** 给定规则和证据的CF值，计算结论的CF值（包括单规则、多规则合成）。
    *   **D-S 证据理论：** 给定两个证据的概率分配，计算正交和 $K$ 值及组合后的信任度。
    *   **模糊关系合成：** 矩阵的 Max-Min 运算。
    *   **归结原理证明：** 将自然语言转化为谓词公式，化为子句集，用归结反演法证明。
2.  **简答题：**
    *   比较宽度优先搜索与深度优先搜索。
    *   解释 $A^*$ 算法中 $h(n)$ 的限制条件及其意义。
    *   遗传算法中交叉和变异的作用是什么？
    *   BP 算法的基本思想及缺陷。
    *   卷积神经网络中“卷积”和“池化”的作用。
3.  **综合分析/设计题：**
    *   给定一个实际问题（如TSP、八数码），设计一个遗传算法或状态空间搜索方案。
    *   画出专家系统或产生式系统的结构图。


这是一份为您精心打造的 **《人工智能基础》第一章：绪论** 的终极复习全书。

虽然第一章通常被认为是“概论”或“吹水”章节，但在期末考试中，它往往承担着**填空题**、**选择题**和**简答题**（尤其是概念辨析）的重要分值。而且，深刻理解第一章的流派和历史，能让你在学习后续具体算法时明白“为什么会有这个算法”以及“它的哲学根基是什么”。

我们将从**核心定义**、**三大流派**、**发展历史**、**研究领域**到**伦理与分类**进行全景式、无死角的深度解析。

---

# 📘 第一章：绪论 (Introduction to AI) —— 复习全书

## 第一部分：什么是人工智能？(Definitions)

人工智能（Artificial Intelligence, AI）这个概念并没有一个唯一的、被所有人接受的定义。在学术界，通常从以下四个维度来界定：

### 1. 四维定义矩阵 (Russell & Norvig)
| 维度 | **像人一样 (Humanly)** | **理性地 (Rationally)** |
| :--- | :--- | :--- |
| **思考 (Thinking)** | **像人一样思考** <br> (认知科学模型，模拟人脑思维过程) | **理性地思考** <br> (逻辑主义，基于推导法则，如三段论) |
| **行动 (Acting)** | **像人一样行动** <br> (图灵测试，模拟人类行为) | **理性地行动** <br> (Agent智能体，为了达成目标做出最优决策) |

*   **考试重点**：目前的AI主流研究更多侧重于**“理性地行动”**，即构建能够实现最佳结果的智能体（Agent）。

### 2. 图灵测试 (The Turing Test) ★必考★
由“人工智能之父”**阿兰·图灵 (Alan Turing)** 于 **1950年** 在论文《计算机器与智能》中提出。这是AI历史上最重要的概念之一。

*   **实验设置**：
    *   三个角色：询问者（人）、被询问者A（机器）、被询问者B（人）。
    *   环境：隔离，通过文本终端交流。
    *   规则：询问者向A和B提问，试图区分谁是机器，谁是人。机器的目标是**欺骗**询问者。
*   **判定标准**：如果机器能让30%以上的询问者在5分钟内误判它是人，则认为机器具有智能。
*   **意义**：给出了“智能”的一个**操作性定义**（Operational Definition），避开了“什么是思维”这种形而上学的争论，关注的是**行为表现**。
*   **反驳：中文屋实验 (The Chinese Room Argument)**
    *   **提出者**：约翰·塞尔 (John Searle)。
    *   **内容**：一个人在屋子里不懂中文，但有一本详尽的“规则书”。外面塞进中文纸条，他照着规则书拼凑出中文回复塞出去。外面的人以为屋里的人懂中文，但实际上他只懂**语法 (Syntax)**，不懂**语义 (Semantics)**。
    *   **结论**：图灵测试不足以证明机器具有“真正的理解”或“意识”。

---

## 第二部分：人工智能的三大流派 (The Three Schools) ★★★

这是本章最核心的理论考点，常考**简答题**或**分类选择题**。你必须清楚每个流派的**别名**、**核心思想**、**代表成果**。

### 1. 符号主义 (Symbolism)
*   **别名**：逻辑主义、心理学派、计算机学派。
*   **核心思想**：**物理符号系统假设**。
    *   认为智能的本质是**符号的操作和运算**。
    *   认为认知过程就是逻辑推理过程。
    *   “自顶向下”的设计思路。
*   **代表成果**：
    *   **逻辑推理**（第三章的归结原理）。
    *   **专家系统**（第七章，如MYCIN）。
    *   **知识图谱**（第二章）。
*   **缺点**：难以处理模糊、直觉和学习问题，面临“常识获取瓶颈”。

### 2. 连接主义 (Connectionism)
*   **别名**：仿生学派、生理学派。
*   **核心思想**：**神经网络**。
    *   认为智能产生于大量简单单元（神经元）的并行协作。
    *   不依赖逻辑规则，而是模仿大脑的生理结构。
    *   “自底向上”的数据驱动思路。
*   **代表成果**：
    *   **感知机 (Perceptron)**。
    *   **BP神经网络**（第八章）。
    *   **深度学习 (Deep Learning)**：CNN, RNN, Transformer。
*   **缺点**：可解释性差（黑盒模型），需要海量数据。

### 3. 行为主义 (Behaviorism)
*   **别名**：进化主义、控制论学派。
*   **核心思想**：**感知-动作 (Perception-Action)**。
    *   认为智能不需要复杂的内部表示，而是取决于“感知环境”并“做出反应”的能力。
    *   强调智能是在与环境的交互中进化出来的（适者生存）。
*   **代表成果**：
    *   **遗传算法 (GA)**（第六章）。
    *   **六足机器人**（罗德尼·布鲁克斯的包容式架构）。
    *   **强化学习**（通过奖励/惩罚来学习）。

---

## 第三部分：人工智能的发展简史 (History)

这部分主要考察**时间节点**、**关键人物**和**里程碑事件**。

### 1. 孕育期 (1956年之前)
*   **M-P模型 (1943)**：麦克洛奇和皮茨提出第一个神经元数学模型（连接主义的萌芽）。
*   **Hebb学习规则 (1949)**：解释了神经元之间连接强度的变化。

### 2. 诞生期 (1956年) ★必考★
*   **事件**：**达特茅斯会议 (Dartmouth Conference)**。
*   **人物（AI的奠基人）**：
    *   **麦卡锡 (John McCarthy)**：会议召集人，**“人工智能”一词的提出者**，LISP语言之父。
    *   **明斯基 (Marvin Minsky)**：神经网络先驱，框架理论提出者。
    *   **西蒙 (Herbert Simon) & 纽厄尔 (Allen Newell)**：开发了“逻辑理论家”程序，符号主义代表。
    *   **香农 (Claude Shannon)**：信息论之父。
*   **意义**：标志着人工智能作为一门独立学科的正式诞生。

### 3. 黄金时期 (1956-1974)
*   通用问题求解器 (GPS)。
*   几何定理证明器。
*   聊天机器人 ELIZA。
*   主要由符号主义主导，人们充满乐观，认为20年内机器能做人能做的一切。

### 4. 第一次寒冬 (1974-1980)
*   **原因**：机器翻译失败、计算能力不足、**“莫拉维克悖论”**（机器做高等数学容易，做三岁小孩的动作很难）。
*   **标志性事件**：1969年，明斯基出版《感知机》一书，从数学上证明了单层感知机无法解决**异或 (XOR)** 问题，直接导致神经网络研究停滞10年。

### 5. 繁荣期 (1980-1987)
*   **标志**：**专家系统**的商业化成功（如XCON系统）。
*   **特点**：从追求“通用智能”转向“特定领域的知识应用”。

### 6. 第二次寒冬 (1987-1993)
*   **原因**：专家系统维护成本高、知识获取困难（瓶颈）、LISP机器市场崩溃。

### 7. 稳步发展与爆发 (1993至今)
*   **1997年**：IBM **深蓝 (Deep Blue)** 战胜国际象棋冠军卡斯帕罗夫（暴力搜索+人工评估）。
*   **2006年**：Geoffrey Hinton 提出**深度学习 (Deep Learning)**，解决了多层网络的训练难题（连接主义复兴）。
*   **2011年**：IBM Watson 在智力问答节目 Jeopardy! 中夺冠。
*   **2016年**：Google **AlphaGo** 战胜围棋冠军李世石（深度学习+蒙特卡洛树搜索）。
*   **2022年**：ChatGPT 横空出世，生成式AI和大模型时代到来。

---

## 第四部分：人工智能的研究领域 (What AI does)

这部分常考**名词解释**或**举例**。

1.  **机器学习 (Machine Learning)**：AI的核心。研究计算机如何模拟人类的学习行为。
2.  **计算机视觉 (Computer Vision, CV)**：让机器“看”。图像分类、目标检测、人脸识别。
3.  **自然语言处理 (NLP)**：让机器“听/读/说”。机器翻译、语音识别、情感分析。
4.  **知识工程/专家系统**：知识的表示、获取和推理。
5.  **机器人学 (Robotics)**：感知、规划、控制。
6.  **博弈 (Game Playing)**：搜索算法的试验田（如国际象棋、围棋）。
7.  **模式识别**：文字识别、语音识别、指纹识别。

---

## 第五部分：人工智能的分类与伦理

### 1. 强AI vs 弱AI (Strong AI vs Weak AI)
*   **弱人工智能 (ANI / Narrow AI)**：
    *   专注于解决**特定领域**的问题。
    *   *现状*：我们现在所有的AI（包括AlphaGo, ChatGPT）本质上都是弱AI。
    *   *特点*：在特定任务上可能超越人类，但不具备真正的意识或通用能力。
*   **强人工智能 (AGI / General AI)**：
    *   拥有**通用**的智能，能像人一样进行推理、计划、解决问题、抽象思维、理解复杂思想、快速学习和从经验中学习。
    *   *现状*：尚未实现，是终极目标。

### 2. 人工智能的伦理挑战
*   **算法偏见**：训练数据中的歧视会导致AI歧视（如招聘筛选）。
*   **责任归属**：自动驾驶撞人，谁负责？
*   **隐私泄露**：大模型训练数据的隐私问题。
*   **就业影响**：替代重复性劳动。

---

# 📝 第一章精选试题库 (含详解)

## 题型一：填空题 (每空2分)
1.  人工智能的英文缩写是 \_\_\_\_\_\_，该词最早是在 \_\_\_\_\_\_ 年的 \_\_\_\_\_\_\_ 会议上提出的。
2.  人工智能的三大主流学派分别是 \_\_\_\_\_\_、\_\_\_\_\_\_ 和 \_\_\_\_\_\_。
3.  图灵测试的主要目的是为了判断机器是否具有 \_\_\_\_\_\_。
4.  1997年战胜国际象棋世界冠军的计算机叫 \_\_\_\_\_\_；2016年战胜围棋世界冠军的程序叫 \_\_\_\_\_\_。
5.  证明了单层感知机无法解决XOR问题，导致神经网络研究进入低谷的学者是 \_\_\_\_\_\_。

**答案**：
1.  AI, 1956, 达特茅斯 (Dartmouth)
2.  符号主义, 连接主义, 行为主义
3.  智能
4.  深蓝 (Deep Blue), AlphaGo
5.  明斯基 (Minsky)

## 题型二：名词解释 (每题5分)
**1. 图灵测试 (Turing Test)**
*   **答案要点**：由阿兰·图灵于1950年提出。测试方式是让询问者通过文本终端与被询问者（一个人和一个机器）交流。如果询问者在一定时间内无法区分哪个是机器，则认为机器通过测试，具有智能。这是一种行为主义的智能定义。

**2. 符号主义 (Symbolism)**
*   **答案要点**：又称逻辑主义。基于“物理符号系统假设”，认为智能的本质是符号的运算和推理。代表性成果包括逻辑推理、知识图谱和专家系统。

## 题型三：简答题 (每题10分)
**Q1: 简述人工智能发展过程中的三次高潮和两次低谷，并分析低谷产生的主要原因。**
*   **答案思路**：
    *   **起步与高潮 (50-60年代)**：证明定理、下棋。
    *   **第一次低谷 (70年代)**：原因——**计算能力不足**（算不动）、**算法局限**（感知机解不了异或）、**期望过高**（机器翻译失败）。
    *   **复苏 (80年代)**：专家系统的成功。
    *   **第二次低谷 (80年代末)**：原因——**专家系统瓶颈**（知识获取难、维护贵、不仅通用）、LISP机市场溃败。
    *   **稳步发展与爆发 (90年代至今)**：网络、大数据、深度学习、算力提升（GPU）。

**Q2: 什么是“中文屋”实验？它试图说明什么问题？**
*   **答案思路**：
    *   **描述**：屋内的人不懂中文，只靠一本规则书将输入的中文符号转换为输出的中文符号，让外面的人误以为他懂中文。
    *   **结论**：试图说明图灵测试是不足的。机器（屋内的人）可以表现得像有智能（通过图灵测试），但实际上并没有真正的理解（Intentionality/Understanding）。它反驳了“强人工智能”的可能性，即语法处理不等于语义理解。

## 题型四：综合分析题 (15分)
**题目**：AlphaGo 战胜李世石是人工智能历史上的里程碑事件。请分析 AlphaGo 主要采用了哪些人工智能技术？它属于哪一个流派的胜利？或者说是流派的融合？

**答案要点**：
1.  **主要技术**：
    *   **深度学习 (Deep Learning)**：使用了**卷积神经网络 (CNN)** 来评估棋盘局势（价值网络）和预测下一步落子（策略网络）。这是**连接主义**的体现。
    *   **蒙特卡洛树搜索 (MCTS)**：一种启发式的搜索策略，用于模拟未来的棋局走向。这是**符号主义/经典搜索**的体现。
    *   **强化学习 (Reinforcement Learning)**：通过自我对弈来不断进化策略。这是**行为主义**的体现。
2.  **流派分析**：
    *   AlphaGo 并非单一流派的胜利，而是**连接主义**（神经网络感知棋盘）与**符号主义**（搜索树进行规划）以及**行为主义**（强化学习）的完美融合。但核心突破在于连接主义（深度学习）解决了传统搜索无法评估围棋复杂局面的难题。

---

### 💡 考前极速复习清单 (Cheat Sheet)
1.  **1956年达特茅斯会议** = AI诞生。
2.  **图灵测试** = 行为判断智能；**中文屋** = 反驳图灵测试。
3.  **符号主义** = 逻辑/规则/专家系统；**连接主义** = 神经网络/深度学习；**行为主义** = 进化/感知行动。
4.  **明斯基** = 骂死感知机（造成第一次寒冬）。
5.  **Hinton** = 深度学习之父（开启现在的AI热潮）。

掌握了这份内容，第一章的考试绝对稳如泰山！祝你开门红！
---
这是一个非常常见的问题。GitHub 的 Markdown 渲染引擎（以及许多通用的 Markdown 编辑器）默认并不总是支持 LaTeX 数学公式（即那些用 `$` 包裹的代码，如 `$\cup$` 或 `$\forall$`）。

为了确保您的复习全书在 **GitHub、Notion、Obsidian** 甚至**手机备忘录**里都能完美显示，我将文中的所有数学符号替换为了 **Unicode 通用字符**。这样不需要任何插件，所有符号都能直接显示出来。

以下是为您修复了符号显示问题的**最终通用版**：

---

# 📘 第二章：知识表示与知识图谱 (复习全书·通用符号版)

## 第一部分：知识的本质与DIKW模型

在深入具体方法前，必须先理解我们到底在表示什么。

### 1. DIKW 模型 (层级金字塔)
这是计算机科学中描述认知层级的经典模型，考试中常作为概念题出现。
*   **数据 (Data)**：原始的、未加工的符号。
    *   *例*：`39` (单独看无意义)。
*   **信息 (Information)**：被赋予了意义的数据。
    *   *例*：`体温 39度` (知道了这是体温)。
*   **知识 (Knowledge)**：信息之间的联系、规律和经验。**这是本章的重点**。
    *   *例*：`如果体温超过37.3度，则属于发热。` (这就是知识)。
*   **智慧 (Wisdom)**：运用知识解决问题的能力。
    *   *例*：`病人发热，医生决定开退烧药并建议休息。`

### 2. 知识表示的定义
**知识表示**是将人类知识形式化、模型化，转化为计算机可以存储、处理和使用的形式。
*   **核心原则**：
    1.  **表达能力**：能不能把复杂的世界描述清楚？
    2.  **推理效率**：机器用这个表示法算得快不快？
    *   *注*：通常这两者是矛盾的。表达能力越强（如自然语言），推理越难；格式越死板（如数据库），推理越快但能表达的东西越少。

---

## 第二部分：经典知识表示法 (四大门派)

### 一、一阶谓词逻辑 (First-Order Predicate Logic, FOPL)
**——“严谨的数学家”**

这是AI中最古老、最严密的方法，源于数学逻辑。

#### 1. 基础组件
*   **命题 (Proposition)**：一个非真即假的陈述句。
    *   *缺陷*：颗粒度太粗，无法拆解内部结构。例如“老李是小李的父亲”，在命题逻辑里只是一个符号 `P`，无法分析“父子关系”。
*   **谓词 (Predicate)**：用来描述个体的性质或个体间的关系。
    *   格式：`P(x₁, x₂, ...)`
    *   *例*：`Father(LaoLi, XiaoLi)`。
*   **项 (Term)**：
    *   **常量**：具体的对象（如 `Apple`, `Beijing`）。
    *   **变量**：泛指的对象（如 `x`, `y`）。
    *   **函数**：返回一个对象的映射（如 `BestFriend(x)` 表示x最好的朋友）。
*   **连接词 (Connectives)**：
    *   **¬** (非)
    *   **∧** (与/合取)
    *   **∨** (或/析取)
    *   **→** (蕴含/条件)
    *   **↔** (等价)
*   **量词 (Quantifiers)**：**考试核心难点**
    *   **全称量词 (∀)**：表示“所有”、“任意”、“每一个”。
    *   **存在量词 (∃)**：表示“存在”、“有一个”、“至少一个”。

#### 2. 翻译实战 (必考题型)
如何将自然语言翻译成谓词公式？**这是考试的必得分项，也是易错项。**

**黄金法则：**
*   **∀ 通常搭配 → (蕴含)**。
    *   *错误写法*：`(∀x)(Student(x) ∧ Smart(x))`。这意思是“全宇宙所有东西都是学生，而且都聪明”，显然荒谬。
    *   *正确写法*：`(∀x)(Student(x) → Smart(x))`。意思是“如果是学生，那么他聪明”。
*   **∃ 通常搭配 ∧ (合取)**。
    *   *错误写法*：`(∃x)(Robot(x) → Color(x, Red))`。这在逻辑上会产生“假前提即真”的陷阱。
    *   *正确写法*：`(∃x)(Robot(x) ∧ Color(x, Red))`。意思是“存在一个x，它既是机器人，又是红色的”。

**经典例题：**
1.  **“所有的人都会死。”**
    *   `(∀x)(Human(x) → Mortal(x))`
2.  **“有些人喜欢所有类型的音乐。”**
    *   `(∃x)(Person(x) ∧ (∀y)(Music(y) → Like(x, y)))`

#### 3. 优缺点评价
*   **优点**：严密、自然、通用性强，有成熟的推理算法（如归结原理）。
*   **缺点**：效率低（组合爆炸），难以表示“不确定性”（比如“大概”、“可能”）。

---

### 二、产生式表示法 (Production Systems)
**——“经验丰富的老师傅”**

这是**专家系统**的基础，模拟人类的“条件反射”或“经验法则”。

#### 1. 基本结构
*   **形式**：`IF P THEN Q` 或 `IF P THEN Q (置信度)`
*   **含义**：
    *   **P (前件)**：前提条件、状态或原因。
    *   **Q (后件)**：结论、动作或结果。

#### 2. 产生式系统的组成 (系统架构)
这是一个动态的系统，不像逻辑那样是静态的公式。
1.  **规则库 (Rule Base)**：**长期记忆**。存放所有的规则（例如医学书上的知识）。
2.  **综合数据库 (Fact Base / Working Memory)**：**短期记忆**。存放当前已知的事实（例如病人的具体症状）。
3.  **推理机 (Inference Engine)**：**大脑**。负责“匹配-冲突消解-执行”的循环。
    *   *匹配*：拿事实去对规则的IF部分。
    *   *冲突消解*：如果好几条规则都满足IF，选哪一条？（比如选最具体的、或是最新的）。
    *   *执行*：执行THEN部分，可能产生新事实存入数据库。

#### 3. 优缺点
*   **优点**：
    *   **模块性**：加一条规则、减一条规则很方便，不影响其他规则。
    *   **可解释性**：机器可以很容易地回答“为什么你得出这个结论？”（列出触发的规则链即可）。
*   **缺点**：
    *   效率较低（规则多了匹配很慢）。
    *   难以表示结构化知识（对象之间的从属关系）。

---

### 三、框架表示法 (Frame Representation)
**——“面向对象编程的鼻祖”**

由明斯基（Minsky）在1975年提出。他认为人脑不是靠一条条逻辑思考的，而是靠**情境（Context）**。

#### 1. 核心概念
*   **框架 (Frame)**：描述一个对象或概念的数据结构。
*   **槽 (Slot)**：框架的属性（例如“教师”框架有“姓名”、“职称”槽）。
*   **侧面 (Facet)**：对槽的进一步约束或描述（例如“年龄”槽的范围是20-60，单位是岁）。
*   **值 (Value)**：具体的填充内容。

#### 2. 关键特性：继承 (Inheritance) ★重点★
这是框架最强大的功能，也是节省存储空间的关键。
*   **ISA关系**：下层框架（子类）自动拥有上层框架（父类）的所有属性。
*   **默认值 (Default)**：上层定义了默认属性，下层如果不特殊说明，就沿用默认值。
    *   *例子*：
        *   框架【鸟】：[运动方式：飞]
        *   框架【企鹅】：ISA 【鸟】；[运动方式：游泳]
    *   这里企鹅继承了鸟的属性，但**覆盖 (Override)** 了运动方式。这种机制非常符合人类认知。

#### 3. 优缺点
*   **优点**：结构化强，深层知识表达能力好，支持继承。
*   **缺点**：缺乏形式化的语义（不如逻辑那么严谨），推理机制复杂。

---

### 四、语义网络 (Semantic Network)
**——“概念的地图”**

*   **结构**：有向图。
    *   **节点**：代表概念、物体、事件。
    *   **弧（边）**：代表关系。
*   **常见关系**：
    *   **IS-A** (泛化关系)：如“猫 IS-A 哺乳动物”。（用于继承）
    *   **A-KIND-OF (AKO)**：类似于IS-A。
    *   **PART-OF** (聚类关系)：如“手指 PART-OF 手”。（属性不一定继承，如手指没了，手还在）。
*   *注*：语义网络可以看作是图形化的框架表示法，两者本质相通。

---

## 第三部分：知识图谱 (Knowledge Graph) —— 现代AI的基石

知识图谱是本章与时俱进的内容，由Google在2012年正式提出，本质上是**基于互联网的大规模语义网络**。

### 1. 核心定义
*   **定义**：一种用图模型来描述知识的技术，由节点（实体）和边（关系）组成。它旨在描述真实世界中存在的各种实体或概念及其关系。
*   **口号**：**"Things, not strings."** (是事物，而不是字符串)。搜索引擎不再只是匹配关键字，而是理解背后的实体。

### 2. 数据模型：三元组 (Triples) ★必背★
知识图谱的最基本存储单元。
*   **形式 1**：`（实体1，关系，实体2）`
    *   *例*：`(姚明, 身高, 2.26米)`，`(中国, 首都, 北京)`。
    *   这里，“中国”是头实体 (Subject)，“首都”是关系 (Predicate)，“北京”是尾实体 (Object)。
*   **形式 2**：`（实体，属性，属性值）`
    *   *例*：`(iPhone 15, 屏幕尺寸, 6.1英寸)`。

### 3. 逻辑架构：模式层 vs 数据层
这是一个常考的简答题知识点。
*   **模式层 (Schema Layer / Ontology)**：
    *   相当于**“模具”**或**“类”**。它定义了世界上的规则。
    *   *内容*：定义了什么是“人”，什么是“电影”，以及“人”可以“导演”“电影”。
    *   *特点*：相对稳定，也是知识图谱的核心。
*   **数据层 (Data Layer)**：
    *   相当于**“实例”**或**“对象”**。它是填充进模具的具体数据。
    *   *内容*：具体的“张艺谋”、“满江红”。
    *   *特点*：规模巨大，动态增长。

### 4. 知识图谱的构建流程 (技术路线) ★高分考点★
构建一个知识图谱通常包含以下步骤，每一步都涉及具体的AI技术：

1.  **知识抽取 (Information Extraction)**：从非结构化文本（如新闻网页）中提炼信息。
    *   **实体识别 (NER)**：识别出文本中的人名、地名、机构名。（例：从“马云创建了阿里”中识别出“马云”和“阿里”）。
    *   **关系抽取 (RE)**：判断两个实体之间的关系。（例：判断出“创建”关系）。
2.  **知识融合 (Knowledge Fusion)**：
    *   **实体对齐 (Entity Alignment)**：解决“多名一义”和“一名多义”的问题。
    *   *例*：把“Trump”、“川普”、“特朗普及”都归并为同一个实体节点。
3.  **知识加工 (Knowledge Processing)**：
    *   **质量评估**：去掉错误的知识。
    *   **知识推理**：补全缺失的知识（如：已知A是B爸爸，B是C爸爸 → 推理出A是C爷爷）。
4.  **知识更新**：随着世界变化实时更新。

### 5. 典型应用
*   **智能搜索**：Google右侧的知识卡片。
*   **智能问答**：Siri、小爱同学（直接给答案，而不是给网页链接）。
*   **推荐系统**：淘宝、抖音（利用物品之间的关系进行关联推荐）。

---

## 第四部分：综合对比与考试策略

### 1. 四种表示法的优缺点大比拼 (论述题素材)

| 表示方法 | 核心思想 | 优点 | 缺点 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- |
| **谓词逻辑** | 数学公式 | 严谨、不仅能表示真假还能推理 | 效率低、难以表示不确定性 | 自动定理证明 |
| **产生式** | IF-THEN | 灵活、模块化、易理解 | 效率低、规则多了难维护 | 专家系统 (诊断、分类) |
| **框架** | 对象+属性 | 结构化、支持继承、省空间 | 灵活性差、推理复杂 | 复杂对象的描述 |
| **知识图谱** | 网状链接 | 规模大、语义丰富、适合搜索 | 构建成本高、噪声多 | 搜索引擎、问答系统 |

### 2. 典型例题解析

#### **题型一：谓词逻辑表示**
**题目**：请用谓词逻辑表示：“每个喜欢计算机的学生都聪明”。
**解析**：
1.  定义谓词：`Student(x)`：x是学生；`Like(x, CS)`：x喜欢计算机；`Smart(x)`：x聪明。
2.  分析量词：“每个” → **∀**。
3.  分析结构：如果是（学生 且 喜欢计算机），那么（聪明）。
4.  **答案**：`(∀x) ( (Student(x) ∧ Like(x, CS)) → Smart(x) )`

#### **题型二：框架表示**
**题目**：请写出“教师”的框架，并举一个具体实例。
**解析**：
*   **框架名**：<教师>
*   **槽**：
    *   姓名：单位（姓、名）
    *   年龄：范围（20-65）
    *   职称：范围（助教、讲师、副教授、教授）；**默认值：讲师**
    *   部门：单位（学院、系）
*   **实例**：<教师-1>
    *   姓名：张三
    *   年龄：35
    *   职称：副教授
    *   部门：计算机学院

#### **题型三：知识图谱概念**
**题目**：知识图谱中的模式层和数据层有什么区别？
**解析**：见上文“逻辑架构”部分。重点强调模式层是**定义规则和类**（元数据），数据层是**具体的事实**（实例）。

### 3. 考前突击重点
1.  **死磕谓词公式**：全称量词（∀ 搭配 →）和存在量词（∃ 搭配 ∧）的规则绝对不能错。
2.  **背诵三元组**：提到知识图谱，必须立刻反应出“实体-关系-实体”。
3.  **理解继承**：在框架和语义网络中，下层自动拥有上层的属性，这叫继承。
4.  **区分数据与知识**：DIKW模型中，有规律、可指导行动的才是知识。
---
这是一份为您精心编撰的**《人工智能基础》第三章：确定性推理**的深度精讲复习全书。

本章是人工智能逻辑主义流派的核心，也是期末考试中**计算题**和**证明题**的绝对重灾区。特别是**鲁宾逊归结原理（Resolution Principle）**，通常占据卷面 15-20 分的分值。

为了达到“极致详细”且“易于理解”的目标，我将从**推理的基本逻辑**、**控制策略**、**自然演绎**，到核心的**归结原理**及其**算法实现细节**进行全方位拆解。

---

# 📘 第三章：确定性推理 (Deterministic Reasoning)

## 第一部分：推理的基石与分类

在让计算机思考之前，我们需要先定义什么是“思考”。

### 1. 什么是推理？
**定义**：从已知的事实（Evidence）出发，通过运用已掌握的知识（Knowledge），按照某种策略，逐步推导出结论（Conclusion）或证明某个假设成立的过程。
*   **公式化表达**：$Knowledge + Evidence \xrightarrow{Strategy} Conclusion$

### 2. 推理方式的分类 (选择/填空考点)

#### (1) 按逻辑基础分类
*   **演绎推理 (Deductive Reasoning)**：
    *   **定义**：从一般到个别。只要前提为真，结论必然为真。
    *   *例子（三段论）*：
        *   大前提：所有金属都能导电。
        *   小前提：铜是金属。
        *   结论：铜能导电。
    *   *地位*：这是确定性推理的核心，计算机最擅长。
*   **归纳推理 (Inductive Reasoning)**：
    *   **定义**：从个别到一般。结论具有或然性（不一定全对）。
    *   *例子*：观察到铁受热膨胀、铜受热膨胀 $\rightarrow$ 结论：所有金属受热都膨胀。
    *   *分类*：完全归纳（考察了所有样本，结论确定） vs 不完全归纳（只考察了部分，结论不确定）。
*   **默认推理 (Default/Defeasible Reasoning)**：
    *   **定义**：在知识不完全的情况下，假设某些条件具备而进行的推理。
    *   *例子*：提到“鸟”，默认“会飞”。如果后来发现是“企鹅”，再撤销之前的结论。
    *   *特性*：这是**非单调**的。

#### (2) 按推理过程中知识的性质分类
*   **单调推理 (Monotonic)**：随着新知识的加入，已有的结论不会被推翻，只会越来越稳固。（经典逻辑都是单调的）。
*   **非单调推理 (Non-monotonic)**：新知识的加入可能导致旧结论失效（如上面的“企鹅”例子）。

---

## 第二部分：推理的控制策略 (The Engine)

有了知识和证据，机器该从哪头开始推？这就涉及到了**推理方向**。

### 1. 正向推理 (Forward Chaining) —— “数据驱动”
*   **逻辑**：$事实 \rightarrow 规则 \rightarrow 结论$。
*   **过程**：
    1.  把用户提供的初始事实放入**综合数据库**。
    2.  用**知识库**中的规则去匹配数据库中的事实。
    3.  如果规则的前提被满足，则触发规则，将结论作为新事实加入数据库。
    4.  重复，直到得出目标或无法继续。
*   **优缺点**：
    *   ✅ 直观，易于实现。
    *   ❌ 盲目性强，可能推导出一堆无用的中间结论，效率低。
*   **适用场景**：用户能提供大量事实，但目标不明确时（如：医生根据症状诊断疾病）。

### 2. 逆向推理 (Backward Chaining) —— “目标驱动”
*   **逻辑**：$假设目标 \rightarrow 寻找支持证据 \rightarrow 验证事实$。
*   **过程**：
    1.  假设一个目标成立。
    2.  寻找能推导出该目标的规则。
    3.  检查该规则的前提是否在数据库中。
    4.  如果在，则目标成立；如果不在，将该前提作为**子目标**，递归寻找支持子目标的规则。
*   **优缺点**：
    *   ✅ 目的性强，只寻找相关知识。
    *   ❌ 初始目标选择盲目，若选错目标则浪费时间。
*   **适用场景**：目标明确，但事实很少时（如：警察查案，假设嫌疑人，找证据）。

### 3. 混合推理 (Hybrid Reasoning)
*   **策略**：先正向推理，从已知事实推导出部分结论，缩小假设范围；再用逆向推理验证这些假设。

### 4. 冲突消解 (Conflict Resolution)
当数据库中的事实同时满足多条规则时，该选哪一条？这就是**冲突**。
**常见消解策略**：
1.  **针对性排序**：优先选择条件最苛刻（条件数量最多）的规则。（因为它最精准）。
2.  **新鲜度排序**：优先用最新推出的事实匹配的规则。
3.  **优先级排序**：专家预先给规则设定优先级。

---

## 第三部分：自然演绎推理 (Natural Deduction)

这是基于经典逻辑的推理，模拟人类的论证过程。虽然不是机器推理的主流（主流是归结），但它是基础。

### 核心规则
*   **假言推理 (Modus Ponens)**：$P, P \rightarrow Q \Rightarrow Q$。（有因必有果）
*   **拒取式推理 (Modus Tollens)**：$P \rightarrow Q, \neg Q \Rightarrow \neg P$。（无果必无因）
*   **全称固化 (Universal Instantiation)**：$\forall x P(x) \Rightarrow P(a)$。（所有人都会死 $\rightarrow$ 张三会死）。

**缺点**：推理规则太多，机器容易“迷路”，产生组合爆炸。

---

## 第四部分：鲁宾逊归结原理 (Resolution Principle) ★★★★★

**（本章核心，必考大题，请打起十二分精神！）**

1965年，J.A. Robinson 提出了一种只需一条规则就能解决所有推理问题的方法——**归结原理**。这让机器定理证明成为了可能。

### 1. 核心思想：反证法 (Refutation)
要证明：$P \rightarrow Q$（即从前提 $P$ 能推出结论 $Q$）。
**机器的做法**：
1.  假设结论是错的，即 $\neg Q$。
2.  将 $\neg Q$ 加入到前提集合 $P$ 中。
3.  试图从 $P \land \neg Q$ 中推导出**逻辑矛盾**（即推导出**空子句 NIL**）。
4.  如果推导出 NIL，说明假设 $\neg Q$ 不成立，因此原结论 $Q$ 成立。

### 2. 预处理：化为子句集 (Clause Form)
谓词公式形式复杂（有 $\forall, \exists, \rightarrow, \leftrightarrow$），机器处理不了。必须先标准化为**子句集**。
**九步法（背诵并熟练应用）：**

*   **Step 1: 消去蕴含符和等价符**
    *   $A \rightarrow B \Leftrightarrow \neg A \lor B$
    *   $A \leftrightarrow B \Leftrightarrow (\neg A \lor B) \land (\neg B \lor A)$

*   **Step 2: 否定内移 (De Morgan's Laws)**
    *   $\neg (A \lor B) \Leftrightarrow \neg A \land \neg B$
    *   $\neg (A \land B) \Leftrightarrow \neg A \lor \neg B$
    *   $\neg \forall x P(x) \Leftrightarrow \exists x \neg P(x)$
    *   $\neg \exists x P(x) \Leftrightarrow \forall x \neg P(x)$
    *   $\neg \neg P \Leftrightarrow P$
    *   *目标*：让 $\neg$ 只出现在原子谓词前面。

*   **Step 3: 变量标准化 (Standardize Variables)**
    *   确保不同量词约束的变量名字不同。
    *   例：$\forall x P(x) \lor \forall x Q(x) \Rightarrow \forall x P(x) \lor \forall y Q(y)$。

*   **Step 4: 消去存在量词 ($\exists$) —— Skolem 化 (重点难点)**
    *   **情况A**：$\exists$ 不在任何 $\forall$ 的辖域内。$\rightarrow$ 用**Skolem常量**替换。
        *   例：$\exists x P(x) \Rightarrow P(a)$ （存在一个人是小偷 $\rightarrow$ 某人a是小偷）。
    *   **情况B**：$\exists$ 在 $\forall$ 的辖域内。$\rightarrow$ 用**Skolem函数**替换。
        *   例：$\forall x \exists y Father(y, x)$ （每个人都有父亲）。
        *   错误：$Father(a, x)$ （所有人都有同一个父亲a，错）。
        *   正确：$Father(f(x), x)$ （对于每个x，都有一个对应的f(x)是他的父亲）。

*   **Step 5: 化为前束形 (Prenex Normal Form)**
    *   把所有全称量词 $\forall$ 移到公式最左边。

*   **Step 6: 化为合取范式 (CNF)**
    *   利用分配律：$A \lor (B \land C) \Leftrightarrow (A \lor B) \land (A \lor C)$。
    *   目标格式：$(...) \land (...) \land (...)$

*   **Step 7: 消去全称量词**
    *   因为前面已经Skolem化了，剩下的变量默认都是全称量词，直接扔掉 $\forall$。

*   **Step 8: 消去连词 $\land$**
    *   把公式拆成一个个独立的**子句 (Clause)**。集合中的子句之间是“与”的关系。

*   **Step 9: 变量换名**
    *   让不同子句中的变量名互不相同（为了后续合一操作方便）。

### 3. 归结推理过程
**归结规则**：对于两个子句 $C_1$ 和 $C_2$，如果它们包含**互补文字**（即 $L$ 和 $\neg L$），则可以消去这对文字，将其余部分析取，得到归结式（Resolvent）。

#### (1) 命题逻辑的归结（简单版）
*   $C_1 = P \lor Q$
*   $C_2 = \neg P \lor R$
*   $P$ 和 $\neg P$ 抵消。
*   归结式：$Q \lor R$。

#### (2) 谓词逻辑的归结（进阶版：含变量）
这就需要**合一 (Unification)**。因为 $P(x)$ 和 $\neg P(a)$ 表面上不互补，必须让 $x=a$ 才能抵消。
*   **最一般合一者 (MGU)**：找到一个置换 $\sigma$，使得两个谓词完全一致。

**归结步骤**：
1.  从子句集中选两个子句（亲本子句）。
2.  找到其中一对待归结的文字（如 $P(x)$ 和 $\neg P(f(y))$）。
3.  寻找合一置换 $\sigma$（如 $\{f(y)/x\}$）。
4.  应用置换，消去互补文字。
5.  生成新子句，加入子句集。
6.  重复，直到推出 **NIL (空子句)**。

---

## 第五部分：实战演练 —— 经典考题详解

### 题目：狼羊问题证明
**已知**：
1.  所有的狼都是肉食动物。
2.  有些肉食动物也吃草。
3.  所有吃草的动物都不是狼。
**求证**：
存在不是狼的肉食动物。

---

### 详细解题步骤

#### 1. 定义谓词
*   $W(x)$: x 是狼 (Wolf)
*   $M(x)$: x 是肉食动物 (Meat-eater)
*   $G(x)$: x 吃草 (Grass-eater)

#### 2. 将事实和结论形式化
*   F1: $\forall x (W(x) \rightarrow M(x))$
*   F2: $\exists x (M(x) \land G(x))$
*   F3: $\forall x (G(x) \rightarrow \neg W(x))$
*   结论 Q: $\exists x (M(x) \land \neg W(x))$

#### 3. 将结论否定
*   $\neg Q$: $\neg \exists x (M(x) \land \neg W(x))$
    *   $\Leftrightarrow \forall x \neg (M(x) \land \neg W(x))$
    *   $\Leftrightarrow \forall x (\neg M(x) \lor W(x))$

#### 4. 化为子句集 (关键步骤)

*   **处理 F1**:
    *   $\neg W(x) \lor M(x)$
    *   **子句 ①**: $\neg W(x) \lor M(x)$

*   **处理 F2**:
    *   $\exists x (M(x) \land G(x))$
    *   Skolem化（用常量 $a$ 替换 $x$）：$M(a) \land G(a)$
    *   拆分：
        *   **子句 ②**: $M(a)$
        *   **子句 ③**: $G(a)$

*   **处理 F3**:
    *   $\neg G(y) \lor \neg W(y)$ （注意换名）
    *   **子句 ④**: $\neg G(y) \lor \neg W(y)$

*   **处理 $\neg Q$**:
    *   $\neg M(z) \lor W(z)$ （注意换名）
    *   **子句 ⑤**: $\neg M(z) \lor W(z)$

#### 5. 归结过程 (画树或步骤列表)

*   **Step 1**: 拿子句 ③ $G(a)$ 和 子句 ④ $\neg G(y) \lor \neg W(y)$ 归结。
    *   合一置换：$\{a/y\}$
    *   消去 $G$，得到新子句 ⑥：**$\neg W(a)$**

*   **Step 2**: 拿子句 ② $M(a)$ 和 子句 ⑤ $\neg M(z) \lor W(z)$ 归结。
    *   合一置换：$\{a/z\}$
    *   消去 $M$，得到新子句 ⑦：**$W(a)$**

*   **Step 3**: 拿子句 ⑥ $\neg W(a)$ 和 子句 ⑦ $W(a)$ 归结。
    *   直接抵消。
    *   得到新子句 ⑧：**NIL (空子句)**

#### 6. 结论
因为推导出了空子句，说明前提与结论的否定存在矛盾，所以**原结论成立**。得证。

---

## 第六部分：归结策略 (怎么归结更快？)

为了防止机器像没头苍蝇一样乱归结，科学家设计了一些策略（选择题常考）：

1.  **删除策略**：
    *   删除包孕子句（如果 $P$ 包含 $P \lor Q$，删掉 $P \lor Q$）。
    *   删除纯文字（如果某个文字在所有子句中只有一种符号，如永远是 $P$，没有 $\neg P$，那包含它的子句都没用）。
2.  **支持集策略 (Set of Support)**：
    *   每次归结必须有一个子句是来自**目标否定的那个集合**（或者由它衍生出来的）。这保证了推理是朝着目标去的。
3.  **线性归结**：
    *   下一次归结必须用到上一次归结产生的新结果。
4.  **输入归结**：
    *   每次归结必须有一个是初始给定的子句。

---

## 第七部分：问题求解与答案提取 (Answer Extraction)

除了证明“是/否”，有时我们需要问“是谁/是什么”。
*   **方法**：在否定结论时，加一个构造的谓词 $ANSWER(x)$。
*   **过程**：
    *   求证：$\exists x P(x)$。
    *   否定：$\forall x \neg P(x)$。
    *   构造：$\forall x (\neg P(x) \lor ANSWER(x))$。
    *   正常归结，直到最后**剩下一个只包含 ANSWER 的子句**。
    *   那个 ANSWER 里的内容，就是问题的解。

*   *例子*：如果最后归结出 $ANSWER(ZhangSan)$，那么凶手就是张三。

---

### 💡 考前复习 CheckList
1.  [ ] 能熟练说出推理的分类（演绎、归纳、默认）。
2.  [ ] 能默写化子句集的9个步骤（特别是Skolem化）。
3.  [ ] 掌握合一操作，能找出两个谓词的MGU。
4.  [ ] 能独立完成一个完整的谓词逻辑归结证明题（如上面的狼羊问题）。
5.  [ ] 理解归结反演的原理：为什么要否定结论？什么是空子句？

这一章是纯逻辑的硬仗，只要把**9步法**和**归结树**练熟，考试时这就是送分题！加油！
---
这是一份为您精心打造的**《人工智能基础》第四章：不确定性推理**的深度复习全书。

这一章是人工智能从“理想国”走向“现实世界”的关键一步。现实世界的信息往往是不完备、模糊、随机和不一致的，传统的逻辑推理（非黑即白）无法处理这些问题。因此，**不确定性推理**应运而生。

本章在期末考试中通常以**大计算题**的形式出现，分值高且步骤繁琐。为了帮助你彻底掌握，我将以**“模型原理 + 核心公式 + 经典例题 + 避坑指南”**的结构进行地毯式讲解。

---

# 📘 第四章：不确定性推理 (Uncertainty Reasoning)

## 🌟 核心导读：不确定性的来源
在进入算法之前，先理解为什么需要不确定性推理？
1.  **随机性 (Randomness)**：事件发生与否具有偶然性。（例如：抛硬币，明天下雨的概率）。
2.  **模糊性 (Fuzziness)**：概念本身的边界不清晰。（例如：“个子高”，“天气热”，“年轻”。18岁是年轻，28岁算不算？）。
3.  **不完全性 (Incompleteness)**：缺乏解决问题所需的全部信息。（例如：侦探破案，线索中断）。
4.  **不一致性 (Inconsistency)**：不同来源的信息相互矛盾。（例如：专家A说这病能治，专家B说治不了）。

---

## 第一部分：可信度方法 (Certainty Factor Model, C-F)
**——“基于经验的直觉推理”**

这是最早应用于医疗专家系统 **MYCIN** 的推理模型。它不追求数学概率上的绝对严谨，而是模拟人类专家对某件事的“确信程度”。

### 1. 核心概念与定义
*   **表示形式**：
    $$IF \quad E \quad THEN \quad H \quad (CF(H, E))$$
    *   $E$ (Evidence)：证据。
    *   $H$ (Hypothesis)：结论/假设。
    *   $CF(H, E)$：**可信度因子**。表示“当证据 $E$ 确定出现时，它对结论 $H$ 为真的支持程度”。
*   **取值范围**：$[-1, +1]$
    *   $+1$：证据 $E$ 能够**完全证明** $H$ 为真。
    *   $-1$：证据 $E$ 能够**完全证明** $H$ 为假。
    *   $0$：证据 $E$ 与 $H$ 无关（既不支持也不反对）。
    *   $>0$：支持 $H$ 为真（正向支持）。
    *   $<0$：支持 $H$ 为假（反向削弱）。

### 2. 关键计算公式 (考试必背)

#### (1) 证据的不确定性 (组合证据)
当规则的前提由多个条件组合而成（AND / OR）时，如何计算组合证据的可信度 $CF(E)$？
*   **合取 (AND) —— “木桶效应”**：
    $$CF(E_1 \land E_2 \land \dots \land E_n) = \min \{ CF(E_1), CF(E_2), \dots, CF(E_n) \}$$
    *   *理解*：只要有一个短板，整体的可信度就被拉低。
*   **析取 (OR) —— “择优录取”**：
    $$CF(E_1 \lor E_2 \lor \dots \lor E_n) = \max \{ CF(E_1), CF(E_2), \dots, CF(E_n) \}$$
    *   *理解*：只要有一个证据很强，整体就可信。

#### (2) 结论的不确定性 (传递算法)
当我们算出证据 $E$ 的可信度 $CF(E)$ 后，如何计算它推导出的结论 $H$ 的可信度 $CF(H)$？
**公式**：
$$CF(H) = CF(H, E) \times \max \{ 0, CF(E) \}$$
*   **规则强度**：$CF(H, E)$（专家给定的）。
*   **证据强度**：$CF(E)$（计算出来的）。
*   **注意**：这里有一个 `max{0, CF(E)}`。意思是：**如果证据本身是假的（$CF(E) < 0$），那么这条规则就不起作用（结果为0），而不是产生负面影响。**

#### (3) 结论的合成 (多条路径)
这是计算题中最复杂的一步。如果 **规则1** 推导出 $H$ 的可信度为 $CF_1$，**规则2** 也推导出 $H$ 的可信度为 $CF_2$，如何把这两个结果合并？

我们需要根据 $CF_1$ 和 $CF_2$ 的符号分三种情况讨论：

*   **情况一：同正（锦上添花）**
    若 $CF_1 > 0, CF_2 > 0$：
    $$CF_{combine} = CF_1 + CF_2 - CF_1 \times CF_2$$
    *   *理解*：两个正向证据叠加，信度增加，但永远不会超过1。
*   **情况二：同负（雪上加霜）**
    若 $CF_1 < 0, CF_2 < 0$：
    $$CF_{combine} = CF_1 + CF_2 + CF_1 \times CF_2$$
    *   *理解*：两个负向证据叠加，信度更低，但永远不会低于-1。
*   **情况三：一正一负（互相抵消）**
    若 $CF_1$ 和 $CF_2$ 异号：
    $$CF_{combine} = \frac{CF_1 + CF_2}{1 - \min(|CF_1|, |CF_2|)}$$
    *   *理解*：强的会抵消弱的，最终符号取决于绝对值大的一方。

---

### 📚 C-F 模型实战例题
**题目**：
*   规则1：如果 发烧 AND 咳嗽，则 肺炎 (CF = 0.8)
*   规则2：如果 胸痛，则 肺炎 (CF = 0.6)
*   已知事实：发烧 (CF = 0.9)，咳嗽 (CF = 0.8)，胸痛 (CF = -0.5)。
*   求：“肺炎”的可信度。

**解析**：
1.  **处理规则1**：
    *   证据部分：$E_1 = 发烧 \land 咳嗽$
    *   $CF(E_1) = \min(0.9, 0.8) = 0.8$
    *   结论部分：$CF_1(肺炎) = 0.8 (\text{规则强度}) \times \max(0, 0.8) = 0.64$
2.  **处理规则2**：
    *   证据部分：$E_2 = 胸痛$
    *   $CF(E_2) = -0.5$
    *   结论部分：$CF_2(肺炎) = 0.6 (\text{规则强度}) \times \max(0, -0.5) = 0.6 \times 0 = 0$
    *   *注意*：因为证据为负，规则不触发。
3.  **合成**：
    *   $CF_{final} = CF_1 + CF_2 - CF_1 \times CF_2 = 0.64 + 0 - 0 = 0.64$
    *   *结论*：肺炎的可信度为 0.64。

---

## 第二部分：D-S 证据理论 (Dempster-Shafer Theory)
**——“处理‘不知道’的艺术”**

贝叶斯概率论要求 $P(A) + P(\neg A) = 1$，这意味着如果你不支持A，你就必须反对A。但 D-S 理论允许**“无知”**的存在。我可以既不支持A，也不反对A，我只是不知道。

### 1. 核心概念 (名词解释考点)

*   **识别框架 ($\Theta$)**：所有可能结果的互斥集合。
    *   例：$\Theta = \{感冒, 肺炎, 过敏\}$。
*   **基本概率分配函数 (BPA, $M$)**：
    *   $M(A)$ 表示证据对命题 $A$ 的**直接**支持程度。
    *   **公理**：$\sum_{A \subseteq \Theta} M(A) = 1$ 且 $M(\Phi) = 0$。
    *   **注意**：这里 $A$ 可以是集合，比如 $M(\{感冒, 肺炎\}) = 0.3$ 表示“我觉得是这两种病之一，但具体是哪个不知道”。
*   **信任函数 (Belief, $Bel$)**：
    *   $Bel(A)$ 表示对 $A$ 的**坚信程度**（所有子集支持度之和）。
    *   公式：$Bel(A) = \sum_{B \subseteq A} M(B)$。
    *   *理解*：这是你确实拿到手的铁证。
*   **似然函数 (Plausibility, $Pl$)**：
    *   $Pl(A)$ 表示对 $A$ 的**不反对程度**（所有与A相交集合的支持度之和）。
    *   公式：$Pl(A) = 1 - Bel(\neg A)$。
    *   *理解*：这是你未来可能拿到的最大支持度（包含“不知道”的部分）。
*   **信任区间**：$[Bel(A), Pl(A)]$。区间越宽，表示越“无知”；区间越窄，表示信息越确定。

### 2. Dempster 合成规则 (正交和公式) ★计算题核心★
当有两个证据源 $M_1$ 和 $M_2$ 时，如何将它们合并成一个新的 $M$？

**步骤详解**：
1.  **画表（正交表）**：横轴列出 $M_1$ 的所有项，纵轴列出 $M_2$ 的所有项。
2.  **计算交叉积**：单元格的值 = 行的M值 $\times$ 列的M值。
3.  **判断集合交集**：看行集合与列集合的交集是什么（如 $\{A\} \cap \{A,B\} = \{A\}$）。
4.  **计算冲突因子 $K$**：
    *   找出所有交集为**空集 ($\Phi$)** 的单元格。
    *   将这些单元格的积加起来，得到 $K$。
    *   $K$ 代表两个证据的冲突程度。$K=1$ 表示完全冲突，无法合并。
5.  **归一化计算**：
    *   对于任意非空集合 $C$，把所有交集结果为 $C$ 的单元格积加起来。
    *   **关键一步**：将总和除以 $(1-K)$。
    *   公式：
        $$M(C) = \frac{1}{1-K} \sum_{A \cap B = C} M_1(A) \times M_2(B)$$

---

### 📚 D-S 理论实战例题
**题目**：辨别嫌疑人 $\Theta = \{A, B, C\}$。
*   目击者1：$M_1(\{A\}) = 0.8, \quad M_1(\Theta) = 0.2$ （80%确定是A，20%不知道）
*   目击者2：$M_2(\{B\}) = 0.6, \quad M_2(\Theta) = 0.4$ （60%确定是B，40%不知道）
求合并后的结果。

**解析**：
1.  **画表计算**：

| $M_1 \setminus M_2$ | $M_2(\{B\}) = 0.6$ | $M_2(\Theta) = 0.4$ |
| :--- | :--- | :--- |
| **$M_1(\{A\}) = 0.8$** | 交集：$\Phi$ (冲突)<br>积：$0.8 \times 0.6 = 0.48$ | 交集：$\{A\}$<br>积：$0.8 \times 0.4 = 0.32$ |
| **$M_1(\Theta) = 0.2$** | 交集：$\{B\}$<br>积：$0.2 \times 0.6 = 0.12$ | 交集：$\Theta$<br>积：$0.2 \times 0.4 = 0.08$ |

2.  **计算冲突 $K$**：
    只有第一格产生了空集 $\Phi$。
    $K = 0.48$。
    归一化系数：$\frac{1}{1-K} = \frac{1}{0.52} \approx 1.923$。

3.  **计算合并后的 $M$**：
    *   **$M(\{A\})$**：
        对应项：$0.32$
        结果：$0.32 / 0.52 \approx 0.615$
    *   **$M(\{B\})$**：
        对应项：$0.12$
        结果：$0.12 / 0.52 \approx 0.231$
    *   **$M(\Theta)$**：
        对应项：$0.08$
        结果：$0.08 / 0.52 \approx 0.154$
    *   **$M(\{C\})$**：0 （没有产生单独的C集合）

4.  **结论**：嫌疑人是A的可能性最大 (0.615)，其次是B (0.231)。

---

## 第三部分：模糊推理 (Fuzzy Reasoning)
**——“像人一样思考”**

模糊逻辑处理的是“程度”问题。在经典集合中，一个元素要么属于A，要么不属于（非0即1）。在模糊集合中，元素可以“部分属于”A（隶属度在0到1之间）。

### 1. 基础运算 (Zadeh 算子) ★必考★
设 $\mu_A(x)$ 和 $\mu_B(x)$ 分别是集合 A 和 B 的隶属度函数。
*   **相等**：$A = B \iff \mu_A(x) = \mu_B(x)$
*   **包含**：$A \subseteq B \iff \mu_A(x) \le \mu_B(x)$
*   **并集 (Union/OR)**：**取大**。
    $\mu_{A \cup B}(x) = \max(\mu_A(x), \mu_B(x))$
*   **交集 (Intersection/AND)**：**取小**。
    $\mu_{A \cap B}(x) = \min(\mu_A(x), \mu_B(x))$
*   **补集 (Complement/NOT)**：**1减**。
    $\mu_{\neg A}(x) = 1 - \mu_A(x)$

### 2. 模糊关系与合成
模糊规则 "IF $x$ is $A$ THEN $y$ is $B$" 本质上是一个模糊关系矩阵 $R$。
*   **构建矩阵 $R$**：通常使用叉积（取小）法。
    $R = A^T \times B$ （如果是离散向量，列向量 $\times$ 行向量）。
    $R_{ij} = \min(a_i, b_j)$。

*   **合成推理 (Max-Min Composition)**：
    已知输入 $A'$，求输出 $B'$。
    公式：$B' = A' \circ R$。
    **计算方法**：类似于矩阵乘法，但把“乘法”换成“取小 (Min)”，把“加法”换成“取大 (Max)”。
    *   *口诀*：**先列对行取小，再对结果取大**。

### 3. 清晰化 (Defuzzification)
模糊推理的结果 $B'$ 是一个模糊集合（比如一组隶属度），机器控制需要一个精确数值（比如“风扇转速调到500转”）。
*   **最大隶属度法**：取隶属度最大的那个元素。
    *   优点：简单。缺点：丢失信息多，不平滑。
*   **重心法 (Centroid)**：计算模糊分布图形的重心坐标。
    *   公式：$u = \frac{\sum x_i \cdot \mu(x_i)}{\sum \mu(x_i)}$
    *   优点：精确，利用了所有信息。

---

## 📝 第四章高频考点总结 (Cheat Sheet)

1.  **C-F 模型**：
    *   AND 取小，OR 取大。
    *   结论传递：$CF(H,E) \times \max(0, CF(E))$。
    *   合成公式：同号相加减乘积，异号相加除以(1-min)。
2.  **D-S 证据理论**：
    *   正交和计算表格。
    *   **必须记得除以 $(1-K)$ 归一化！**
    *   $K$ 是空集的积之和。
3.  **模糊逻辑**：
    *   交取小，并取大，补是一减。
    *   矩阵合成运算 Max-Min。
    *   清晰化重心法公式。

掌握了以上三个板块的计算逻辑，第四章的分数就稳拿了！这部分一定要多做两道计算题练手感。祝复习顺利！
---
这是一份为您精心准备的**《人工智能基础》第五章：搜索求解策略**的深度复习全书。

搜索是人工智能最核心、最基础的模块之一。甚至可以说，早期的AI（GOFAI, Good Old-Fashioned AI）本质上就是**搜索+知识**。无论是AlphaGo下围棋，还是高德地图导航，其底层逻辑都离不开本章的内容。

本章内容较多，逻辑性强，为了达到“极致详细且易懂”的目标，我们将从**基本概念**、**盲目搜索**、**启发式搜索**、**博弈搜索**四个维度进行拆解。

---

# 📘 第五章：搜索求解策略 (Search Solving Strategies)

## 第一部分：搜索的底层逻辑与状态空间表示

在学习具体算法前，必须先建立“世界观”。计算机解决问题的方式，就是在一个巨大的“可能世界”中寻找一条通往“目标世界”的路。

### 1. 核心概念定义
*   **问题求解 (Problem Solving)**：将问题转化为一个图（Graph）或树（Tree），然后在其中寻找路径的过程。
*   **状态空间 (State Space)**：表示问题所有可能状态的集合。通常用四元组表示：$(S, O, S_0, G)$。
    *   **$S$ (States)**：状态集合。比如八数码问题中，每一个数字排列就是一个状态。
    *   **$O$ (Operators)**：操作算子集合。从一个状态转移到另一个状态的动作。比如“向上移动空格”。
    *   **$S_0$ (Initial State)**：初始状态。
    *   **$G$ (Goal State)**：目标状态（可能是一个具体状态，也可能是满足某些条件的状态集合）。

### 2. 搜索的两大关键表
在计算机实现搜索时，内存中主要维护两张表（必考）：
1.  **OPEN 表 (Open List)**：存放**已生成但尚未扩展**的节点。
    *   *通俗理解*：这是“待办事项清单”，下一步要探索的节点都排在这里。不同的搜索算法（BFS/DFS/A*）的区别，本质上就是**怎么从OPEN表中取节点**（排队规则不同）。
2.  **CLOSED 表 (Closed List)**：存放**已扩展过**的节点。
    *   *通俗理解*：这是“已办事项清单”或“历史记录”。
    *   *作用*：防止死循环（避免回到老路上），并在图搜索中避免重复计算。

---

## 第二部分：盲目搜索 (Blind/Uninformed Search)
**——“不撞南墙不回头的暴力美学”**

盲目搜索的特点是：**没有任何关于“离目标还有多远”的额外信息**。只知道当前在哪里，能往哪里走。

### 1. 宽度优先搜索 (Breadth-First Search, BFS)
*   **策略**：先搜索离起始点最近的节点。像水波纹一样一层层向外扩展。
*   **OPEN表的数据结构**：**队列 (Queue)** —— **先进先出 (FIFO)**。
    *   每次把新生成的节点放到队尾，取节点从队头取。
*   **算法特性（考点）**：
    *   **完备性 (Completeness)**：**是**。只要有解，BFS一定能找到。
    *   **最优性 (Optimality)**：**是**。如果所有边的代价（权重）相等（例如都是1），BFS找到的一定是最短路径（步数最少）。
    *   **时间/空间复杂度**：$O(b^d)$。（$b$是分支因子，$d$是解的深度）。
    *   *致命弱点*：**空间爆炸**。需要存储每一层的所有节点，内存极易耗尽。

### 2. 深度优先搜索 (Depth-First Search, DFS)
*   **策略**：一条路走到黑。先访问子节点，直到没有子节点（到底了）再回溯。
*   **OPEN表的数据结构**：**栈 (Stack)** —— **后进先出 (LIFO)**。
    *   每次把新生成的节点放到栈顶，取节点也从栈顶取。
*   **算法特性（考点）**：
    *   **完备性**：**否**（在无限空间中）。如果树有一条无限长的分支，DFS可能永远陷在里面出不来，哪怕解就在隔壁分支的浅层。
    *   **最优性**：**否**。它可能绕了一大圈才找到目标，而不是最短的那条路。
    *   **优点**：**省内存**。空间复杂度仅为 $O(b \times d)$（线性增长），只需要存储当前路径上的节点。

### 3. 代价一致搜索 (Uniform Cost Search, UCS)
*   **策略**：这是BFS的升级版。当边权不相等时（比如地图上路段长度不同），BFS失效。UCS每次从OPEN表中取出**路径累积代价最小**的节点进行扩展。
*   **数据结构**：优先队列 (Priority Queue)，按 $g(n)$ （从起点到n的代价）排序。
*   **特性**：完备且最优。

### 💡 **盲目搜索对比总结（考试必背）**

| 特性 | 宽度优先 (BFS) | 深度优先 (DFS) |
| :--- | :--- | :--- |
| **数据结构** | 队列 (FIFO) | 栈 (LIFO) |
| **完备性** | 是 (一定能找到解) | 否 (可能陷入死循环) |
| **最优性** | 是 (若边权相等) | 否 |
| **空间复杂度** | 指数级 (大，容易爆内存) | 线性 (小，节省内存) |
| **适用场景** | 寻找最短路径，解较浅 | 只要找到解即可，解较深 |

---

## 第三部分：启发式搜索 (Heuristic/Informed Search)
**——“带有指南针的智能探索”**

这是本章的**核心难点**。我们引入了**启发函数 (Heuristic Function)**，利用关于问题的**领域知识**来指导搜索方向，从而加速寻找最优解。

### 1. 估价函数 $f(n)$
这是启发式搜索的灵魂公式：
$$f(n) = g(n) + h(n)$$
*   **$g(n)$**：**已经付出的代价**。从初始节点 $S_0$ 走到当前节点 $n$ 的实际耗费。（代表“过去”）
*   **$h(n)$**：**预计还要付出的代价**。从当前节点 $n$ 到目标节点 $G$ 的估计耗费。（代表“未来”，即启发信息）
*   **$f(n)$**：经过节点 $n$ 的解路径的总代价估计。

### 2. A* 算法 (A-Star Algorithm) ★★★★★
这是人工智能历史上最著名的搜索算法，考试必考，大题小题都可能出现。

#### (1) 算法流程
1.  把初始节点 $S$ 放入 OPEN 表，计算其 $f(S)$。
2.  如果 OPEN 表为空，失败退出。
3.  从 OPEN 表中取出 **$f(n)$ 值最小** 的节点 $n$ 放入 CLOSED 表。
4.  判断 $n$ 是否为目标：如果是，成功退出，回溯路径。
5.  扩展 $n$ 的所有子节点。对于每个子节点 $m$：
    *   计算 $f(m) = g(m) + h(m)$。
    *   如果 $m$ 既不在 OPEN 也不在 CLOSED：加入 OPEN。
    *   如果 $m$ 已经在 OPEN：比较新的 $f$ 值和旧的 $f$ 值，如果新的更小，更新 OPEN 中的值和父指针。
    *   如果 $m$ 已经在 CLOSED：比较新的 $f$ 值和旧的 $f$ 值，如果新的更小，将 $m$ 移回 OPEN 并更新值（这一步在单调性限制下可省略）。
6.  跳回步骤2。

#### (2) 核心性质：可采纳性 (Admissibility)
这是 A* 算法最关键的数学性质。
*   **定义**：若启发函数 $h(n)$ 满足 **$h(n) \le h^*(n)$**，则 A* 算法一定能找到**最优解**。
    *   $h^*(n)$：从 $n$ 到目标的**真实**最小代价（上帝视角才知道的值）。
    *   $h(n)$：我们设计的估计值。
*   **通俗解释**：你的估计不能“冒进”，必须“保守”或“乐观”。你估计的距离（$h$）必须小于等于真实距离（$h^*$）。
    *   *例子*：地图导航。两点之间直线最短。如果用“直线距离”作为 $h(n)$，因为直线距离永远 $\le$ 真实路网距离，所以这个 $h(n)$ 是可采纳的，A* 保证找到最短路。

#### (3) h(n) 的极端情况分析
*   **若 $h(n) = 0$**：$f(n) = g(n)$。算法退化为 **Dijkstra算法** 或 **宽度优先搜索 (BFS)**（若边权为1）。虽然保证最优，但搜索范围极大，效率最低。
*   **若 $h(n) = h^*(n)$**：这是完美的估计。算法将沿着最优路径一步不差地走到终点，效率最高，没有多余搜索。
*   **若 $h(n) > h^*(n)$**：不再满足可采纳性。搜索速度可能很快（贪婪），但**不能保证找到最优解**。

#### (4) 单调性 (Monotonicity) / 一致性
*   **定义**：对于任意节点 $n$ 和其子节点 $m$，满足 $h(n) \le c(n, m) + h(m)$。（类似三角形不等式）。
*   **意义**：如果 $h(n)$ 满足单调性，那么 A* 算法一旦选定了一个节点进行扩展，就已经找到了到达该节点的最优路径。这意味着我们**永远不需要从 CLOSED 表中重新捞出节点**，极大地简化了算法实现。

---

## 第四部分：局部搜索与优化 (Local Search)
**——“只看脚下的登山者”**

当状态空间大到无法存储完整路径，或者我们只关心最终状态而不关心路径时，使用局部搜索。

### 1. 爬山法 (Hill Climbing)
*   **策略**：贪婪地向周围最好的邻居移动。如果所有邻居都比自己差，就停止。
*   **比喻**：在大雾中登山，只能看清脚下一小块地，哪边高往哪边走。
*   **三大缺陷（常考简答）**：
    1.  **局部最优 (Local Maxima)**：爬到了一个小山头，以为是最高峰，其实旁边还有珠穆朗玛峰（但要先下山才能过去，爬山法不许下山）。
    2.  **高原 (Plateau)**：平原区域，四周都一样高，迷失方向。
    3.  **山脊 (Ridge)**：像刀背一样的山脊，虽然整体是向上的，但每一步的左右方向都是向下的，导致无法前进。

### 2. 模拟退火算法 (Simulated Annealing, SA)
*   **策略**：为了解决爬山法陷入局部最优的问题，允许算法以一定概率**接受“变差”的移动**（下山）。
*   **物理背景**：模拟金属退火过程。温度 $T$ 高时，分子运动剧烈，容易接受差解；温度 $T$ 降低，趋于稳定，只接受好解。
*   **Metropolis 准则 (核心公式)**：
    设 $\Delta E = E_{new} - E_{old}$ （能量差，目标是能量最小化）。
    *   若 $\Delta E < 0$（新状态更好）：**100% 接受**。
    *   若 $\Delta E > 0$（新状态更差）：以概率 $P = e^{-\Delta E / T}$ 接受。
    *   *关键点*：$T$ 越大，接受差解概率越大；$\Delta E$ 越小（差得不多），接受概率越大。

---

## 第五部分：博弈搜索 (Adversarial Search)
**——“你死我活的零和游戏”**

主要用于双人、完备信息、零和博弈（如：五子棋、象棋、围棋）。

### 1. 极大极小过程 (Minimax Algorithm)
*   **角色**：
    *   **MAX (我方)**：想让估价函数值最大。
    *   **MIN (对手)**：想让估价函数值最小（假设对手极度聪明，总能走出最狠的一步）。
*   **流程**：
    1.  生成博弈树到一定深度。
    2.  计算叶子节点的估值。
    3.  **倒推**：
        *   在 MIN 层，父节点取子节点中的**最小值**。
        *   在 MAX 层，父节点取子节点中的**最大值**。
    4.  根节点的值即为当前最佳策略的估值。

### 2. Alpha-Beta 剪枝 (Alpha-Beta Pruning) ★必考计算★
为了提高 Minimax 的效率，没必要搜索那些“注定不会被选中”的分支。

*   **两个参数**：
    *   **$\alpha$ (Alpha)**：MAX 节点目前发现的**最好（最大）**的下界。即“我至少能拿这么多分”。（初值 $-\infty$）
    *   **$\beta$ (Beta)**：MIN 节点目前发现的**最好（最小）**的上界。即“对手最多只让我拿这么多分”。（初值 $+\infty$）
*   **剪枝规则**：
    1.  **Beta 剪枝 (发生在 MIN 层)**：如果当前 MIN 节点发现了一个子节点的值 $v \le \alpha$（比MAX爸爸已经有的保底值还小），那么MAX爸爸绝不会选这条路，**剪掉**该节点后续所有子节点。
    2.  **Alpha 剪枝 (发生在 MAX 层)**：如果当前 MAX 节点发现了一个子节点的值 $v \ge \beta$（比MIN爷爷已经发现的上限还大），那么MIN爷爷绝不会让局面发展到这一步，**剪掉**后续。
*   **口诀**：
    *   MAX 更新 $\alpha$，若 $v \ge \beta$，剪枝。
    *   MIN 更新 $\beta$，若 $v \le \alpha$，剪枝。

---

# 📝 精选配套试题与详解 (下部)

## 题型一：A* 算法路径规划 (20分)
**题目**：
如下图所示的城市地图（假设图结构），边上的数字是实际距离 $g(n)$。括号内的数字是该城市到目标城市 G 的直线距离 $h(n)$。
起点 S，终点 G。
S -> A (2), S -> B (5)
A -> C (2), A -> D (4)
B -> D (1), B -> E (4)
C -> G (4)
D -> G (2)
E -> G (3)
各点 $h$ 值：$h(S)=6, h(A)=5, h(B)=4, h(C)=4, h(D)=2, h(E)=3, h(G)=0$。
请写出 A* 算法搜索过程，列出每一步 OPEN 表和 CLOSED 表的变化。

**答案与解析**：
**初始状态**：
*   OPEN = { (S, f=0+6=6) }
*   CLOSED = { }

**Step 1**：取出 S。扩展 S 的邻居 A, B。
*   A: $g=2, h=5, f=7$. 父节点 S。
*   B: $g=5, h=4, f=9$. 父节点 S。
*   OPEN = { (A, 7), (B, 9) } （按f排序）
*   CLOSED = { S }

**Step 2**：取出 A (f最小)。扩展 A 的邻居 C, D。
*   C: $g=2+2=4, h=4, f=8$. 父节点 A。
*   D: $g=2+4=6, h=2, f=8$. 父节点 A。
*   OPEN = { (C, 8), (D, 8), (B, 9) } （C和D的f相同，顺序随意，假设先C）
*   CLOSED = { S, A }

**Step 3**：取出 C。扩展 C 的邻居 G。
*   G: $g=4+4=8, h=0, f=8$. 父节点 C。
*   OPEN = { (G, 8), (D, 8), (B, 9) }
*   CLOSED = { S, A, C }

**Step 4**：取出 G。
*   G 是目标！算法结束。
*   注意：此时虽然 D 的 f 也是 8，但我们通常设定“目标优先”或只要取出目标就停止。
*   **路径回溯**：G -> C -> A -> S。
*   **最终路径**：S -> A -> C -> G，总代价 8。

*(注：如果 Step 2 选了 D，路径可能是 S->A->D->G，代价也是 2+4+2=8。A* 能找到所有最优路径之一)*

## 题型二：Alpha-Beta 剪枝 (15分)
**题目**：
给定如下博弈树（图略，文字描述）：
根节点 A (MAX)。
A 有子节点 B, C。
B (MIN) 有子节点 D (3), E (5), F (6)。
C (MIN) 有子节点 G (1), H (2), I (4)。
请按从左到右的顺序进行 Alpha-Beta 剪枝，说明哪些节点被剪枝了，并求出根节点的值。

**答案与解析**：
1.  **访问 B 分支**：
    *   D = 3。B 是 MIN 节点，当前值 $\le 3$。更新 $\beta = 3$。
    *   E = 5。5 > 3，对 MIN 没用。
    *   F = 6。6 > 3，对 MIN 没用。
    *   B 的最终值为 3。
2.  **回到 A (MAX)**：
    *   A 收到 B 的值 3。A 是 MAX 节点，当前值 $\ge 3$。更新 $\alpha = 3$。
    *   这意味着：A 至少能拿 3 分。如果右边 C 分支给出的值小于 3，A 就不会选 C。
3.  **访问 C 分支**：
    *   G = 1。C 是 MIN 节点，C 会选 1（或者更小）。
    *   此时 C 的临时值 1 $\le \alpha$ (3)。
    *   **触发 Alpha 剪枝！**
    *   解释：因为 C 是 MIN 节点，它一定会选 $\le 1$ 的值传给 A。而 A 已经在左边拿到了 3 分，绝对不会选 C 这条路（$3 > 1$）。所以 C 下面剩下的 H 和 I 根本不用看了。
4.  **结论**：
    *   **剪枝节点**：H, I。
    *   **根节点 A 的值**：3。

## 题型三：遗传算法概念辨析 (10分)
**题目**：
1.  为什么说变异操作对于防止遗传算法“早熟收敛”非常重要？
2.  在适应度函数设计中，如果 $f(x)$ 可能为负数，应该怎么处理？

**答案**：
1.  **变异的重要性**：
    遗传算法主要靠“选择”和“交叉”来利用已有的优秀基因。但如果初期种群多样性不够，或者很快收敛到某个局部最优解，整个种群的基因会变得非常相似，交叉操作无法产生新的基因型，导致算法停滞在局部最优（早熟）。
    **变异**操作通过随机改变基因，引入了新的遗传物质（新的搜索方向），赋予了算法跳出局部最优陷阱的能力，保证了算法的**全局搜索性**。

2.  **适应度非负处理**：
    遗传算法的轮盘赌选择概率 $P_i = f_i / \sum f$ 要求 $f_i$ 必须非负。如果目标函数值可能为负，通常采用**平移法**或**指数变换**：
    *   $Fit(x) = f(x) + C_{min}$ （$C_{min}$ 是一个足够大的正数，确保结果 $>0$）。
    *   或者使用 $Fit(x) = e^{f(x)}$。

---

# 🎓 最终考前冲刺建议

1.  **画图大法**：做归结原理题、搜索题、博弈树题，千万不要吝啬纸笔，**画图**是理清思路最快的方法。
2.  **概念对号入座**：把第一章的“三大流派”和后面章节对应起来（符号主义->逻辑/专家系统，连接主义->神经网络，行为主义->遗传算法），这样能建立宏观框架。
3.  **细节决定成败**：
    *   A* 算法的 $f=g+h$ 别把 $g$ 漏了（漏了就变成贪婪搜索了）。
    *   Alpha-Beta 剪枝的判断条件 $\ge \beta$ 和 $\le \alpha$ 别搞反。
    *   D-S 理论算 $K$ 值时，要把所有空集的积加起来，最后别忘了除以 $1-K$。

希望这份复习全书能助你一臂之力，拿下高分！加油！
---
这是一份针对**第六章：智能计算及应用**的深度复习全书。本章内容在人工智能基础课程中占据极高地位，因为它解决的是传统数学方法无法解决的**NP难问题**和**复杂非线性优化问题**。

为了满足“极致详细”和“备考高分”的需求，我将从算法原理、数学模型、关键算子、改进策略到实际应用进行全方位拆解。

---

# 📘 第六章：智能计算及应用 (Intelligent Computing)

## 🌟 核心导读：什么是智能计算？
智能计算（Computational Intelligence, CI），又称软计算，是受自然界生物进化（Evolution）、群体行为（Swarm Behavior）和神经系统启发而设计的算法集合。
*   **核心特点**：不依赖问题的精确数学模型（无梯度），具有**鲁棒性**、**并行性**和**全局搜索能力**。
*   **两大支柱**：
    1.  **进化计算 (Evolutionary Computing)**：以**遗传算法 (GA)** 为代表，模拟“优胜劣汰”。
    2.  **群智能 (Swarm Intelligence)**：以**粒子群 (PSO)** 和 **蚁群 (ACO)** 为代表，模拟“群体协作”。

---

## 第一部分：进化算法与遗传算法 (Genetic Algorithm, GA)

### 1. 生物学背景与术语映射 (必考概念题)
GA 是把问题的解空间映射到生物的遗传空间。考题常考这种对应关系：

| 生物学术语 | 计算机/优化术语 | 解释 |
| :--- | :--- | :--- |
| **个体 (Individual)** | **解 (Solution)** | 一个具体的候选方案 |
| **染色体 (Chromosome)** | **编码 (Code)** | 解的表示形式（如二进制串 `1001`） |
| **基因 (Gene)** | **特征分量** | 编码中的某一位或某一段 |
| **适应度 (Fitness)** | **目标函数值** | 解的好坏程度（越大越好） |
| **种群 (Population)** | **解集** | 当前所有候选解的集合 |
| **进化 (Evolution)** | **迭代 (Iteration)** | 算法的一轮循环 |

### 2. 基本遗传算法 (SGA) 的五大要素
SGA 的执行流程是：**编码 $\rightarrow$ 初始化 $\rightarrow$ 评估 $\rightarrow$ 选择 $\rightarrow$ 交叉 $\rightarrow$ 变异 $\rightarrow$ 终止**。

#### (1) 编码方法 (Encoding) ★难点★
如何把现实问题变成计算机能处理的串？
*   **二进制编码 (Binary Encoding)**：
    *   *原理*：用 0 和 1 组成的字符串表示。例如 $x=10 \rightarrow 1010$。
    *   *优点*：操作简单，易于实现交叉变异；符合模式定理。
    *   *缺点*：**汉明悬崖 (Hamming Cliff)** 问题。
        *   *解释*：相邻整数在二进制上差别巨大。例如 $7(0111)$ 和 $8(1000)$，虽然数值只差1，但二进制所有位都变了。这会导致算法难以通过微调找到邻居。
    *   *解决方案*：**格雷码 (Gray Code)**。特点是相邻两个整数的编码只有一位不同（如 $7 \rightarrow 0100, 8 \rightarrow 1100$）。
*   **实数编码 (Real Encoding)**：
    *   直接用实数数组表示，适用于高维、高精度的连续函数优化。
    *   *例子*：$x = [1.2, 3.5, -0.4]$。

#### (2) 适应度函数 (Fitness Function)
*   **作用**：作为“指挥棒”，决定哪些个体能活下去。
*   **要求**：必须是**非负**的，且值越大越好。
*   **尺度变换 (Scaling)**：防止算法初期“早熟”（超级个体统治种群）或后期“停滞”（大家差别不大，选不出优劣）。
    *   **线性变换**：$F' = aF + b$。
    *   **指数变换**：$F' = e^{\alpha F}$（拉大差距）。

#### (3) 选择算子 (Selection)
目的是为了“优胜劣汰”。
*   **轮盘赌选择 (Roulette Wheel)**：最经典。
    *   *原理*：个体被选中的概率 $P_i$ 与其适应度 $f_i$ 成正比。
    *   *公式*：$P_i = \frac{f_i}{\sum_{j=1}^{N} f_j}$。
    *   *特点*：随机性强，适应度高的可能不被选中（虽然概率低）。
*   **锦标赛选择 (Tournament)**：
    *   *原理*：随机挑 $k$ 个个体打比赛，最牛的那个进入下一代。
    *   *特点*：效率高，易于并行化，常用于实数编码。
*   **最佳个体保存策略 (Elitism)**：
    *   *原理*：把历代出现过的**最强王者**直接复制到下一代，不进行交叉变异。
    *   *作用*：**保证算法最终收敛**（理论证明GA收敛的必要条件）。

#### (4) 交叉算子 (Crossover) —— “灵感的来源”
*   **地位**：遗传算法中**最重要**的算子，负责产生新个体，拓展搜索空间。
*   **单点交叉**：在染色体中间切一刀，交换后半部分。
*   **发生概率 $P_c$**：通常较大（0.4 ~ 0.9）。

#### (5) 变异算子 (Mutation) —— “防止死循环”
*   **地位**：辅助算子，负责维持种群多样性，防止**早熟收敛**（陷入局部最优）。
*   **位点变异**：以极小的概率（如 0.001 ~ 0.1）将二进制位取反（0变1，1变0）。
*   **重要性**：如果没有变异，一旦种群中所有个体的某一位都变成了1，那么无论怎么交叉，这一位永远是1，算法就“死”了。

---

### 3. 遗传算法的改进 (Advanced GA)

#### (1) 双倍体遗传 (Diploid)
*   **生物学原理**：显性基因和隐性基因。
*   **应用场景**：**动态环境**。当环境变化时，之前隐藏在隐性基因里的特征可能突然变成适应环境的显性特征（记忆功能）。

#### (2) 双种群/多种群 (Dual/Multi Population)
*   **原理**：不仅有一个种群在进化，而是多个“部落”同时进化。
*   **关键操作**：**移民 (Migration)**。每隔几代，不同部落之间交换几个优秀个体。
*   **作用**：打破平衡态，极大地提高了全局搜索能力。

#### (3) 自适应遗传算法 (AGA) ★重点算法★
*   **痛点**：固定不变的交叉概率 $P_c$ 和变异概率 $P_m$ 很难平衡“探索”和“开发”。
*   **Srinivas 提出的 AGA 思想**：
    *   **针对群体**：如果群体趋于一致（陷入局部最优），这就需要**增大** $P_c, P_m$ 来破坏现状；如果群体比较分散（正在快速进化），则**减小** $P_c, P_m$。
    *   **针对个体**：
        *   对于**优秀个体**（适应度 > 平均值）：给它**低**的 $P_c, P_m$，保护它，让它别乱变。
        *   对于**差劲个体**（适应度 < 平均值）：给它**高**的 $P_c, P_m$，甚至可以高达 0.5，要么淘汰，要么突变成强者。

---

## 第二部分：粒子群优化算法 (Particle Swarm Optimization, PSO)

### 1. 算法隐喻
模拟**鸟群觅食**。鸟儿们不知道食物在哪，但知道自己离食物多远，也知道**队友中谁离食物最近**。
*   **粒子 (Particle)**：每只鸟。没有体积和质量。
*   **属性**：只有**位置 ($x$)** 和 **速度 ($v$)**。

### 2. 核心公式 (考试必背，计算题核心)
在 $k+1$ 时刻，第 $i$ 个粒子的速度和位置更新公式：

**(1) 速度更新公式：**
$$v_{id}^{k+1} = \omega \cdot v_{id}^k + c_1 r_1 (p_{id}^k - x_{id}^k) + c_2 r_2 (p_{gd}^k - x_{id}^k)$$

**(2) 位置更新公式：**
$$x_{id}^{k+1} = x_{id}^k + v_{id}^{k+1}$$

**参数详解（简答题考点）：**
*   **$v_{id}^k$ (惯性部分)**：粒子保持原来运动状态的趋势。
    *   **$\omega$ (惯性权重)**：
        *   $\omega$ 大：飞得快，适合**全局搜索**（到处乱窜找大方向）。
        *   $\omega$ 小：飞得慢，适合**局部精细搜索**（在小范围内找最优解）。
        *   *策略*：通常采用**线性递减**策略，开始大（先粗略找），后来小（后精细找）。
*   **$c_1 r_1 (p_{id} - x_{id})$ (认知部分)**：粒子向**自己历史最好位置 ($pbest$)** 的学习。代表“自我经验”。
*   **$c_2 r_2 (p_{gd} - x_{id})$ (社会部分)**：粒子向**群体历史最好位置 ($gbest$)** 的学习。代表“社会协作”。
    *   若 $c_1=0$：**无私模型**（只听领导的），收敛快但易陷局部最优。
    *   若 $c_2=0$：**认知模型**（闭门造车），各搜各的，很难找到全局最优。

---

## 第三部分：蚁群算法 (Ant Colony Optimization, ACO)

### 1. 算法隐喻
模拟**蚂蚁觅食**。核心机制是**信息素 (Pheromone)** 的正反馈。
*   蚂蚁走过的路会留下信息素。
*   路径越短 $\rightarrow$ 蚂蚁往返次数越多 $\rightarrow$ 留下的信息素越浓。
*   后来的蚂蚁越倾向于走信息素浓的路 $\rightarrow$ 导致信息素更浓（正反馈）。

### 2. 关键参数与模型
以解决 **TSP (旅行商问题)** 为例：

**(1) 状态转移概率 (蚂蚁怎么选路?)**
蚂蚁 $k$ 从城市 $i$ 选择去城市 $j$ 的概率 $P_{ij}^k$：
$$P_{ij}^k = \frac{[\tau_{ij}(t)]^\alpha \cdot [\eta_{ij}(t)]^\beta}{\sum [\tau_{is}(t)]^\alpha \cdot [\eta_{is}(t)]^\beta}$$
*   **$\tau_{ij}$ (信息素)**：历史经验，大家都走这。
*   **$\eta_{ij}$ (启发信息)**：通常取距离的倒数 $1/d_{ij}$。代表贪婪思想（眼下哪个城市近就去哪个）。
*   **$\alpha$ (信息素因子)**：$\alpha$ 越大，蚂蚁越盲从大流，容易早熟。
*   **$\beta$ (启发因子)**：$\beta$ 越大，蚂蚁越短视（只看眼前距离），容易陷入局部最优。

**(2) 信息素更新 (路上的气味怎么变?)**
$$\tau_{ij}(t+1) = (1-\rho) \cdot \tau_{ij}(t) + \Delta \tau_{ij}$$
*   **$\rho$ (挥发系数)**：$1-\rho$ 表示残留。挥发是必须的！如果不挥发，旧路径的信息素会无限积累，新路径永远没机会被探索。
*   **$\Delta \tau_{ij}$ (增量)**：
    *   **Ant-Cycle 模型 (最常用)**：$\Delta \tau = Q / L_k$ ($Q$是常数，$L_k$是蚂蚁走完一圈的总长度)。这利用了**全局信息**，效果最好。
    *   Ant-Density / Ant-Quantity 模型：利用局部信息，效果一般。

---

## 第四部分：高分备考题库 (含详细解析)

### 题型一：概念辨析与简答 (20分)

**Q1: 为什么遗传算法中的变异操作概率 $P_m$ 通常取得很小，而交叉概率 $P_c$ 取得较大？**
*   **解析**：
    *   **交叉**是GA的主要搜索算子，它负责将父代的优良基因组合起来，产生新的个体，大幅度搜索解空间，所以概率要大（0.4-0.9）。
    *   **变异**是辅助算子，类似于生物界的基因突变。如果变异概率过大，算法就会退化为纯粹的**随机搜索**，失去了遗传进化的意义。变异的主要目的是防止单一基因统治种群（早熟），维持多样性。

**Q2: 比较粒子群算法 (PSO) 与遗传算法 (GA) 的异同。**
*   **解析**：
    *   **相同点**：都是基于群体的、随机的、启发式优化算法；都需要计算适应度；都初始化为随机种群。
    *   **不同点**：
        1.  **机制**：GA基于进化（交叉、变异、选择）；PSO基于模拟鸟群（速度-位置更新，跟踪极值）。
        2.  **信息保留**：PSO中的粒子有记忆（$pbest$），GA中的个体一般没有记忆（除非用Elitism）。
        3.  **参数**：GA调整$P_c, P_m$；PSO调整$w, c_1, c_2$。PSO通常收敛速度比GA快，但容易早熟。

**Q3: 在蚁群算法中，如果信息素挥发系数 $\rho$ 接近于0（不挥发）会发生什么？**
*   **解析**：如果 $\rho \to 0$，表示信息素几乎不挥发。这会导致历史上所有蚂蚁留下的信息素无限累积。结果是：算法将极度依赖早期的搜索结果，搜索范围被迅速限制在早期发现的路径上，新发现的更优路径因为信息素积累不够而很难被选中，导致算法**过早收敛于局部最优解**，失去探索新路径的能力。

### 题型二：计算题 (25分)

**Q4: 遗传算法-轮盘赌选择计算**
设种群有4个个体，适应度分别为 $f_1=10, f_2=2, f_3=8, f_4=30$。
1.  计算每个个体的选择概率 $P_i$。
2.  计算累积概率 $PP_i$。
3.  若产生随机数 $r=0.35$，选中哪个个体？

**解析**：
1.  总适应度 $\sum f = 10+2+8+30 = 50$。
    *   $P_1 = 10/50 = 0.2$
    *   $P_2 = 2/50 = 0.04$
    *   $P_3 = 8/50 = 0.16$
    *   $P_4 = 30/50 = 0.6$
2.  累积概率（画大饼）：
    *   $PP_1 = 0.2$
    *   $PP_2 = 0.2 + 0.04 = 0.24$
    *   $PP_3 = 0.24 + 0.16 = 0.40$
    *   $PP_4 = 0.40 + 0.6 = 1.0$
3.  随机数 $r=0.35$。
    *   因为 $0.24 < 0.35 \le 0.40$ (即 $PP_2 < r \le PP_3$)。
    *   所以选中 **个体3**。

**Q5: 粒子群 (PSO) 速度位置更新**
(此题与上一部分复习全书中的题型一致，但数值可能变化，重点掌握公式)
*   **关键点**：考试时注意看清 $pbest$ 和 $gbest$ 的坐标。千万别把 $x_{id}$ 算漏了。
*   **公式记忆**：$v_{new} = w v_{old} + 认知 + 社会$。

### 题型三：算法设计与应用 (15分)

**Q6: 请简述如何用遗传算法解决 TSP 问题（旅行商问题）。**
**解析**（按步骤给分）：
1.  **编码**：采用**整数排列编码**。例如5个城市，一条染色体为 `(1, 3, 5, 2, 4)`，代表走访顺序。
2.  **适应度函数**：目标是总路径 $D$ 越短越好。适应度 $Fit = 1/D$。
3.  **选择**：轮盘赌或锦标赛。
4.  **交叉 (难点)**：不能用普通的单点交叉（会产生重复城市，如 `1, 3, 3, 2, 4`）。必须用**部分匹配交叉 (PMX)** 或 **次序交叉 (OX)**，保证子代也是合法的城市排列。
5.  **变异**：**交换变异**（随机交换两个城市的位置）或 **逆转变异**（将一段路径逆序）。
6.  **终止**：达到最大代数。

---

### 💡 考前极速记忆卡 (Cheat Sheet)

1.  **GA三大算子**：选择、交叉、变异。
2.  **Schema定理**：低阶、短定义距、高平均适应度的模式在群体中呈指数增长。
3.  **PSO公式**：$v = wv + c_1r_1(p-x) + c_2r_2(g-x)$。
4.  **ACO核心**：正反馈（信息素越多越有人走）、挥发（防止死循环）。
5.  **AGA思想**：好的个体少变异（保护），差的个体多变异（淘汰/改进）。

这份复习资料覆盖了第六章95%以上的考点。只要掌握了上述的计算逻辑和核心概念，这部分的分数就能稳稳拿下！加油！
---
这是一份为您定制的**《人工智能基础》第七章：专家系统**的深度复习全书。

专家系统（Expert Systems, ES）是人工智能发展史上**第一个真正实现商业化应用**的领域，代表了**符号主义**（Symbolism）的巅峰。在期末考试中，这一章通常考察**系统架构**、**工作原理**以及**经典案例分析**。

为了达到“极致详细”的要求，我们将从历史背景、核心概念、内部构造、开发流程到经典实例进行地毯式讲解。

---

# 📘 第七章：专家系统 (Expert Systems)

## 第一部分：专家系统的起源与本质

### 1. 什么是专家系统？
*   **定义（费根鲍姆 E.A. Feigenbaum）**：专家系统是一种智能的计算机程序，它运用**知识**和**推理**来解决只有专家才能解决的复杂问题。
*   **通俗理解**：它不是一个普通的计算器，而是一个被灌输了顶级专家毕生经验的“数字大脑”。它不靠蛮力计算，而是靠“经验法则”来看病、找矿或修机器。

### 2. 专家系统的四大特点 (简答题考点)
1.  **专业性 (Expertise)**：拥有该领域的高水平知识（不仅是书本知识，还有专家的**启发式经验**）。
2.  **推理能力 (Reasoning)**：能利用知识进行推导，不仅能处理确定性问题，还能处理**不确定性**问题（如第四章学的C-F模型）。
3.  **启发性 (Heuristic)**：它不一定能遍历所有路径找到最优解，但能利用经验快速找到满意解。
4.  **透明性/解释能力 (Transparency/Explanation)**：**这是ES区别于神经网络的最大特征**。它能告诉你“为什么”得出这个结论（Why），以及“如何”得出的（How）。

### 3. 专家系统 vs 传统程序 (对比题考点)

| 维度 | 传统程序 (Conventional Programs) | 专家系统 (Expert Systems) |
| :--- | :--- | :--- |
| **核心要素** | 数据 + 算法 | **知识 + 推理** |
| **处理对象** | 数值计算、结构化数据 | 符号处理、概念、非结构化信息 |
| **控制与知识** | 纠缠在一起（代码即逻辑） | **分离**（知识库独立于推理机） |
| **运行结果** | 总是产生唯一正确解 | 可能产生满意解，允许有错误 |
| **解释功能** | 无（黑盒） | **有**（白盒，能解释思路） |
| **修改维护** | 难（需重写代码） | 易（只需修改知识库规则） |

---

## 第二部分：专家系统的核心架构 (The Anatomy) ★★★★★
这是本章的**绝对核心**，考试常要求**画图**或**解释各部分功能**。

一个典型的专家系统由 **6大部件** 组成。我们可以把它比喻成一个“虚拟医生”。

### 1. 知识库 (Knowledge Base) —— “医生的大脑/教科书”
*   **功能**：存储领域内的所有知识。
*   **内容**：
    *   **事实性知识**：基本概念、公理（如：体温39度算发烧）。
    *   **启发式知识**：专家的经验法则、直觉（如：IF 发烧 AND 咳嗽 THEN 可能是流感）。
*   **地位**：知识库的质量决定了专家系统的性能水平（即“知识是瓶颈”）。

### 2. 综合数据库 (Global Database) —— “病人的病历本”
*   *注：也称工作内存 (Working Memory) 或 黑板 (Blackboard)。*
*   **功能**：存储**当前**问题求解过程中的所有信息。
*   **内容**：
    *   用户的初始输入（如：病人主诉头痛）。
    *   推理过程中产生的中间结论（如：推断出免疫力低下）。
*   **动态性**：知识库是静态的（除非升级），综合数据库是动态的（随推理过程不断刷新）。

### 3. 推理机 (Inference Engine) —— “医生的思维逻辑”
*   **功能**：负责调度和控制整个系统的运行。它决定“何时”使用知识库中的“哪条规则”来处理综合数据库中的事实。
*   **核心任务**：
    *   **匹配**：拿事实去匹配规则的前提。
    *   **冲突消解**：如果多条规则同时满足，决定先用哪一条（策略：针对性、新鲜度等）。
    *   **执行**：将规则的结论加入综合数据库。
*   **推理策略**：正向推理（诊断）、逆向推理（验证）、混合推理。

### 4. 解释机构 (Explanation Facility) —— “医患沟通”
*   **功能**：回答用户的提问，解释推理过程。
*   **常见问题**：
    *   **Why?** （系统问你问题时，你问系统：你为什么要问我这个？）$\rightarrow$ 系统展示当前的推理路径和目标。
    *   **How?** （系统给出结论时，你问系统：你是怎么得出这个结论的？）$\rightarrow$ 系统展示激活的规则链条。

### 5. 知识获取机构 (Knowledge Acquisition Facility) —— “进修学习”
*   **功能**：主要提供给**知识工程师**和**领域专家**使用，用于向系统添加、修改或扩充知识。
*   **难点**：**“知识获取瓶颈”**。专家很难将潜意识的经验显性化表达出来。

### 6. 人机接口 (User Interface) —— “诊室大门”
*   **功能**：用户与系统交互的界面。
*   **趋势**：从早期的命令行到现在自然语言处理、图形化界面。

---

## 第三部分：专家系统的构建与开发

### 1. 关键角色
开发一个专家系统通常需要三类人合作：
1.  **领域专家 (Domain Expert)**：提供知识源（如老中医）。
2.  **知识工程师 (Knowledge Engineer)**：**架构师**。负责采访专家，把专家的知识翻译成计算机能懂的规则（IF-THEN），并选择合适的推理引擎。
3.  **程序员**：负责底层编码和界面实现。

### 2. 开发工具的演变
*   **第一代：通用编程语言**
    *   LISP, PROLOG, C++。效率高，但开发难度极大，一切从零开始。
*   **第二代：骨架系统 (Shells)**
    *   **概念**：把一个成功的专家系统（如MYCIN）的**知识库掏空**，保留推理机、解释机构和人机接口。
    *   *比喻*：就像给你一个空的Excel模板，你往里面填入新的公式和数据，它就能变成财务系统或考勤系统。
    *   *优点*：开发周期极短。
    *   *例子*：**EMYCIN** (Empty MYCIN), **KAS** (Knowledge Acquisition System)。
*   **第三代：通用开发环境**
    *   提供多种知识表示方法和推理机制的工具包（如CLIPS, JESS）。

### 3. 开发步骤 (一般了解)
问题定义 $\rightarrow$ 原型设计 $\rightarrow$ 扩充与完善 $\rightarrow$ 验证与测试 $\rightarrow$ 维护。

---

## 第四部分：经典专家系统案例 (必考)

这部分内容常出现在选择题或简答题中，要求对应**系统名称、应用领域、核心技术**。

### 1. DENDRAL (1968年) —— 鼻祖
*   **领域**：化学（推断有机化合物的分子结构）。
*   **地位**：世界上**第一个**专家系统。
*   **贡献**：证明了利用启发式知识可以解决复杂问题，确立了“知识工程”的方向。

### 2. MYCIN (1972-1976年) —— 最经典的教材
*   **领域**：医学（诊断血液感染病并开处方）。
*   **特点**：
    *   使用**产生式规则** (Rule-based)。
    *   引入了**C-F模型**（可信度方法）处理不确定性推理（第四章学过）。
    *   具有强大的解释功能。
    *   *衍生*：它的骨架系统叫 **EMYCIN**。

### 3. PROSPECTOR (1976年) —— 最赚钱
*   **领域**：地质勘探（寻找矿藏）。
*   **特点**：
    *   使用**语义网络**和产生式规则结合。
    *   使用**Duda主观贝叶斯方法**处理不确定性。
*   **战绩**：成功预测了华盛顿州某地的钼矿，价值数亿美元，证明了AI的经济价值。
*   *衍生*：它的骨架系统叫 **KAS**。

### 4. AM (Automated Mathematician)
*   **领域**：数学（发现数学概念和定理）。
*   **特点**：不仅是应用知识，还体现了机器的**学习**和**发现**能力。

### 5. XCON (R1)
*   **领域**：计算机配置（为DEC公司的VAX计算机装配零部件）。
*   **地位**：第一个在工业界获得巨大商业成功的专家系统。

---

## 第五部分：专家系统的优缺点分析

### 1. 优点
*   **高水平**：能达到甚至超过人类专家的水平。
*   **高效率**：比人类快，且不知疲倦。
*   **高可靠性**：不会像人类一样因为情绪、疲劳而犯错，且知识具有永久性（人类专家会退休）。
*   **易于传播**：由于知识与推理分离，易于复制和移植。

### 2. 缺点与局限性 (Bottlenecks)
1.  **知识获取瓶颈**：专家很难把潜意识的经验说清楚，且不同专家意见可能冲突。
2.  **知识面窄**：只能解决特定领域的狭窄问题，缺乏**常识 (Common Sense)**。
    *   *例子*：医疗专家系统可能知道所有病毒，但不知道“人死不能复生”。
3.  **脆弱性**：遇到超出知识库范围的问题，能力会断崖式下跌（不像人类能类比推理）。
4.  **推理能力弱**：大多基于浅层知识（经验规则），缺乏深层知识（因果机制、物理原理）。

---

# 📝 精选配套试题与详解

## 题型一：概念辨析题 (15分)
**题目**：
1.  简述专家系统与传统应用程序（如数据库管理系统）在处理对象和系统结构上的主要区别。
2.  什么是“专家系统骨架 (Shell)”？它有什么作用？

**答案**：
1.  **区别**：
    *   **处理对象**：传统程序主要处理数值和结构化数据；专家系统处理符号、概念及不确定性知识。
    *   **系统结构**：传统程序中，知识（逻辑）隐含在代码算法中，修改困难；专家系统中，**知识库与推理机分离**，知识库存储规则，推理机负责控制，修改知识只需更新知识库，无需重写程序。

2.  **骨架 (Shell)**：
    *   **定义**：指将一个成熟的专家系统（如MYCIN）中的具体领域知识（如医学规则）清空，只保留推理机、解释机构、人机接口等通用组件所形成的软件工具。
    *   **作用**：用户只需向骨架中填入新的领域的知识，就可以快速构建一个新的专家系统（如用EMYCIN构建肺病诊断系统PUFF），大大缩短开发周期。

## 题型二：架构分析题 (20分)
**题目**：
下图是专家系统的一般结构图，请填写①②③④处的内容，并简述部件③的功能。
[ 用户 ] <--> [ ① ] <--> [ ② ] <--> [ ③ ] <--> [ ④ ]
                           ^
                           |
                     [ 知识获取 ] <--> [ 专家 ]

*(注：脑补标准架构图)*

**答案**：
*   **填空**：
    *   ① **人机接口**
    *   ② **推理机** (有些图中推理机连接综合数据库和知识库)
    *   ③ **综合数据库** (或工作内存/黑板)
    *   ④ **知识库**
    *(注：具体的连接顺序视教材图示而定，但核心逻辑是：推理机是枢纽，连接知识库和综合数据库)*

*   **部件③（综合数据库）的功能**：
    综合数据库用于存储推理过程中所需的**初始证据**（用户输入的事实）以及推理产生的**中间结论**。它是动态的，随着推理的进行不断更新，反映了问题求解的当前状态。

## 题型三：案例分析题 (15分)
**题目**：
MYCIN 是人工智能历史上最著名的专家系统之一。请回答：
1.  MYCIN 主要用于解决什么领域的问题？
2.  它采用什么方法来处理知识的不确定性？
3.  它的推理方向主要是什么（正向/逆向）？

**答案**：
1.  **领域**：医疗诊断。主要用于诊断血液感染性疾病（细菌感染）并提供抗生素治疗方案。
2.  **不确定性处理**：采用了**可信度方法 (Certainty Factor, C-F模型)**。用 $CF(H,E)$ 来量化证据支持结论的程度。
3.  **推理方向**：主要采用**逆向推理 (Backward Chaining)**。先假设病人感染了某种细菌（目标），然后去寻找支持该假设的症状和化验数据（证据）。

## 题型四：逻辑推理题 (15分)
**题目**：
在一个基于产生式规则的专家系统中，现有如下规则库：
R1: IF A AND B THEN C
R2: IF C THEN D
R3: IF A AND E THEN F
综合数据库中的初始事实为：{A, B, E}。
请演示推理机如何利用**正向推理**得出结论 D 的过程。

**答案**：
**推理过程**：
1.  **第一轮扫描**：
    *   检查 R1：已知 A, B，满足 R1 前提。$\rightarrow$ 触发 R1，将结论 **C** 加入综合数据库。
    *   当前数据库：{A, B, E, **C**}
    *   检查 R3：已知 A, E，满足 R3 前提。$\rightarrow$ 触发 R3，将结论 **F** 加入综合数据库。
    *   当前数据库：{A, B, E, C, **F**}
2.  **第二轮扫描**：
    *   检查 R2：已知 C（上一轮推出的），满足 R2 前提。$\rightarrow$ 触发 R2，将结论 **D** 加入综合数据库。
    *   当前数据库：{A, B, E, C, F, **D**}
3.  **结束**：
    *   所有规则判断完毕，无法产生新结论。最终推导出结论 **D** (以及 F)。

---

### 💡 考前极速记忆清单 (Cheat Sheet)

1.  **核心定义**：知识 + 推理。
2.  **核心分离**：知识库（静态）与推理机（动态）分离。
3.  **四大特点**：专业、推理、启发、解释。
4.  **三大瓶颈**：知识获取难、知识面窄、推理能力弱（缺乏常识）。
5.  **四大金刚（案例）**：
    *   DENDRAL (化学，最早)。
    *   MYCIN (医疗，C-F模型，骨架EMYCIN)。
    *   PROSPECTOR (地质，找矿，Duda贝叶斯)。
    *   XCON (工业配置，R1，最成功商业化)。

掌握以上内容，这一章的考点基本可以全覆盖！祝复习顺利！

---
                    
这是一份为您精心打造的**《人工智能基础》第八章：人工神经网络（ANN）**的终极复习全书。

这一章是现代人工智能（特别是深度学习）的基石，也是连接主义（Connectionism）流派的核心。在期末考试中，这一章通常是**分值最高、难度最大**的部分，涉及**算法原理推导**、**计算题**以及**综合应用题**。

为了达到“极致详细”的效果，我将从**生物学基础**到**数学模型**，再到**BP算法推导**、**Hopfield网络**以及**深度学习（CNN/GAN）**进行全景式解析。

---

# 📘 第八章：人工神经网络 (Artificial Neural Networks)

## 第一部分：神经网络的起源与基础模型

### 1. 生物学灵感
人工神经网络并非凭空创造，而是对人类大脑皮层微观结构的模仿。
*   **生物神经元结构**：
    *   **树突 (Dendrites)**：输入端，接收来自其他神经元的信号。
    *   **细胞体 (Soma)**：处理中心，整合信号。
    *   **轴突 (Axon)**：输出端，传输信号。
    *   **突触 (Synapse)**：连接点，决定信号传递的强弱（对应人工网络中的**权值**）。
*   **赫布学习规则 (Hebb Rule)**：Donald Hebb在1949年提出。**“共同激发的神经元，其连接会增强。”** 这是神经网络学习机制的生物学基础。

### 2. M-P 神经元模型 (The M-P Neuron) ★基础考点★
1943年，McCulloch和Pitts提出了第一个形式化神经元模型。它是所有神经网络的原子单位。

**模型公式**：
$$y = f\left( \sum_{i=1}^{n} w_i x_i - \theta \right)$$
或者写成向量形式（将阈值 $\theta$ 作为偏置 $b$）：
$$y = f(\mathbf{W}^T \mathbf{X} + b)$$

**参数详解**：
*   $x_i$：来自第 $i$ 个突触的**输入信号**。
*   $w_i$：**连接权值 (Weight)**。$w > 0$ 表示兴奋，$w < 0$ 表示抑制。权值的大小决定了该输入的重要性。
*   $\sum$：**线性加权求和**。
*   $\theta$：**阈值 (Threshold)** 或 $b$ (Bias)。只有总输入超过这个门槛，神经元才会被激活。
*   $f(\cdot)$：**激活函数 (Activation Function)**。这是神经网络的灵魂，它引入了**非线性**因素，使得神经网络能解决复杂问题。

### 3. 常见的激活函数 (必背)
考试常考图像识别或填空，必须掌握其特性。

| 激活函数 | 图像特征 | 公式 | 优点 | 缺点 |
| :--- | :--- | :--- | :--- | :--- |
| **阶跃函数 (Step)** | 不是0就是1 | $f(x)=1, x\ge0; 0, x<0$ | 简单，二值逻辑 | 不连续，不可导，无法用于反向传播 |
| **Sigmoid (S型)** | 平滑的S曲线 | $f(x) = \frac{1}{1+e^{-x}}$ | 输出在(0,1)之间，可导 | **梯度消失**问题，收敛慢，非零均值 |
| **Tanh (双曲正切)** | 类似S型，但在原点对称 | $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | 输出在(-1,1)之间，零均值 | 依然有梯度消失问题 |
| **ReLU (线性整流)** | 折线 (x>0时为x) | $f(x) = \max(0, x)$ | **计算快，解决梯度消失** | Dead ReLU问题 (部分神经元永久坏死) |

---

## 第二部分：BP 神经网络 (Back Propagation Network) ★★★★★

这是本章的**绝对核心**，考试中关于神经网络的计算题和原理题，90%都出自这里。

### 1. 核心概念
*   **定义**：一种**多层前馈**神经网络，采用**误差反向传播**算法进行训练。
*   **拓扑结构**：
    *   **输入层 (Input Layer)**：只负责传数据，不做计算。
    *   **隐层 (Hidden Layer)**：一层或多层，负责提取特征，进行非线性变换。
    *   **输出层 (Output Layer)**：输出预测结果。
    *   *注意：层与层之间全连接，同一层神经元之间无连接。*

### 2. BP 算法的工作流程 (The Learning Process)
可以把它想象成一个“射击训练”的过程：瞄准（正向） $\rightarrow$ 报靶（算误差） $\rightarrow$ 修正姿势（反向传导）。

#### **第一阶段：正向传播 (Forward Propagation)**
输入信号从输入层经过隐层逐层处理，传向输出层。
*   若隐层神经元 $j$ 的输入为 $I_j$，输出为 $H_j$：
    $$I_j = \sum w_{ij} x_i - \theta_j$$
    $$H_j = f(I_j)$$
*   直到计算出输出层的实际输出 $Y_k$。

#### **第二阶段：反向传播 (Backward Propagation)**
比较实际输出 $Y$ 与期望输出 $D$（标签）。
1.  **计算误差**：定义损失函数（通常是均方误差 MSE）：
    $$E = \frac{1}{2} \sum (d_k - y_k)^2$$
2.  **误差反传**：如果误差大于允许范围，就将误差信号由输出层 $\rightarrow$ 隐层 $\rightarrow$ 输入层逐层反传。
3.  **权值修正**：根据**梯度下降法 (Gradient Descent)**，沿着误差减小最快的方向修改权值。
    *   核心公式（权值调整量）：
        $$\Delta w = -\eta \frac{\partial E}{\partial w}$$
        *   $\eta$：**学习率 (Learning Rate)**。太大容易震荡，太小收敛太慢。
        *   $\frac{\partial E}{\partial w}$：误差对权值的偏导数（梯度）。

### 3. BP 网络的优缺点 (简答题常客)
*   **优点**：
    1.  **通用逼近性**：只要隐层节点足够多，一个三层BP网络可以逼近任意连续函数（BP定理）。
    2.  **非线性映射**：能处理复杂的非线性关系。
*   **缺点**：
    1.  **收敛速度慢**：需要训练成千上万次。
    2.  **局部极小值 (Local Minima)**：梯度下降法容易陷在“山腰的坑里”，到不了“山脚（全局最优）”。
    3.  **过拟合 (Overfitting)**：死记硬背训练数据，泛化能力差。
    4.  **结构难定**：隐层节点数选多少？主要靠经验公式或试凑。

---

## 第三部分：Hopfield 神经网络
**——“具有记忆功能的反馈网络”**

与BP网络（前馈）不同，Hopfield是**反馈型（Recurrent）**网络。

### 1. 结构特点
*   **全连接**：每一个神经元都和其他所有神经元连接。
*   **对称权重**：$w_{ij} = w_{ji}$。
*   **无自反馈**：$w_{ii} = 0$（自己不连自己）。
*   **二值状态**：神经元状态通常只有 $1$（激活）和 $-1$（抑制）。

### 2. 核心原理：能量函数 (Energy Function)
这是Hopfield网络最天才的设计。
*   定义网络的能量函数 $E$：
    $$E = -\frac{1}{2} \sum \sum w_{ij} x_i x_j + \sum \theta_i x_i$$
*   **稳定性定理**：在异步工作方式下，随着神经元状态的演化，网络的能量 $E$ **总是单调递减或保持不变**，最终达到一个**极小值（稳定状态）**。
*   **物理隐喻**：就像一个球在凹凸不平的曲面上滚动，最终一定会停在某个低洼处（吸引子）。

### 3. 两大应用
1.  **联想记忆 (Associative Memory)**：
    *   将需要记忆的模式（如一张图片）设计为网络的**能量极小点（吸引子）**。
    *   当输入一个残缺或有噪声的模式时（就像把球放在坑附近），网络会自动演化到对应的稳定状态（球滚进坑底），从而恢复出完整的记忆。
2.  **优化计算 (Optimization)**：
    *   解决 **TSP (旅行商问题)**。
    *   将问题的目标函数转化为网络的能量函数，问题的最优解对应能量的最小点。

---

## 第四部分：深度学习 (Deep Learning) —— 神经网络的进阶

### 1. 卷积神经网络 (CNN)
专门为处理网格数据（如图像）而生。
*   **核心层级**：
    1.  **卷积层 (Convolution Layer)**：使用**卷积核 (Filter)** 提取局部特征（如边缘、纹理）。
        *   *关键特性*：**局部连接**（看局部）和 **权值共享**（同一个卷积核扫遍全图，大大减少参数量）。
    2.  **池化层 (Pooling Layer)**：也叫下采样层。
        *   *作用*：降低数据维度，减少计算量，防止过拟合，保持特征的平移不变性。
        *   *方法*：最大池化 (Max Pooling)、平均池化。
    3.  **全连接层 (Fully Connected Layer)**：位于最后，将提取的特征拉平，进行分类输出。

*   **经典模型**：**LeNet-5**（最早用于识别手写数字）。

### 2. 生成对抗网络 (GAN)
由Goodfellow提出，包含两个对抗的网络：
*   **生成器 (Generator, G)**：造假者。输入随机噪声，试图生成逼真的样本（如假图片）。
*   **判别器 (Discriminator, D)**：警察。负责判断输入的样本是真实的还是G生成的。
*   **训练过程**：零和博弈。G 努力骗过 D，D 努力不被骗。最终达到纳什均衡，G 生成的图片以假乱真。

---

# 📝 精选配套试题与详解

## 题型一：BP算法计算题 (20分)
**题目**：
设有一个简单的单层感知机（不含隐层），输入层有两个节点 $x_1, x_2$，输出层有一个节点 $y$。
激活函数为 Sigmoid 函数 $f(x) = \frac{1}{1+e^{-x}}$。
连接权值 $w_1 = 0.5, w_2 = -0.5$，阈值 $\theta = 0.2$。
学习率 $\eta = 0.5$。
给定一个训练样本：输入 $X = (1, 0)$，期望输出 $d = 1$。
请计算：
1.  前向传播后的实际输出 $y$。
2.  进行一次反向传播后，权值 $w_1$ 的更新值。

**答案与解析**：

**1. 正向传播计算**：
*   计算净输入 $net$：
    $$net = w_1 x_1 + w_2 x_2 - \theta = 0.5 \times 1 + (-0.5) \times 0 - 0.2 = 0.3$$
*   计算输出 $y$：
    $$y = f(net) = \frac{1}{1 + e^{-0.3}} \approx \frac{1}{1 + 0.7408} \approx \frac{1}{1.7408} \approx 0.574$$

**2. 反向传播更新权值**：
*   Sigmoid函数的导数性质：$f'(x) = f(x)(1-f(x))$。
    所以 $f'(net) = y(1-y) = 0.574 \times (1 - 0.574) \approx 0.2445$。
*   计算输出层误差梯度 $\delta$：
    $$\delta = (d - y) \cdot f'(net) = (1 - 0.574) \times 0.2445 = 0.426 \times 0.2445 \approx 0.104$$
*   计算权值调整量 $\Delta w_1$：
    $$\Delta w_1 = \eta \cdot \delta \cdot x_1 = 0.5 \times 0.104 \times 1 = 0.052$$
*   更新权值 $w_1^{new}$：
    $$w_1^{new} = w_1^{old} + \Delta w_1 = 0.5 + 0.052 = 0.552$$

**结果**：实际输出为 0.574，更新后的 $w_1$ 为 0.552。

---

## 题型二：Hopfield网络分析 (15分)
**题目**：
对于一个离散型 Hopfield 神经网络：
1.  如果其权值矩阵 $W$ 对角线元素不为0（即 $w_{ii} \neq 0$），网络是否一定稳定？为什么？
2.  该网络如何实现“联想记忆”功能？请简述过程。

**答案**：
1.  **稳定性分析**：
    如果不满足 $w_{ii} = 0$ 或 $w_{ij} = w_{ji}$ 的条件，网络**不一定**稳定。
    Hopfield 网络的稳定性证明依赖于能量函数 $E$ 的单调递减性。如果 $w_{ii} \neq 0$（有自反馈），在状态更新过程中，能量的变化量 $\Delta E$ 可能大于0，导致能量不减反增，网络可能在几个状态之间产生振荡，无法收敛到稳定状态。

2.  **联想记忆过程**：
    *   **存储阶段**：将需要记忆的模式（如标准图像）编码为向量，根据 Hebb 规则计算权值矩阵 $W$，使得这些模式对应能量函数的**极小值点（吸引子）**。
    *   **回忆阶段**：当输入一个受损或模糊的模式（作为初始状态）时，由于它位于某个吸引子的吸引域内，网络会根据动态方程不断演化，能量不断降低，最终落入该吸引子（稳定状态）。这个稳定状态就是网络“联想”出的完整记忆。

---

## 题型三：CNN 概念填空与简答 (15分)
**题目**：
1.  在卷积神经网络中，假设输入图像大小为 $32 \times 32$，使用 $5 \times 5$ 的卷积核，步长（Stride）为 1，不使用填充（Padding）。则卷积后的特征图大小为 \_\_\_\_\_\_\_\_\_。
2.  CNN 中“权值共享”是什么意思？有什么好处？

**答案**：
1.  **计算**：
    公式：$Output = (Input - Kernel + 2 \times Padding) / Stride + 1$
    代入：$(32 - 5 + 0) / 1 + 1 = 28$。
    **答案**：$28 \times 28$。

2.  **权值共享**：
    *   **含义**：在卷积层中，同一个卷积核（Filter）在遍历整个图像进行卷积运算时，其参数（权重）是固定不变的。也就是说，图像上不同位置的特征提取使用的是同一组参数。
    *   **好处**：
        1.  **大大减少了参数数量**，降低了模型复杂度，防止过拟合。
        2.  赋予了网络**平移不变性**（Translation Invariance），即无论特征出现在图像的哪个位置，都能被同一个卷积核识别出来。

---

## 题型四：综合论述题 (20分)
**题目**：
请对比 **BP 神经网络** 和 **深度学习（如CNN）**。为什么在很长一段时间内 BP 网络陷入低潮，而深度学习近年来却爆发式增长？

**答案要点**：
1.  **结构深度**：传统 BP 网络通常只有 1-2 个隐层（浅层），而深度学习（如 CNN）通常有几十甚至上百个隐层。
2.  **梯度消失问题**：传统 BP 使用 Sigmoid 等饱和激活函数，层数多了之后，反向传播时梯度会逐层衰减，导致底层参数无法有效更新（梯度消失）。深度学习通过引入 **ReLU** 激活函数、**残差网络 (ResNet)** 等技术解决了这个问题。
3.  **特征提取方式**：
    *   BP 网络：通常需要人工设计特征（Feature Engineering），输入的是加工好的特征向量，对原始数据处理能力差。
    *   深度学习：能够从原始数据（如像素）中**自动学习特征**，且特征是分层抽象的（低层边缘 $\to$ 中层形状 $\to$ 高层物体），表达能力极强。
4.  **数据与算力**：
    *   过去：数据量少，算力（CPU）不足，深层网络容易过拟合且训练太慢。
    *   现在：大数据时代提供了海量训练样本（解决过拟合），**GPU** 的出现解决了大规模矩阵运算的算力瓶颈。

---

### 💡 考前极速复习清单 (Cheat Sheet)

1.  **M-P模型**：$y = f(WX+b)$。
2.  **BP算法核心**：梯度下降，反向传播误差。$\Delta w = -\eta \frac{\partial E}{\partial w}$。
3.  **Hopfield**：反馈网络，能量函数 $E$，用于联想记忆和TSP。
4.  **CNN三宝**：局部连接、权值共享、池化。
5.  **激活函数**：Sigmoid（易梯度消失），ReLU（主流，快）。

第八章是AI的重中之重，掌握了这里，不仅考试能拿高分，对理解现代AI技术也至关重要！加油！
---
这是一份基于南京理工大学霍雨翀老师的《人工智能基础》第九章课件的**深度解析复习全书**。

本章重点在于从**个体（智能体/Agent）**到**群体（多智能体系统/MAS）**的跨越，探讨了计算机如何从“被动执行指令的工具”进化为“具有自主性、社会性的智能实体”。

---

# 第九章：智能体与多智能体系统 (Agents and Multi-Agent Systems)

## 🔍 导读：为什么需要智能体？
在传统的软件工程中，程序是被动的，只有在用户调用时才工作。但在复杂的网络环境（如物联网、云计算、电子商务）中，我们需要软件能够：
1.  **主动服务**：不需要人踢一下才动一下。
2.  **适应环境**：环境变了，软件能自己调整策略。
3.  **互相合作**：一个软件搞不定，能找其他软件帮忙。

这就是**智能体（Agent）**和**多智能体系统（MAS）**诞生的背景。

---

## 9.1 智能体的概念与结构

### 9.1.1 智能体（Agent）的定义
**核心定义**：Agent是一个计算实体（程序或物理设备），它**嵌入**在特定的环境中，能够通过**传感器**感知环境，并通过**效应器**（执行器）自主地作用于环境，以实现设计目标。

*   **形象理解**：
    *   **传统程序**：像一个计算器，你输入 `1+1`，它输出 `2`。它不管周围发生了什么。
    *   **Agent**：像一个自动驾驶汽车。它不仅要计算路径，还要通过摄像头（传感器）看路况，通过方向盘刹车（效应器）控制车，目的是安全到达终点。

*   **交互闭环**：
    1.  **感知 (Perception)**：环境 $\rightarrow$ 传感器 $\rightarrow$ Agent
    2.  **决策 (Decision)**：Agent内部处理
    3.  **作用 (Action)**：Agent $\rightarrow$ 执行器 $\rightarrow$ 环境

### 9.1.2 智能体的四大特性（核心考点）
为了区分普通程序和Agent，我们定义了以下四个关键特性（Wooldridge & Jennings 定义）：

1.  **自主性 (Autonomy)**：
    *   **含义**：Agent有自己的控制权，不受人直接干预。它有自己的内部状态和知识。
    *   *例子*：火星探测车在通讯延迟情况下，自己决定避开障碍物，而不是等地球指令。
2.  **反应性 (Reactivity)**：
    *   **含义**：能够实时感知环境变化，并及时做出响应。
    *   *例子*：恒温器感知温度低了，立刻启动加热器。
3.  **社会性 (Social Ability)**：
    *   **含义**：Agent不是孤岛，它能通过通信语言（如ACL）与其他Agent或人类进行交互（协作、协商、竞争）。
    *   *例子*：两个送货机器人协商谁去送哪个包裹。
4.  **进化性/预动性 (Pro-activeness/Evolution)**：
    *   **含义**：不仅是被动反应，还能由目标驱动采取主动行为；并且能在交互中学习，适应环境。
    *   *例子*：私人助理软件观察到你每天早上9点开会，主动在8:50提醒你，并根据你的反馈调整提醒时间。

### 9.1.3 智能体的结构
公式：**Agent = 体系结构 (Architecture) + 程序 (Program)**
*   **体系结构**：硬件或软件框架，为Agent提供感知、通信和执行的物理/逻辑基础。
*   **程序**：实现Agent功能的具体算法逻辑。

#### 三种典型的Agent体系结构：
1.  **反应式 Agent (Reactive Agent)**
    *   **原理**：“刺激-响应”模式。没有复杂的内部推理模型，直接将感知映射到动作。
    *   **结构**：传感器 $\rightarrow$ 当前世界状态 $\rightarrow$ 条件-动作规则库 $\rightarrow$ 动作。
    *   **优点**：响应速度极快，鲁棒性好。
    *   **缺点**：不够智能，难以处理这就需要长远规划的任务。
    *   *例子*：昆虫的避障行为。

2.  **慎思式 Agent (Deliberative Agent)**
    *   **原理**：基于知识的符号AI系统。拥有环境的显式模型（世界模型），通过逻辑推理来规划行动。
    *   **结构**：感知 $\rightarrow$ 建模 $\rightarrow$ 规划 $\rightarrow$ 决策 $\rightarrow$ 行动。通常包含**BDI模型**（Belief信念、Desire愿望、Intention意图）。
    *   **优点**：能解决复杂问题，有长远目标。
    *   **缺点**：计算量大，反应慢。

3.  **复合式/混合式 Agent (Hybrid Agent)**
    *   **原理**：结合了前两者的优点。分层结构，下层处理紧急反应，上层处理长远规划。
    *   **模块**：
        *   **感知/执行**：与环境交互。
        *   **反射模块**：处理紧急情况（反应式）。
        *   **建模/规划/决策**：处理复杂任务（慎思式）。
        *   **通信/协作/协商**：处理社会性交互。

### 9.1.7 Agent 的应用领域
*   **电信**：网络负载均衡，故障自动修复。
*   **电子商务**：自动价格谈判，兴趣匹配推荐。
*   **用户助理**：邮件过滤，日程安排。
*   **决策支持**：数据挖掘，危机预警。
*   **移动计算**：在网络不稳时离线工作，联网后同步。

---

## 9.2 多智能体系统的概念与结构

### 9.2.1 什么是多智能体系统 (MAS)？
单个Agent能力有限，**MAS (Multi-Agent System)** 是由多个Agent组成的集合，它们协同工作以解决单个Agent无法解决的复杂问题。

**MAS 的核心特点**：
1.  **分布性**：数据、控制、知识都是分布的，没有全局控制中心。
2.  **自主性**：每个成员都是独立的。
3.  **协作性**：通过通信和协调解决冲突，提高效率。
4.  **异构性**：系统里可以混有不同类型的Agent（如专家系统A和神经网络系统B一起工作）。

### 9.2.2 MAS 的基本模型
1.  **BDI模型**：基于信念、愿望、意图的理性推理模型。
2.  **协商模型**：解决冲突的核心，如合同网。
3.  **协作规划模型**：多个Agent如何制定不冲突的行动计划。
4.  **自协调模型**：适应环境变化的自组织能力。

### 9.2.3 MAS 的体系结构（组织形式）
1.  **网络结构 (Network)**：
    *   **特点**：所有Agent地位平等，两两之间可以直接通信。
    *   **适用**：规模较小的系统。
2.  **联盟结构 (Federation)**：
    *   **特点**：引入**“中介”**或**“协助者” (Facilitator)**。Agent不直接找对方，而是找中介，中介负责路由和撮合。
    *   **优点**：扩展性好，Agent不需要知道所有其他人的地址，适合大型跨网系统。
3.  **黑板结构 (Blackboard)**：
    *   **特点**：非直接通信。大家把信息写在一个公共的“黑板”上，也从黑板上读取信息。
    *   **优点**：实现局部数据共享，适合解决复杂的协作求解问题。

---

## 9.3 多智能体系统的通信

**通信是MAS的基石**。没有通信，就是一堆孤独的程序，算不上系统。

### 9.3.1 通信过程与类型
**通信过程**：发送方意图 $\rightarrow$ 编码 $\rightarrow$ 发送 $\rightarrow$ 传输媒介 $\rightarrow$ 接收 $\rightarrow$ 解码 $\rightarrow$ 接收方理解。

**通信类型**：
1.  **共享存储区 (Tell/Ask)**：
    *   Agent通过读写共享的知识库来交互。
    *   *缺点*：耦合度太高，容易造成瓶颈。
2.  **消息传递 (Message Passing)**：
    *   这是主流方式。Agent之间发送结构化的消息包。
    *   需要统一的**通信语言 (ACL, Agent Communication Language)**。

### 9.3.2 通信方式
1.  **黑板系统**：
    *   **知识源 (KS)**：相当于Agent，拥有特定知识。
    *   **黑板**：公共工作区，存放问题状态和假设。
    *   **控制机制**：监控黑板变化，激活相应的知识源。
    *   *比喻*：一群医生围着病人的病历（黑板）会诊，谁有想法谁就写在病历上。
2.  **消息/对话系统**：
    *   **直接通信**：点对点发送（需要知道对方ID）。
    *   **中介通信**：通过通信服务器转发（类似于电子邮件服务器）。

### 9.3.3 智能体通信语言 (重点：KQML)
为了让不同语言（Java, C++, Lisp）写的Agent能交流，需要通用的“世界语”。

1.  **KIF (Knowledge Interchange Format)**：
    *   基于一阶谓词逻辑，用于**知识内容的表示**。
    *   主要解决“说的是什么内容”。

2.  **KQML (Knowledge Query and Manipulation Language)**：
    *   **地位**：MAS通信的事实标准。
    *   **结构**：分三层。
        *   **通信层**：底层传输参数（Sender, Receiver）。
        *   **消息层 (核心)**：定义**言语行为 (Performatives)**。告诉对方这句话是“请求(ask)”、“告知(tell)”还是“回复(reply)”。
        *   **内容层**：实际传输的数据（可以用KIF、SQL等描述）。
    *   **优点**：将**通信意图**与**通信内容**分离。即便读不懂内容，也能根据消息层知道对方是想问问题还是给答案。

---

## 9.4 多智能体系统的协调 (Coordination)

**协调的定义**：管理Agent之间的依赖关系，避免有害的相互作用（如死锁、资源争夺），促进有益的相互作用。

### 四种主要的协调方法：
1.  **基于集中规划的协调**：
    *   有一个**主控Agent**（老大哥），它收集所有信息，制定全局计划，分发给其他Agent执行。
    *   *优点*：容易实现全局最优。
    *   *缺点*：中心节点是瓶颈，容错性差。
2.  **基于协商的协调**：
    *   Agent之间通过对话、讨价还价来达成一致。
    *   *典型*：合同网。
3.  **基于对策论 (博弈论) 的协调**：
    *   假设Agent是理性的、自私的。利用博弈论（如纳什均衡）来设计规则，使得大家在追求自身利益最大化的同时，也能维持系统的平衡。
    *   *分类*：无通信协调（猜对方怎么想）、有通信协调。
4.  **基于社会规划的协调**：
    *   制定**社会规范 (Social Laws)** 或交通规则。
    *   *例子*：规定所有机器人在走廊里必须靠右行驶，这样就不需要每次相遇都协商怎么避让了。

---

## 9.5 多智能体系统的协作 (Cooperation)

**协作**是协调的一种特例，指的是非对抗的Agent为了**共同的目标**一起工作。

### 9.5.1 协作类型
1.  **完全协作型**：像蚂蚁搬家，只有集体目标，没有私心。
2.  **协作型**：有共同大目标，但也有自己的小算盘（比如公司各部门）。
3.  **自私型**：像自由市场，大家各顾各的，但在规则下形成秩序。

### 9.5.2 经典协作方法：合同网 (Contract Net Protocol) ★★★
这是最著名的分布式协作机制，模仿人类商业活动中的“招标-投标-中标”过程。

*   **角色**：
    *   **管理者 (Manager)**：有任务需要外包的Agent。
    *   **工作者 (Contractor)**：有能力执行任务的Agent。
    *   *注意*：角色是动态的，一个Agent此时是管理者，下一刻可能是工作者。

*   **流程**：
    1.  **任务发布**：管理者广播“招标书”（任务描述、要求）。
    2.  **投标**：工作者评估自身能力，如果能做，发送“投标书”（报价、预计时间）。
    3.  **中标**：管理者评估收到的标书，选择最优者，发送“中标通知”。
    4.  **执行与报告**：工作者执行任务，向管理者报告结果。

*   **优点**：动态分配负载，灵活性高，鲁棒性强（一个工作者挂了可以找别的）。

### 9.5.3 其他协作方法
*   **黑板模型**：通过共享数据区进行间接协作。
*   **市场机制**：引入虚拟货币，通过价格机制调节资源分配（如计算资源的拍卖）。

---

## 9.6 多智能体系统的协商 (Negotiation)

**协商**是解决冲突、达成一致的过程。

### 9.6.1 协商三要素
1.  **协商协议 (Protocol)**：
    *   **规则**：规定谁先说话、说什么话、怎么结束。
    *   *例子*：轮流出价协议、拍卖协议。
2.  **协商策略 (Strategy)**：
    *   **决策逻辑**：Agent根据自身目标，决定下一步出什么价、是否接受对方报价。
    *   *分类*：
        *   **竞争型**：寸步不让，利益最大化（基于博弈论）。
        *   **协作型**：为了双赢，愿意妥协。
        *   **单方让步**：为了达成协议，主动降价。
3.  **协商处理 (Process)**：
    *   对协商过程的分析、评估、实施。

### 9.6.2 典型应用场景
*   **资源分配**：多个程序争夺打印机或带宽。
*   **电子商务**：买家Agent和卖家Agent自动砍价。
*   **交通控制**：路口红绿灯Agent协商放行时间。

---

# 🎓 总结与复习要点

1.  **Agent vs 程序**：记住自主性、反应性、社会性、进化性这四个词。
2.  **MAS的核心**：通信、协调、协作、协商。这四个词层层递进。
    *   **通信**是基础。
    *   **协调**是管理依赖，避免冲突。
    *   **协作**是共同完成任务。
    *   **协商**是解决分歧的手段。
3.  **关键技术**：
    *   **KQML**：通信的标准语言（分层结构）。
    *   **合同网**：解决任务分配的经典协议（招标-投标）。
    *   **黑板系统**：解决知识共享和复杂推理的架构。

这一章的内容偏向理论架构和系统设计，理解其**去中心化**、**交互式**的思想是关键。
---
这是一份为您量身定制的**《人工智能基础》第十章：自然语言处理及其应用（NLP）**的深度复习全书。

自然语言处理（Natural Language Processing, NLP）被比尔·盖茨誉为“**人工智能皇冠上的明珠**”。它是计算机科学领域与人工智能领域中的一个重要方向，研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。

本章内容既包含**语言学的理论层次**，又包含**统计学与深度学习的算法应用**。为了确保你拿下高分，我们将从**核心概念**、**语言处理层次**、**关键技术（机器翻译与语音识别）**以及**深度学习前沿**四个维度进行地毯式拆解。

---

# 📘 第十章：自然语言处理 (NLP) 复习全书

## 第一部分：核心概念与发展史

### 1. 什么是自然语言处理？
*   **定义**：NLP 是一门研究如何让计算机能够理解、生成和处理人类自然语言（如中文、英文）的学科。
*   **目标**：
    *   **NLU (Natural Language Understanding)**：自然语言理解。让机器“听懂”人话（如：阅读理解、情感分析）。
    *   **NLG (Natural Language Generation)**：自然语言生成。让机器“说”人话（如：写作机器人、对话系统）。

### 2. 为什么 NLP 很难？（考试常考简答）
自然语言与计算机编程语言（如Python）不同，它具有以下特性导致了处理的极度复杂性：
1.  **歧义性 (Ambiguity)**：这是最大的挑战。
    *   *词法歧义*：“行”是读 *xíng*（行走）还是 *háng*（银行）？
    *   *句法歧义*：“他打死了老虎” vs “他打死了桌子”。
    *   *语义歧义*：“乒乓球拍卖完了”。是“球拍/卖完了”还是“球/拍卖/完了”？
2.  **非结构化**：自然语言是线性序列，缺乏像数据库那样的明确结构。
3.  **演化性**：语言是活的，网络热词（如“yyds”）层出不穷，字典永远滞后。
4.  **知识依赖**：理解语言往往需要**常识**。比如“他把香蕉吃了因为**它**很饿”和“他把香蕉吃了因为**它**熟透了”，两个“它”指代完全不同。

### 3. 发展历程（历史考点）
1.  **萌芽期 (1940s-1950s)**：
    *   **图灵测试**的提出。
    *   **机器翻译**的尝试（1954年乔治城实验）。当时主要靠查字典，效果很差。
2.  **符号/规则主义时期 (1950s-1980s)**：
    *   **乔姆斯基 (Chomsky)**：提出了**形式语言理论**（生成文法），试图用数学规则描述语言的语法结构。
    *   **SHRDLU系统**：在“积木世界”里能进行完美的对话，但无法推广到现实世界。
    *   **ALPAC报告 (1966)**：美国给机器翻译泼了冷水，指出机器翻译又贵又差，导致NLP研究进入寒冬。
3.  **统计主义时期 (1990s-2010)**：
    *   **核心思想**：不再试图教机器语法规则，而是让机器从海量数据（语料库）中统计概率。
    *   **HMM (隐马尔可夫模型)**：统治了语音识别领域。
    *   **N-gram**：统计语言模型。
4.  **连接主义/深度学习时期 (2010-至今)**：
    *   **Word2Vec**：将词转化为向量。
    *   **RNN/LSTM/Seq2Seq**：处理序列数据。
    *   **Transformer/BERT/GPT**：大模型时代，效果突飞猛进。

---

## 第二部分：语言处理的五个层次 (The Hierarchy)

这是一道经典的**填空题**或**简答题**，必须背诵顺序和含义。

从低级到高级，NLP的处理过程分为：

### 1. 语音分析 (Phonology)
*   **任务**：处理声音信号。
*   **过程**：将连续的声波波形转化为音素序列。

### 2. 词法分析 (Morphology) —— “切词”
*   **任务**：找出词汇的各个词素，确定词义。
*   **英文**：处理词缀（如 un-break-able），词性还原（went $\to$ go）。
*   **中文**：**分词 (Word Segmentation)** 是核心难点。因为中文没有空格。
    *   *算法*：正向最大匹配法、逆向最大匹配法、结巴分词（基于统计）。
    *   *难点*：未登录词（新词）识别、歧义切分（“南京市/长江大桥” vs “南京/市长/江大桥”）。

### 3. 句法分析 (Syntax) —— “画树”
*   **任务**：分析句子的语法结构，判断句子是否符合文法，并生成**句法树 (Parse Tree)**。
*   **理论基础**：乔姆斯基的**上下文无关文法 (CFG)**。
*   **例子**：分析“我 吃 苹果”。
    *   S (句子) $\to$ NP (名词短语) + VP (动词短语)
    *   NP $\to$ "我"
    *   VP $\to$ V (动词 "吃") + NP (名词短语 "苹果")

### 4. 语义分析 (Semantics) —— “懂意思”
*   **任务**：从句法结构推导出句子的真实含义。
*   **难点**：多义词消歧。
*   **方法**：**格文法 (Case Grammar)**。分析句子中的深层逻辑关系（施事、受事、工具等）。
    *   例：“钥匙打开了门”和“张三用钥匙打开了门”。虽然主语不同，但“钥匙”在语义深层都是“工具格”。

### 5. 语用分析 (Pragmatics) —— “听话听音”
*   **任务**：研究语言在特定环境下的意图。
*   **例子**：
    *   字面意思：“这里有点冷。”
    *   语用含义：“请把窗户关上。”
*   这是目前AI最难突破的领域。

---

## 第三部分：机器翻译 (Machine Translation, MT)

机器翻译是NLP最经典的应用之一。

### 1. 发展阶段
*   **第一代：基于规则 (RBMT)**
    *   *原理*：词典匹配 + 语法规则转换。
    *   *流程*：源语言分析 $\to$ 中间表示 $\to$ 目标语言生成。
    *   *缺点*：规则写不完，例外情况太多。著名的笑话：“The spirit is willing, but the flesh is weak”（心有余而力不足）被翻译成“酒是好的，肉变质了”。
*   **第二代：基于统计 (SMT)**
    *   *原理*：**贝叶斯公式**。寻找概率最大的译文 $T$。
    *   $$P(T|S) = \frac{P(S|T) \cdot P(T)}{P(S)}$$
        *   $P(T)$：**语言模型**。保证译文通顺（像人话）。
        *   $P(S|T)$：**翻译模型**。保证意思对等（词对词翻译概率）。
    *   *优点*：无需懂语法，只要有海量的双语对照语料库（如联合国文件）。
*   **第三代：神经机器翻译 (NMT) ★当前主流★**
    *   *架构*：**Encoder-Decoder (编码器-解码器)** 模型。
    *   *原理*：
        1.  **编码器**：把源语言句子压缩成一个固定长度的上下文向量 (Context Vector)。
        2.  **解码器**：根据这个向量生成目标语言。
    *   *核心技术*：**注意力机制 (Attention Mechanism)**。解决长句子翻译问题，让机器在翻译“apple”时，注意力集中在原文的“苹果”上，而不是其他词。

### 2. 评价指标
*   **BLEU Score**：通过比较机器译文和人工参考译文的n-gram重合度来打分。

---

## 第四部分：语音识别 (Automatic Speech Recognition, ASR)

ASR 的目标是将人类的语音信号转换为文本。这部分涉及**信号处理**和**概率模型**。

### 1. 核心流程 (必考流程图)
**语音信号 $\to$ 预处理 $\to$ 特征提取 $\to$ 解码(声学模型+语言模型) $\to$ 文本**

### 2. 关键步骤详解
#### (1) 预处理 (Preprocessing)
*   **预加重**：提升高频信号（因为语音的高频部分能量低，容易被噪声淹没）。
*   **分帧 (Framing)**：语音信号是短时平稳的。我们把它切成一小段一小段（比如25毫秒一帧），每一帧看作是静止的。
*   **加窗 (Windowing)**：通常使用**汉明窗 (Hamming Window)**，防止分帧造成的频谱泄露。

#### (2) 特征提取 (Feature Extraction)
*   机器听不懂波形图，需要提取特征向量。
*   **MFCC (梅尔频率倒谱系数)**：这是最常用的特征。它模拟了人耳对声音频率的非线性感知（人耳对低频敏感，高频迟钝）。

#### (3) 隐马尔可夫模型 (HMM) ★难点·重点★
这是统计语音识别的核心模型。
*   **基本思想**：语音是一个双重随机过程。
    1.  **隐含层 (Hidden States)**：不仅看到的（听到的）波形，背后隐藏的是**音素**（Phonemes）。
    2.  **观察层 (Observations)**：我们实际提取到的语音特征向量。
*   **三个问题**：
    1.  **评估**：给定模型，计算产生某段语音的概率。
    2.  **解码 (Decoding)**：给定语音，推测最可能的音素序列（即：说了什么？）。通常使用**维特比算法 (Viterbi Algorithm)**。
    3.  **学习**：给定语音数据，训练HMM的参数（Baum-Welch算法）。

#### (4) 混合模型 (GMM-HMM 到 DNN-HMM)
*   传统方法用**高斯混合模型 (GMM)** 来拟合状态发射概率。
*   现代方法（深度学习）用**深度神经网络 (DNN)** 替代GMM，识别率大幅提升。

---

## 第五部分：深度学习在NLP中的革命 (Modern NLP)

这部分是区分高分考生的关键，涉及当前最前沿的技术。

### 1. 词向量 (Word Embedding)
*   **One-hot 编码**：旧方法。`[0, 0, 1, 0...]`。缺点：维度灾难，无法表示词之间的相似度（“猫”和“狗”的正交的）。
*   **Word2Vec**：新方法。将词映射到低维稠密向量空间。
    *   *特性*：语义相似的词在空间距离上更近。甚至支持运算：$Vector(King) - Vector(Man) + Vector(Woman) \approx Vector(Queen)$。

### 2. 循环神经网络 (RNN) 与 LSTM
*   **RNN**：专门处理序列数据，具有“记忆”功能。缺点：**梯度消失**，记不住长距离的信息。
*   **LSTM (长短期记忆网络)**：引入了“门控机制”（遗忘门、输入门、输出门），解决了长距离依赖问题。

### 3. Transformer 与 预训练模型 (Pre-training)
*   **Transformer**：抛弃了循环结构，完全基于**自注意力机制 (Self-Attention)**。并行计算能力强，效果好。
*   **BERT (Bidirectional Encoder Representations from Transformers)**：
    *   *思想*：先在海量文本上进行无监督的**预训练 (Pre-training)**（做完形填空），学到通用的语言知识。
    *   *应用*：在具体任务上进行**微调 (Fine-tuning)**。这开启了NLP的“ImageNet时刻”。
*   **GPT (Generative Pre-trained Transformer)**：侧重于生成（接龙预测下一个词），是ChatGPT的基座。

---

# 📝 必刷精选试题与详解

## 题型一：基本概念填空 (20分)
1.  自然语言处理中的 \_\_\_\_\_\_\_\_ 是指同一个句子在不同的语境下可能有不同的含义。
2.  在语音识别预处理中，为了提升高频部分的能量，通常采用 \_\_\_\_\_\_\_\_ 技术。
3.  统计机器翻译的核心公式基于 \_\_\_\_\_\_\_\_ 定理。
4.  目前最流行的词向量表示模型之一是 Google 提出的 \_\_\_\_\_\_\_\_。

**答案**：
1.  语用歧义 (或 歧义性)
2.  预加重
3.  贝叶斯
4.  Word2Vec

## 题型二：简答题 (30分)

**Q1: 请简述 HMM (隐马尔可夫模型) 在语音识别中的作用，并解释什么是“隐含状态”和“观察状态”。**
*   **答案**：
    *   HMM 用于对语音的时序特性进行建模。它将语音识别看作是一个概率寻找过程。
    *   **隐含状态 (Hidden States)**：指语音中不可直接观察到的语言单位，通常对应**音素**（或者状态、词）。我们无法直接“看到”音素，只能推测。
    *   **观察状态 (Observations)**：指我们实际采集并处理得到的语音信号特征（如 **MFCC特征向量**）。
    *   语音识别的任务就是根据观察到的特征序列（观察状态），利用HMM推断出最可能的音素序列（隐含状态）。

**Q2: 相比于基于规则的机器翻译，统计机器翻译 (SMT) 有什么优点和缺点？**
*   **答案**：
    *   **优点**：
        1.  **无需人工编写规则**：依靠数据驱动，开发周期短，不需要语言学家构建复杂的语法树。
        2.  **鲁棒性强**：对不符合语法的句子也能给出概率最高的翻译，不会直接报错。
        3.  **语言无关性**：只要有双语语料库，同一套算法可以用于任何语言对。
    *   **缺点**：
        1.  **过度依赖语料**：对于稀缺语言（小语种），因为缺乏双语对照数据，效果很差。
        2.  **缺乏深层理解**：容易出现语法通顺但逻辑错误的“流利废话”。

## 题型三：综合分析题 (25分)

**题目**：
深度学习（特别是 Seq2Seq 模型和 Attention 机制）的出现极大地改变了机器翻译的技术路线。请结合下图（Encoder-Decoder架构图，脑补一下），解释：
1.  Encoder 和 Decoder 分别起什么作用？
2.  为什么说传统的定长向量编码是瓶颈？Attention 机制是如何解决这个问题的？

**答案与解析**：
1.  **作用**：
    *   **Encoder (编码器)**：负责“读”和“理解”。它将源语言句子（序列输入）通过RNN/LSTM处理，压缩成一个固定长度的**上下文向量 (Context Vector)**。这个向量包含了句子的语义信息。
    *   **Decoder (解码器)**：负责“写”和“生成”。它读取上下文向量，并逐个生成目标语言的单词，直到生成结束符。

2.  **瓶颈与解决方案**：
    *   **瓶颈**：传统的Encoder必须把整个句子的信息（无论多长）都压缩到一个固定长度的向量中。对于长句子，前面的信息容易被遗忘，导致翻译质量下降（信息丢失）。
    *   **Attention (注意力机制) 的解决之道**：
        *   Attention 机制允许 Decoder 在生成每一个词时，不只看那个固定的上下文向量，而是可以**“回头看”** Encoder 中所有的中间状态。
        *   它会计算一个**权重**，决定当前生成的词应该重点关注源句子中的哪些词（例如翻译“Apple”时，重点关注源句中的“苹果”）。
        *   这打破了固定长度向量的限制，极大地提升了长句翻译的效果。

---

### 💡 考前冲刺口诀
1.  **分词难点**：歧义和新词。
2.  **五大层次**：音、词、句、义、用。
3.  **语音识别**：MFCC提特征，HMM建模型，维特比找路径。
4.  **机器翻译**：规则太死板，统计靠概率，神经网路加注意力最强。
5.  **Word2Vec**：词变向量，可算加减。

祝你在《人工智能基础》考试中势如破竹，全线飘红（高分）！

这份试卷是严格根据**南京理工大学《人工智能基础》**（黄琦龙老师）课程大纲，结合你提供的**第1-8章**重点内容生成的。

**考试范围**：第1章（绪论）至 第8章（神经网络），**不包含**第9、10章。
**难度系数**：中等偏难（贴近真实期末考难度）。

---

# 《人工智能基础》期末模拟试卷

**适用专业**：计算机/自动化/人工智能等相关专业
**考试时间**：120分钟
**总分**：100分

---

### 一、选择题（共10题，每题2分，共20分）

1.  **（第1章）** 1956年，在达特茅斯会议上正式提出了“人工智能”这一术语，标志着人工智能学科的诞生。下列哪位学者**不**属于该会议的发起人？
    A. 麦卡锡 (John McCarthy)
    B. 明斯基 (Marvin Minsky)
    C. 图灵 (Alan Turing)
    D. 香农 (Claude Shannon)

2.  **（第2章）** 在一阶谓词逻辑中，将命题“每个人都有父亲”翻译成谓词公式，正确的是（设 $P(x)$ 为人，$F(x,y)$ 表示 $x$ 是 $y$ 的父亲）：
    A. $(\forall x)(P(x) \land (\exists y)F(y,x))$
    B. $(\forall x)(P(x) \rightarrow (\exists y)F(y,x))$
    C. $(\exists y)(\forall x)(P(x) \rightarrow F(y,x))$
    D. $(\forall x)(\forall y)(P(x) \rightarrow F(y,x))$

3.  **（第3章）** 在鲁宾逊归结原理中，设 $C_1 = P(x) \lor Q(x)$， $C_2 = \neg P(a) \lor R(y)$。若要对 $C_1$ 和 $C_2$ 进行归结，所需的合一置换 $\sigma$ 是：
    A. $\{a/x\}$
    B. $\{x/a\}$
    C. $\{a/y\}$
    D. $\{x/y\}$

4.  **（第4章）** 在C-F模型中，已知 $CF(E_1)=0.8, CF(E_2)=-0.6$，规则为 $IF\ E_1\ AND\ E_2\ THEN\ H\ (0.9)$，则结论 $H$ 的可信度 $CF(H)$ 为：
    A. 0.72
    B. -0.54
    C. 0
    D. 0.9

5.  **（第5章）** 在 $A^*$ 算法中，若估价函数 $f(n) = g(n) + h(n)$，其中 $g(n)$ 是从初始节点到节点 $n$ 的实际代价，$h(n)$ 是从 $n$ 到目标节点的估计代价，$h^*(n)$ 是实际最小代价。保证找到最优解的条件是：
    A. $h(n) > h^*(n)$
    B. $h(n) = 0$
    C. $h(n) \le h^*(n)$
    D. $g(n) \ge h(n)$

6.  **（第6章）** 遗传算法中，为了防止“早熟收敛”（局部最优），最有效的操作算子是：
    A. 选择 (Selection)
    B. 交叉 (Crossover)
    C. 变异 (Mutation)
    D. 编码 (Encoding)

7.  **（第6章）** 在粒子群算法（PSO）的速度更新公式中，用来表示“社会协作”（即向群体最优学习）的部分是：
    A. $\omega v_{id}$
    B. $c_1 r_1 (p_{id} - x_{id})$
    C. $c_2 r_2 (p_{gd} - x_{id})$
    D. $x_{id} + v_{id}$

8.  **（第7章）** 专家系统区别于传统程序的最大特点之一是：
    A. 处理数值计算
    B. 知识库与推理机分离
    C. 必须使用LISP语言编写
    D. 只能产生唯一的正确解

9.  **（第8章）** 关于BP神经网络，下列说法错误的是：
    A. 是一种多层前馈神经网络
    B. 采用误差反向传播算法进行训练
    C. 理论上可以逼近任意连续函数
    D. 网络层与层之间无连接，同一层神经元之间全连接

10. **（第8章）** Hopfield神经网络的主要应用领域是：
    A. 联想记忆与组合优化
    B. 语音识别
    C. 逻辑推理
    D. 自然语言生成

---

### 二、填空题（共20空，每空1分，共20分）

1.  **（第1章）** 人工智能的三大主要流派是符号主义、\_\_\_\_\_\_\_\_ 和 \_\_\_\_\_\_\_\_。
2.  **（第2章）** 知识表示的方法主要有谓词逻辑表示法、\_\_\_\_\_\_\_\_、框架表示法、语义网络表示法和知识图谱等。
3.  **（第2章）** 知识图谱的基本存储单元是 \_\_\_\_\_\_\_\_，通常由（实体，关系，实体）构成。
4.  **（第3章）** 谓词公式化为子句集的步骤中，消去存在量词 $\exists$ 的方法称为 \_\_\_\_\_\_\_\_ 化。
5.  **（第4章）** 在D-S证据理论中，识别框架 $\Theta$ 的基本概率分配函数 $M$ 必须满足两个条件：$M(\Phi)=0$ 和 \_\_\_\_\_\_\_\_。
6.  **（第4章）** 模糊集合的运算中，设 $A, B$ 为模糊集，则交集 $A \cap B$ 的隶属度函数 $\mu_{A \cap B}(x) =$ \_\_\_\_\_\_\_\_ $\{\mu_A(x), \mu_B(x)\}$ （填max或min）。
7.  **（第5章）** 盲目搜索中，利用**队列**作为OPEN表数据结构的算法是 \_\_\_\_\_\_\_\_；利用**栈**作为OPEN表数据结构的算法是 \_\_\_\_\_\_\_\_。
8.  **（第5章）** 在博弈搜索中，为了提高Minimax算法的效率，通常采用 \_\_\_\_\_\_\_\_ 剪枝技术。
9.  **（第7章）** 专家系统的核心组成部分包括 \_\_\_\_\_\_\_\_、推理机、综合数据库、解释机构等。其中，MYCIN系统的骨架系统被称为 \_\_\_\_\_\_\_\_。
10. **（第8章）** M-P神经元模型中，$y = f(\sum w_i x_i - \theta)$，其中 $\theta$ 代表 \_\_\_\_\_\_\_\_，$f(\cdot)$ 代表 \_\_\_\_\_\_\_\_。
11. **（第8章）** 卷积神经网络（CNN）中的 \_\_\_\_\_\_\_\_ 层主要用于特征提取，\_\_\_\_\_\_\_\_ 层主要用于降维和防止过拟合。
12. **（第6章）** 遗传算法的三个基本遗传算子是选择、\_\_\_\_\_\_\_\_ 和 \_\_\_\_\_\_\_\_。
13. **（第6章）** 蚁群算法是受蚂蚁在寻找食物过程中释放 \_\_\_\_\_\_\_\_ 的行为启发而提出的。

---

### 三、计算证明题（共4题，共30分）

**1. （归结原理证明，7分）**
**已知**：
(1) 任何去过北京的人都登过长城。
(2) 张三去过北京。
**求证**：张三登过长城。
**要求**：
1.  定义谓词。
2.  将前提和结论的否定转换为子句集。
3.  利用归结原理进行证明（画出归结树或写出归结步骤）。

**2. （C-F模型计算，8分）**
设有如下规则：
*   $r_1$: IF $E_1$ AND $E_2$ THEN $H$ ($CF=0.8$)
*   $r_2$: IF $E_3$ THEN $H$ ($CF=-0.5$)
已知初始证据的可信度为：$CF(E_1)=0.9, CF(E_2)=0.7, CF(E_3)=0.6$。
求：结论 $H$ 的最终可信度 $CF(H)$。

**3. （D-S证据理论计算，8分）**
设识别框架 $\Theta = \{A, B\}$。
现有两个独立的证据源 $M_1, M_2$：
*   $M_1(\{A\}) = 0.8, \quad M_1(\Theta) = 0.2$
*   $M_2(\{B\}) = 0.6, \quad M_2(\Theta) = 0.4$
请利用Dempster合成规则计算组合后的基本概率分配函数：$M(\{A\}), M(\{B\}), M(\Theta)$。

**4. （BP网络权值更新，7分）**
设一个单层感知机（无隐层），输入节点 $x=1$，输出节点 $y$，目标输出 $d=0$。
激活函数为 Sigmoid 函数 $f(net) = \frac{1}{1+e^{-net}}$。
当前权值 $w=2$，阈值 $\theta=0$，学习率 $\eta=0.5$。
**请计算**：
1.  当前的前向输出 $y$。
2.  经过一次反向传播后，权值 $w$ 的新值。（提示：Sigmoid导数为 $y(1-y)$，权值更新公式 $\Delta w = \eta (d-y) y(1-y) x$）

---

### 四、简答题（共3题，共20分）

**1. （搜索策略，6分）**
请简述宽度优先搜索（BFS）与深度优先搜索（DFS）的区别，并说明什么情况下 BFS 优于 DFS？

**2. （遗传算法，7分）**
什么是遗传算法中的“早熟收敛”现象？它是如何产生的？可以采取什么措施来抑制早熟？

**3. （神经网络，7分）**
请简述卷积神经网络（CNN）中“权值共享（Weight Sharing）”的概念及其作用。

---

### 五、设计题（共1题，10分）

**题目**：
有一个“0-1背包问题”：有 $N$ 个物品，每个物品都有自己的重量 $w_i$ 和价值 $v_i$。现在有一个背包，最大载重为 $W$。要求在不超过背包载重的前提下，选择物品装入背包，使得背包内物品的总价值最大。
请设计一个**遗传算法**来解决这个问题。
**要求说明以下几点**：
1.  **编码方式**：如何用染色体表示一个解？
2.  **适应度函数**：如何设计适应度函数（需考虑约束条件）？
3.  **初始化与算子**：简述初始化方法及选择、交叉、变异的具体操作方式。

---
---

# ✅ 参考答案与详解

### 一、选择题
1.  **C** (图灵提出了图灵测试，但他于1954年去世，未参加1956年会议)
2.  **B** ($\forall$ 配 $\rightarrow$, $\exists$ 配 $\land$)
3.  **A** (将 $x$ 替换为 $a$)
4.  **C** (规则要求 AND，取 $E_1, E_2$ 最小值 0.8，但证据 $E_2$ 为负数 -0.6？题目未给负值。假设题目意思是$CF(E_1)=0.8, CF(E_2)=-0.6$。这道题考的是陷阱。如果AND中有一个证据不可信(小于0)或者合成结果小于0？不。
    *修正解析*：题干给出的是规则的可信度0.9。我们需要计算证据 $E = E_1 \land E_2$。
    如果是 $CF(E_1)=0.8, CF(E_2)=-0.6$，则 $CF(E) = \min(0.8, -0.6) = -0.6$。
    因为 $CF(E) < 0$，根据传递公式 $\max(0, CF(E))$，规则不触发，结果为0。
    故选 **C**。
    *(注：若题意是证据都为正，例如0.8和0.6，则 $\min=0.6, CF=0.9 \times 0.6 = 0.54$。按原题数据选C)*)
5.  **C** (可采纳性条件)
6.  **C** (变异增加多样性)
7.  **C** (c1是认知，c2是社会)
8.  **B** (核心区别)
9.  **D** (BP网络同层之间无连接)
10. **A** (Hopfield两大功能)

### 二、填空题
1.  连接主义、行为主义
2.  产生式表示法
3.  三元组
4.  Skolem
5.  $\sum M(A) = 1$
6.  min
7.  宽度优先搜索 (BFS)、深度优先搜索 (DFS)
8.  Alpha-Beta ($\alpha-\beta$)
9.  知识库、EMYCIN
10. 阈值、激活函数
11. 卷积 (Convolution)、池化 (Pooling)
12. 交叉、变异
13. 信息素 (Pheromone)

### 三、计算证明题

**1. 归结原理证明**
**解**：
1.  **定义谓词**：$P(x)$: 去过北京; $C(x)$: 登过长城。常量 $Zhang$。
2.  **形式化**：
    *   前提1：$(\forall x)(P(x) \rightarrow C(x)) \Rightarrow \neg P(x) \lor C(x)$ (子句1)
    *   前提2：$P(Zhang)$ (子句2)
    *   结论：$C(Zhang)$
    *   **否定结论**：$\neg C(Zhang)$ (子句3)
3.  **归结**：
    *   子句1与子句2归结（置换 $\{Zhang/x\}$）：
        $\neg P(Zhang) \lor C(Zhang)$ 与 $P(Zhang)$ 归结 $\Rightarrow C(Zhang)$ (子句4)
    *   子句4与子句3归结：
        $C(Zhang)$ 与 $\neg C(Zhang)$ 归结 $\Rightarrow$ **NIL**
    *   得证。

**2. C-F模型计算**
**解**：
1.  **计算规则1**：
    *   证据强度：$CF(E_{and}) = \min(CF(E_1), CF(E_2)) = \min(0.9, 0.7) = 0.7$
    *   结论1可信度：$CF_1(H) = 0.8 \times \max(0, 0.7) = 0.56$
2.  **计算规则2**：
    *   证据强度：$CF(E_3) = 0.6$
    *   结论2可信度：$CF_2(H) = -0.5 \times \max(0, 0.6) = -0.3$
3.  **合成结论**：
    *   由于 $CF_1 > 0, CF_2 < 0$，属于一正一负。
    *   公式：$(CF_1 + CF_2) / (1 - \min(|CF_1|, |CF_2|))$
    *   $CF(H) = (0.56 - 0.3) / (1 - \min(0.56, 0.3))$
    *   $CF(H) = 0.26 / (1 - 0.3) = 0.26 / 0.7 \approx 0.37$

**3. D-S证据理论计算**
**解**：
1.  **计算正交积**：
    *   $M_1(A) \times M_2(B) = 0.8 \times 0.6 = 0.48$ ($\to \Phi$，冲突)
    *   $M_1(A) \times M_2(\Theta) = 0.8 \times 0.4 = 0.32$ ($\to A$)
    *   $M_1(\Theta) \times M_2(B) = 0.2 \times 0.6 = 0.12$ ($\to B$)
    *   $M_1(\Theta) \times M_2(\Theta) = 0.2 \times 0.4 = 0.08$ ($\to \Theta$)
2.  **计算冲突因子 K**：
    $K = 0.48$
    归一化分母 $1-K = 0.52$
3.  **归一化计算**：
    *   $M(A) = 0.32 / 0.52 \approx 0.615$
    *   $M(B) = 0.12 / 0.52 \approx 0.231$
    *   $M(\Theta) = 0.08 / 0.52 \approx 0.154$

**4. BP网络计算**
**解**：
1.  **前向计算**：
    $net = w \cdot x - \theta = 2 \times 1 - 0 = 2$
    $y = f(2) = \frac{1}{1+e^{-2}} \approx 0.881$
2.  **反向传播更新**：
    *   误差信号 $\delta = (d-y) \cdot f'(net) = (d-y) \cdot y(1-y)$
    *   $\delta = (0 - 0.881) \times 0.881 \times (1 - 0.881) \approx -0.881 \times 0.105 \approx -0.092$
    *   权值调整量 $\Delta w = \eta \cdot \delta \cdot x = 0.5 \times (-0.092) \times 1 = -0.046$
    *   新权值 $w_{new} = 2 + (-0.046) = 1.954$

---

### 四、简答题

**1. BFS vs DFS**
*   **区别**：
    *   **数据结构**：BFS使用队列（先进先出），DFS使用栈（后进先出）。
    *   **搜索顺序**：BFS由近及远层层扩展；DFS一条路走到黑，撞墙回溯。
    *   **性质**：BFS完备且能保最优（边权相等时）；DFS不完备（可能陷死循环），非最优。
*   **BFS优于DFS的情况**：
    *   当要求找到**最短路径**或**最优解**（步数最少）时。
    *   当解在搜索树的较浅层，而树的深度非常深（甚至无限）时，DFS可能无法终止，必须用BFS。

**2. 早熟收敛**
*   **定义**：遗传算法在进化早期，所有个体的基因变得高度相似，算法停止在局部最优解，无法找到全局最优解。
*   **原因**：
    *   超级个体（适应度极高）在早期大量繁殖，占据种群。
    *   选择压力过大，多样性丧失。
*   **措施**：
    *   **增加变异概率**：引入新基因。
    *   **改进选择策略**：如使用排名选择、锦标赛选择代替轮盘赌。
    *   **适应度尺度变换**：降低超级个体的竞争力。
    *   **多种群策略**：引入移民机制。

**3. CNN权值共享**
*   **概念**：在卷积层中，同一个卷积核（Filter）在遍历整个图像进行卷积操作时，其参数（权重）是固定不变的。
*   **作用**：
    1.  **减少参数量**：相比全连接层，参数量呈指数级下降，降低模型复杂度，防止过拟合。
    2.  **平移不变性**：无论特征（如猫的耳朵）出现在图像哪个位置，都能被同一个卷积核识别提取出来。

---

### 五、设计题 (遗传算法解决0-1背包)

**答案要点**：

1.  **编码方式**：
    *   采用**二进制编码**。
    *   染色体长度为 $N$（物品数量）。
    *   每一位对应一个物品，`1` 表示放入背包，`0` 表示不放入。
    *   例如：`10010` 表示选择了第1和第4个物品。

2.  **适应度函数**：
    *   我们需要总价值最大，同时必须满足重量约束。
    *   **罚函数法**：
        令 $V = \sum x_i v_i$ (总价值)，$W_{total} = \sum x_i w_i$ (总重量)。
        $$Fit(x) = \begin{cases} V, & \text{if } W_{total} \le W \\ 0 \text{ (或极小值)}, & \text{if } W_{total} > W \end{cases}$$
    *   这样超重的个体适应度极低，会被自然淘汰。

3.  **初始化与算子**：
    *   **初始化**：随机生成 $M$ 个长度为 $N$ 的二进制串。
    *   **选择**：采用**轮盘赌选择法**，适应度越大的个体被选中的概率越大。
    *   **交叉**：采用**单点交叉**。随机选择两个父代，在随机位置切断并交换后半部分，产生两个新个体。
    *   **变异**：采用**位点变异**。遍历染色体的每一位，以极小的概率 $P_m$ 将 `0` 变为 `1` 或 `1` 变为 `0`。

这是一份严格依据南京理工大学《人工智能基础》课程大纲（第1-8章）生成的期末模拟试卷及详解。

---

# 南京理工大学《人工智能基础》期末模拟试卷

**适用专业**：自动化/计算机/人工智能等
**考试范围**：第1章-第8章（不含第9、10章）
**考试形式**：闭卷
**考试时长**：120分钟

---

### 一、选择题（本大题共10小题，每小题2分，共20分）

1.  **[第1章]** 下列哪一项**不是**人工智能的三大主流流派之一？
    A. 符号主义 (Symbolism)
    B. 连接主义 (Connectionism)
    C. 逻辑主义 (Logicism)
    D. 行为主义 (Behaviorism)

2.  **[第2章]** 在谓词逻辑中，命题“所有的人都喜欢某些音乐”应表示为（设 $P(x)$ 为人，$M(y)$ 为音乐，$L(x,y)$ 为 $x$ 喜欢 $y$）：
    A. $(\forall x)(P(x) \rightarrow (\exists y)(M(y) \land L(x,y)))$
    B. $(\forall x)(P(x) \land (\forall y)(M(y) \rightarrow L(x,y)))$
    C. $(\exists x)(P(x) \land (\exists y)(M(y) \land L(x,y)))$
    D. $(\forall x)(\exists y)(P(x) \rightarrow M(y) \rightarrow L(x,y))$

3.  **[第3章]** 在归结原理中，若子句 $C_1 = P(x) \lor Q(f(x))$，子句 $C_2 = \neg P(a) \lor R(z)$，则 $C_1$ 和 $C_2$ 的归结式是：
    A. $Q(f(x)) \lor R(z)$
    B. $Q(f(a)) \lor R(z)$
    C. $Q(f(a)) \lor R(a)$
    D. 无法归结

4.  **[第4章]** 在C-F模型中，已知 $CF(H, E) = 0.8$，证据 $E$ 的可信度 $CF(E) = -0.5$，则结论 $H$ 的可信度 $CF(H)$ 为：
    A. -0.4
    B. 0.4
    C. 0
    D. -0.5

5.  **[第5章]** 关于$A^*$算法，若估价函数为 $f(n) = g(n) + h(n)$，要保证算法找到最优解，启发函数 $h(n)$ 必须满足的条件是（$h^*(n)$为实际最小代价）：
    A. $h(n) = g(n)$
    B. $h(n) \ge h^*(n)$
    C. $h(n) \le h^*(n)$
    D. $h(n) = 0$

6.  **[第6章]** 遗传算法中，主要负责产生新个体、决定算法全局搜索能力的核心算子是：
    A. 选择算子
    B. 交叉算子
    C. 变异算子
    D. 倒位算子

7.  **[第6章]** 粒子群优化算法（PSO）中，粒子速度更新公式由三部分组成，分别是：
    A. 惯性部分、社会部分、变异部分
    B. 惯性部分、认知部分、社会部分
    C. 认知部分、社会部分、随机部分
    D. 惯性部分、选择部分、交叉部分

8.  **[第7章]** 专家系统（Expert System）的核心组成部分是：
    A. 知识库和推理机
    B. 数据库和算法库
    C. 神经网络和权值矩阵
    D. 传感器和执行器

9.  **[第8章]** BP神经网络在训练过程中，若出现“过拟合”现象，通常是因为：
    A. 训练样本太少
    B. 隐层节点数过少
    C. 训练次数过多或网络结构过复杂
    D. 学习率设置过大

10. **[第8章]** 卷积神经网络（CNN）中，**池化层（Pooling Layer）**的主要作用是：
    A. 提取图像的边缘特征
    B. 增加数据的维度
    C. 降低数据维度，减少计算量，保持特征不变性
    D. 进行全连接分类

---

### 二、填空题（本大题共20空，每空1分，共20分）

1.  **[第1章]** 1950年，图灵发表论文《计算机器与智能》，提出了著名的 \_\_\_\_\_\_\_\_，用以判定机器是否具有智能。
2.  **[第2章]** 知识表示的方法主要有：一阶谓词逻辑、\_\_\_\_\_\_\_\_、框架表示法、语义网络和知识图谱。其中知识图谱的基本单位是 \_\_\_\_\_\_\_\_。
3.  **[第3章]** 将谓词公式化为子句集的过程中，消去存在量词 $\exists$ 的步骤称为 \_\_\_\_\_\_\_\_ 化；若存在量词在全称量词的辖域内，需使用 \_\_\_\_\_\_\_\_ 函数。
4.  **[第4章]** D-S证据理论中，基本概率分配函数 $M$ 满足 $\sum M(A) = $ \_\_\_\_\_\_\_\_ 且 $M(\Phi) = $ \_\_\_\_\_\_\_\_。信任函数 $Bel(A)$ 表示对命题 $A$ 的 \_\_\_\_\_\_\_\_ （填“信任下限”或“信任上限”）。
5.  **[第5章]** 盲目搜索策略中，宽度优先搜索（BFS）通常使用 \_\_\_\_\_\_\_\_ （数据结构）来存储节点，它具有 \_\_\_\_\_\_\_\_ 性（填“完备”或“不完备”）。
6.  **[第6章]** 遗传算法的生物学基础是达尔文的进化论，其核心思想是“\_\_\_\_\_\_\_\_，\_\_\_\_\_\_\_\_”。
7.  **[第7章]** 专家系统的推理机通常支持三种推理方向：正向推理、\_\_\_\_\_\_\_\_ 和 \_\_\_\_\_\_\_\_。
8.  **[第8章]** M-P神经元模型中，输出 $y = f(\sum w_ix_i - \theta)$，其中 $\theta$ 称为 \_\_\_\_\_\_\_\_，$f$ 称为 \_\_\_\_\_\_\_\_。
9.  **[第8章]** Hopfield神经网络是一种 \_\_\_\_\_\_\_\_ （填“前馈”或“反馈”）型网络，其能量函数在运行过程中总是 \_\_\_\_\_\_\_\_ （填“递增”或“递减”）的。

---

### 三、计算证明题（本大题共4小题，共30分）

**1. （归结原理证明，7分）**
**已知**：
(1) 凡是自然数都是大于等于0的整数。
(2) 所有整数不是奇数就是偶数。
(3) 偶数是非奇数。
**求证**：所有自然数都不是奇数就是非奇数。
**提示**：
定义谓词：$N(x)$: x是自然数；$I(x)$: x是整数；$GZ(x)$: x大于等于0；$O(x)$: x是奇数；$E(x)$: x是偶数。
(1) 形式化已知前提和结论。
(2) 化为子句集。
(3) 利用归结原理进行证明。

**2. （C-F模型计算，8分）**
设有如下推理规则：
*   $r_1$: IF $E_1$ THEN $H$ ($CF=0.8$)
*   $r_2$: IF $E_2$ THEN $H$ ($CF=0.6$)
*   $r_3$: IF $E_3$ THEN $H$ ($CF=-0.5$)
已知初始证据的可信度为：$CF(E_1)=0.7, CF(E_2)=0.8, CF(E_3)=0.9$。
**求**：结论 $H$ 的最终综合可信度 $CF(H)$。

**3. （D-S证据理论，7分）**
设识别框架 $\Theta = \{A, B\}$。
有两个证据源 $m_1, m_2$，其基本概率分配如下：
*   $m_1(\{A\}) = 0.5, \quad m_1(\Theta) = 0.5$
*   $m_2(\{B\}) = 0.4, \quad m_2(\Theta) = 0.6$
请计算组合后的基本概率分配函数 $m(\{A\}), m(\{B\}), m(\Theta)$。

**4. （BP神经网络，8分）**
考虑一个简单的神经元，输入 $x=[1, 2]^T$，权重 $w=[0.5, -0.5]^T$，偏置 $b=0$。激活函数使用 Sigmoid 函数 $f(z) = \frac{1}{1+e^{-z}}$。期望输出 $d=0.8$。学习率 $\eta=0.1$。
(1) 计算前向传播的输出 $y$。
(2) 定义损失函数 $E = \frac{1}{2}(d-y)^2$，请计算权重 $w_1$ 的更新值。（需写出推导过程）

---

### 四、简答题（本大题共3小题，共20分）

**1. （第5章，6分）**
请简述局部搜索算法中的**爬山法（Hill Climbing）**的主要缺点，并说明**模拟退火算法（Simulated Annealing）**是如何克服这些缺点的。

**2. （第8章，7分）**
请对比**BP神经网络**和**卷积神经网络（CNN）**。
(1) CNN相比全连接BP网络，在处理图像时主要引入了哪两个核心特性来减少参数量？
(2) 简述这两个特性的含义。

**3. （第6章，7分）**
在遗传算法中，**种群多样性**（Diversity）为何重要？如果多样性丧失（早熟收敛），算法会出现什么问题？通常可以用什么算子来维持多样性？

---

### 五、设计题（本大题共1小题，共10分）

**题目**：旅行商问题（TSP）是一个经典的组合优化问题：一个推销员要访问 $N$ 个城市，已知各城市之间的距离，要求他走遍所有城市（每个城市只去一次）并回到出发点，使总路径最短。
请设计一个**粒子群优化算法（PSO）**或**遗传算法（GA）**来求解该问题（二选一）。
**请回答以下内容**：
1.  **编码方案**：如何用数学形式（如粒子位置或染色体）表示一条路径？
2.  **适应度函数/目标函数**：如何定义解的优劣？
3.  **核心操作**：
    *   若选GA：如何进行交叉操作以避免产生无效路径（如重复访问城市）？
    *   若选PSO：TSP是离散问题，而PSO通常处理连续问题，你将如何定义“速度”和“位置更新”？（或简述离散PSO的思路）

---
---

# ✅ 参考答案与详解

### 一、选择题
1.  **C** （图灵未参加达特茅斯会议。由于本课程将逻辑主义视为符号主义的别称或基础，通常三大流派指符号、连接、行为）
2.  **A** （$\forall$ 配 $\rightarrow$，$\exists$ 配 $\land$）
3.  **B** （需对 $P(x)$ 和 $\neg P(a)$ 进行合一，置换为 $\{a/x\}$，故 $x$ 变为 $a$）
4.  **C** （当 $CF(E) < 0$ 时，规则不被触发，或者说支持度为0，不产生负面影响）
5.  **C** （可采纳性条件：估计代价 $\le$ 实际代价）
6.  **B** （交叉是产生新个体的主要手段，变异是辅助。但在某些教材中强调变异维持多样性。若问全局搜索的核心动力，通常选交叉；若问防止早熟，选变异。本题问产生新个体/搜索算子，倾向于交叉。） *注：如果是为了防止早熟，选C；如果是主要算子，选B。根据题意“主要负责产生新个体”，选B。*
7.  **B** （认知部分是 $c_1$，社会部分是 $c_2$）
8.  **A** （专家系统核心：知识库+推理机）
9.  **C** （过拟合通常因模型太复杂或训练过度）
10. **C** （池化层作用：降维、特征不变性）

### 二、填空题
1.  图灵测试
2.  产生式表示法；三元组
3.  Skolem；Skolem
4.  1；0；信任下限
5.  队列；完备
6.  物竞天择；适者生存
7.  逆向推理；混合推理
8.  阈值；激活函数
9.  反馈；递减

### 三、计算证明题

**1. 归结证明**
**解**：
1.  **形式化**：
    *   F1: $(\forall x)(N(x) \rightarrow (I(x) \land GZ(x)))$
        $\Rightarrow \neg N(x) \lor I(x)$ (子句1), $\neg N(x) \lor GZ(x)$ (子句2)
    *   F2: $(\forall x)(I(x) \rightarrow (O(x) \lor E(x)))$
        $\Rightarrow \neg I(x) \lor O(x) \lor E(x)$ (子句3)
    *   F3: $(\forall x)(E(x) \rightarrow \neg O(x))$
        $\Rightarrow \neg E(x) \lor \neg O(x)$ (子句4, 题目意为“是偶数则不是奇数”)
        *注意：题目“偶数是非奇数”通常理解为互斥，但也可能隐含等价。此处按单向蕴含处理即可，或者理解为$E \leftrightarrow \neg O$。按最简处理*。
    *   结论 Q: $(\forall x)(N(x) \rightarrow (O(x) \lor \neg O(x)))$  *(注：这其实是排中律，逻辑上恒真。题目可能意在推导 $N \rightarrow (\neg O \lor \neg O)$? 题目表述“不是奇数就是非奇数”即 $O \lor \neg O$。这道题如果是“所有自然数都不是奇数”，则需归结。若按字面意思“不是奇数就是非奇数”，这是废话。假设题目意图是证明“自然数是整数”之类。我们按题目字面逻辑走流程)*。
    *   *修正题目理解*：通常这类题是证明“某具体结论”。若题目无误，证明过程如下：
    *   否定结论 $\neg Q$: $\exists x (N(x) \land \neg (O(x) \lor \neg O(x)))$
        $\Rightarrow N(a) \land \neg O(a) \land O(a)$ (德摩根律)
    *   子句5: $N(a)$
    *   子句6: $\neg O(a)$
    *   子句7: $O(a)$
    *   **归结**：子句6与子句7直接归结 $\Rightarrow$ **NIL**。
    *   (这就得证了，甚至没用到前提。这说明结论本身是重言式。)

**2. C-F 计算**
**解**：
1.  **计算各规则的CF**：
    *   $r_1$: $CF_1(H) = 0.8 \times \max(0, 0.7) = 0.56$
    *   $r_2$: $CF_2(H) = 0.6 \times \max(0, 0.8) = 0.48$
    *   $r_3$: $CF_3(H) = -0.5 \times \max(0, 0.9) = -0.45$
2.  **合成**：
    *   先合成 $r_1, r_2$ (同正)：
        $CF_{12} = 0.56 + 0.48 - 0.56 \times 0.48 = 1.04 - 0.2688 = 0.7712$
    *   再合成 $CF_{12}, r_3$ (一正一负)：
        $CF_{final} = \frac{0.7712 + (-0.45)}{1 - \min(|0.7712|, |-0.45|)} = \frac{0.3212}{1 - 0.45} = \frac{0.3212}{0.55} \approx 0.584$

**3. D-S 理论计算**
**解**：
1.  **计算交叉积**：
    *   $A \cap B = \Phi$: $0.5 \times 0.4 = 0.2$ (冲突)
    *   $A \cap \Theta = A$: $0.5 \times 0.6 = 0.3$
    *   $\Theta \cap B = B$: $0.5 \times 0.4 = 0.2$
    *   $\Theta \cap \Theta = \Theta$: $0.5 \times 0.6 = 0.3$
2.  **计算 K**：
    $K = 0.2$
    $1-K = 0.8$
3.  **归一化**：
    *   $m(A) = 0.3 / 0.8 = 0.375$
    *   $m(B) = 0.2 / 0.8 = 0.25$
    *   $m(\Theta) = 0.3 / 0.8 = 0.375$

**4. BP 计算**
**解**：
1.  **前向计算**：
    $net = 0.5 \times 1 + (-0.5) \times 2 - 0 = -0.5$
    $y = \frac{1}{1+e^{-(-0.5)}} = \frac{1}{1+e^{0.5}} \approx \frac{1}{1+1.648} \approx 0.3775$
2.  **反向传播**：
    *   Sigmoid 导数 $f'(net) = y(1-y) = 0.3775 \times (1-0.3775) \approx 0.235$
    *   误差信号 $\delta = (d-y)f'(net) = (0.8 - 0.3775) \times 0.235 \approx 0.099$
    *   权值调整 $\Delta w_1 = \eta \cdot \delta \cdot x_1 = 0.1 \times 0.099 \times 1 \approx 0.0099$
    *   新权值 $w_1^{new} = 0.5 + 0.0099 = 0.5099$

---

### 四、简答题
**1. 爬山法 vs 模拟退火**
*   **爬山法缺点**：容易陷入**局部最优**，无法到达全局最优；在“高原”区域无法确定方向；在“山脊”区域搜索效率低。
*   **模拟退火的改进**：采用Metropolis准则，以一定概率**接受差解**（即允许“下山”），从而跳出局部最优陷阱。温度高时接受差解概率大，温度低时概率小，最终趋于稳定。

**2. CNN vs BP**
(1) 两个核心特性：**局部连接 (Local Connectivity)** 和 **权值共享 (Weight Sharing)**。
(2) **含义**：
*   **局部连接**：每个神经元只与输入图像的一个局部区域（感受野）相连，而不是全连接，符合视觉原理。
*   **权值共享**：同一个卷积核（滤波器）在图像所有位置使用相同的参数。这大大减少了参数数量，并赋予网络平移不变性。

**3. 遗传算法多样性**
*   **重要性**：多样性是算法进行全局搜索的基础。如果多样性丧失，种群中个体趋同，算法将失去探索新解空间的能力，陷入**早熟收敛（局部最优）**。
*   **措施**：主要是**变异算子**。通过随机改变基因，引入新的遗传物质。此外，也可以采用改进的选择策略（如锦标赛选择）或多种群并行进化策略。

---

### 五、设计题 (TSP)

**以遗传算法为例**：

1.  **编码方式**：
    采用**整数排列编码**（Path Representation）。
    一条染色体为一个 $1$ 到 $N$ 的排列，代表访问城市的顺序。
    例如：$N=5$，染色体 `(1, 5, 3, 2, 4)` 表示路径 $1 \to 5 \to 3 \to 2 \to 4 \to 1$。

2.  **适应度函数**：
    目标是路径总长度 $D = \sum d_{ij}$ 最小。
    适应度函数可设为 $Fit(x) = \frac{1}{D(x)}$。路径越短，适应度越大。

3.  **核心操作（交叉）**：
    由于要求路径合法（每个城市有且仅有一次），不能使用简单的单点交叉（会导致重复或缺失）。
    应采用**部分匹配交叉 (PMX)** 或 **次序交叉 (OX)**。
    *   **PMX简述**：随机选定两个交叉点，交换两个父代该区段的基因，建立映射关系；然后将染色体中其他位置上冲突的基因根据映射关系进行置换，确保合法性。

这是一份针对**第六章：智能计算（智能优化算法）**的深度解析。

这一章的内容不再是死板的逻辑推理，而是**向大自然学习**。我们将深入探讨三大核心算法：**遗传算法（进化）**、**粒子群算法（鸟群）**、**蚁群算法（蚁群）**。它们统称为**元启发式算法（Meta-heuristics）**，是解决“难（NP-hard）”问题的杀手锏。

---

# 第一部分：进化计算与遗传算法 (Genetic Algorithm, GA)

## 1. 核心哲学：达尔文的“自然选择”
遗传算法的本质是**“优胜劣汰，适者生存”**。
*   **自然界**：种群中，适应环境的个体活下来，把基因传给后代；不适应的死掉。经过千万年，生物越来越强。
*   **计算机界**：解空间中，目标函数值（适应度）高的解保留下来，通过交叉变异产生新解；差的解被删除。经过几百代迭代，得到最优解。

## 2. 算法的微观操作机制

### (1) 编码 (Encoding) —— “把现实问题DNA化”
这是GA最关键的第一步。计算机无法直接处理“怎么排课表”或“怎么设计机翼”，必须转化成数字串。

*   **二进制编码 (Binary)**：
    *   *形式*：`10010110`
    *   *适用*：简单的函数优化、背包问题。
    *   *缺点*：**汉明悬崖 (Hamming Cliff)**。例如数字 $7(0111)$ 和 $8(1000)$ 是相邻整数，但二进制每一位都变了。变异很难让7变成8，这会阻碍搜索。
    *   *改进*：使用**格雷码 (Gray Code)**，相邻整数只有一位不同。
*   **实数/浮点数编码 (Real-value)**：
    *   *形式*：`[1.2, -3.5, 0.8, ...]`
    *   *适用*：高维、高精度的连续函数优化，神经网络权值优化。
*   **排列编码 (Permutation)**：★**TSP问题专用**★
    *   *形式*：`[1, 5, 3, 2, 4]` (代表城市访问顺序)
    *   *注意*：不能出现重复数字。

### (2) 选择算子 (Selection) —— “谁有资格生孩子？”
*   **轮盘赌 (Roulette Wheel)**：概率与适应度成正比。
    *   *公式*：$P_i = \frac{f_i}{\sum f_j}$。
    *   *问题*：若初期有一个超级个体适应度极大，它会占据轮盘大部分，导致早熟；若后期大家适应度差不多，轮盘赌就变成了纯随机，进化停滞。
*   **锦标赛 (Tournament)**：
    *   随机挑 $k$ 个个体打架，赢的留下。
    *   *优点*：不依赖适应度的绝对值，只看相对大小，易于并行化。
*   **精英策略 (Elitism)**：
    *   **强制保留**历代出现过的最强个体，不让它参与交叉变异（防止被破坏）。这是保证算法**理论收敛**的关键。

### (3) 交叉算子 (Crossover) —— “信息的重组”
这是GA**区别于其他算法的核心特征**，也是产生新解的主要动力。
*   **单点/多点交叉**：切断、交换。适用于二进制编码。
*   **部分匹配交叉 (PMX) / 次序交叉 (OX)**：适用于**TSP排列编码**。
    *   *难点*：普通交叉会产生非法解（如 `1-2-3` 和 `2-3-1` 交叉可能变成 `1-3-3`，城市3重复了）。
    *   *PMX原理*：交换区间内的基因，并建立映射关系（如 $2 \leftrightarrow 5$），把染色体其他位置的冲突基因按映射关系替换掉。

### (4) 变异算子 (Mutation) —— “天才的火花”
*   **作用**：维持种群多样性，防止**早熟收敛（Premature Convergence）**。
*   如果没有变异，种群最终会变成一模一样的个体，算法就“死”了。
*   **操作**：以极小概率（如0.01）翻转某一位（0变1）。

## 3. 进阶：自适应遗传算法 (AGA)
**痛点**：传统的 $P_c$（交叉概率）和 $P_m$（变异概率）是固定的。
*   **AGA思想**：根据个体的表现**动态调整**参数。
    *   **对于大神（适应度高）**：降低 $P_c, P_m$，保护它，让它稳定遗传。
    *   **对于菜鸟（适应度低）**：提高 $P_c, P_m$，反正你已经很差了，不如大胆突变，说不定能变出个天才。
    *   **对于种群停滞**：当大家适应度都差不多时，提高 $P_m$ 以打破僵局。

---

# 第二部分：粒子群优化算法 (Particle Swarm Optimization, PSO)

## 1. 核心哲学：鸟群的“社会心理学”
PSO 模拟的是鸟群找食。鸟儿们没有上帝视角，但它们遵循两条简单的规则：
1.  **认知 (Cognitive)**：记得自己吃过最好吃的东西在哪（**个体历史最优 pbest**）。
2.  **社会 (Social)**：知道整个鸟群目前发现的最好吃的东西在哪（**全局历史最优 gbest**）。

## 2. 核心数学模型 (必考计算)
粒子没有体积，只有**位置 ($X$)** 和 **速度 ($V$)**。

### (1) 速度更新公式 (The Soul of PSO)
$$V_{id}^{k+1} = \underbrace{\omega V_{id}^k}_{\text{惯性}} + \underbrace{c_1 r_1 (P_{id}^k - X_{id}^k)}_{\text{自我认知}} + \underbrace{c_2 r_2 (P_{gd}^k - X_{id}^k)}_{\text{社会经验}}$$

*   **第一项（惯性）**：$\omega$ 是惯性权重。
    *   $\omega$ 大：像在大草原飞奔，适合**全局搜索**，探索新区域。
    *   $\omega$ 小：像在显微镜下找东西，适合**局部开发**，精细收敛。
    *   *策略*：通常让 $\omega$ 随迭代次数从 0.9 线性递减到 0.4。
*   **第二项（自我）**：$c_1$ 是学习因子。
    *   把粒子拉向它自己经历过的最好位置。如果 $c_1=0$，粒子就变成“随大流”，容易陷入局部最优。
*   **第三项（社会）**：$c_2$ 是学习因子。
    *   把粒子拉向群体发现的最好位置。如果 $c_2=0$，粒子就变成“闭门造车”，各搜各的，效率低。
*   **$r_1, r_2$**：[0, 1] 之间的随机数，增加行为的不可预测性，防止死循环。

### (2) 位置更新公式
$$X_{id}^{k+1} = X_{id}^k + V_{id}^{k+1}$$

## 3. PSO 与 GA 的对比 (高频考点)
*   **相同点**：都是群体智能，都是随机初始化，都用适应度评估。
*   **不同点**：
    *   **信息传递**：GA 靠交叉变异（基因重组）；PSO 靠单向的信息广播（大家都看向 gbest）。
    *   **记忆性**：PSO 的粒子有记忆（知道 pbest），GA 的个体一般没记忆（除非精英保留）。
    *   **参数**：GA 调概率；PSO 调权重。
    *   **应用**：PSO 更适合处理**连续优化问题**（如神经网络权值训练）；GA 更适合离散组合优化（如TSP）。

---

# 第三部分：蚁群算法 (Ant Colony Optimization, ACO)

## 1. 核心哲学：信息素的正反馈
蚁群算法模拟的是蚂蚁找路。
*   **Stigmergy (共识主动性)**：蚂蚁之间不直接对话，而是通过改变环境（留下**信息素 Pheromone**）来间接交流。
*   **正反馈循环**：
    路短 $\rightarrow$ 走的时间短 $\rightarrow$ 单位时间通过的蚂蚁多 $\rightarrow$ 留下的气味浓 $\rightarrow$ 吸引更多蚂蚁 $\rightarrow$ 气味更浓……

## 2. 关键机制详解 (以TSP问题为例)

### (1) 状态转移规则 (蚂蚁怎么选路?)
蚂蚁 $k$ 在城市 $i$ 选择去城市 $j$ 的概率 $P_{ij}$：
$$P_{ij}^k = \frac{[\tau_{ij}(t)]^\alpha \cdot [\eta_{ij}(t)]^\beta}{\sum [\tau_{is}(t)]^\alpha \cdot [\eta_{is}(t)]^\beta}$$

*   **$\tau_{ij}$ (信息素)**：历史经验。“大家都走这条路，我也走。”
*   **$\eta_{ij}$ (启发函数)**：通常取距离的倒数 $1/d_{ij}$。代表贪婪思想。“这条路看起来比较短，我走走看。”
*   **$\alpha$ (信息素重要程度)**：
    *   $\alpha=0$：蚂蚁变成了贪婪算法，只选最近的城市，极易陷入局部最优。
    *   $\alpha$ 过大：蚂蚁完全盲从，变成“随大流”，算法过早收敛。
*   **$\beta$ (启发式重要程度)**：
    *   $\beta=0$：蚂蚁变成了纯随机搜索（如果信息素初始相等），很难收敛。

### (2) 信息素更新规则 (气味怎么变?)
每一轮结束后，必须更新地图上的气味。
$$\tau_{ij}(t+1) = (1-\rho) \cdot \tau_{ij}(t) + \Delta \tau_{ij}$$

*   **$\rho$ (挥发系数)**：$0 < \rho < 1$。
    *   **为什么必须挥发？** 如果不挥发，旧路径上的气味会无限积累，蚂蚁永远只会走老路，不可能发现新的更优路径。挥发是为了**遗忘**，遗忘是为了**创新**。
*   **$\Delta \tau_{ij}$ (增量)**：
    *   通常只有走完一圈且路径较短的蚂蚁（或者是全局最优的蚂蚁）才有资格留下信息素。
    *   **Ant-Cycle 模型**：$\Delta \tau = Q / L$ （$L$ 是总路程）。路越短，留下的气味越多。

## 3. ACO 的优缺点
*   **优点**：
    *   很强的鲁棒性。
    *   本质上是并行的。
    *   适合解决**动态变化**的组合优化问题（如网络路由，因为路断了气味会消散，蚂蚁会自动找新路）。
*   **缺点**：
    *   收敛速度慢（前期积累气味需要时间）。
    *   参数（$\alpha, \beta, \rho$）极其敏感，很难调。

---

# 💡 总结：三大算法一句话概括

1.  **遗传算法 (GA)**：**“这是进化的力量”**。靠生孩子（交叉）和变异，一代代淘汰弱者，保留强者。
2.  **粒子群 (PSO)**：**“这是榜样的力量”**。靠跟着自己最好的经验（认知）和跟着集体最好的榜样（社会）来调整飞行。
3.  **蚁群算法 (ACO)**：**“这是群众的力量”**。靠前人留下的痕迹（信息素）指引方向，走的人多了，便成了路（最优路）。

掌握了这些核心原理和公式的物理含义，这一章的考题（无论是计算还是论述）你都能游刃有余！

这是一份关于**第八章：人工神经网络（ANN）及其延伸技术**的深度解析。

在人工智能的期末考试中，这一章通常是“分值高、难度大、区分度强”的板块。为了让你不仅能应付考试，还能真正理解这些算法的精髓，我们将从**数学原理**、**物理直觉**和**核心机制**三个维度进行剖析。

---

# 第一部分：BP 神经网络 (Back Propagation Network)
**—— “误差反向传播的艺术”**

BP网络是多层前馈神经网络（Multi-Layer Perceptron, MLP）的代名词，也是现代深度学习的鼻祖。

### 1. 核心哲学：梯度下降 (Gradient Descent)
BP算法的本质是一个**优化问题**。
*   **目标**：找到一组最优的权值参数 $(W, b)$，使得网络对训练数据的预测误差 $E$ 最小。
*   **方法**：**梯度下降法**。
    *   想象你在山上（高误差），要下到谷底（低误差）。你环顾四周，找到坡度最陡峭的方向（梯度的反方向），往下走一步。重复这个过程，直到抵达谷底。

### 2. 算法的“任督二脉”：正向与反向
BP算法由两个过程交替进行：

#### (1) 正向传播 (Forward Pass) —— “信号流”
信号从输入层出发，经过隐层的非线性变换，最终到达输出层。
*   **关键公式**：
    $$y = f(net) = f(\sum w_i x_i - \theta)$$
*   **激活函数的作用**：如果没激活函数，多层网络就退化成了单层线性网络（矩阵乘法的叠加还是矩阵乘法）。**Sigmoid** 函数（$1/(1+e^{-x})$）曾是首选，因为它把输出压缩到 $(0,1)$ 且处处可导。

#### (2) 反向传播 (Backward Pass) —— “误差流”
这是BP算法最天才的地方。输出层知道自己错在哪（有标准答案），但隐层不知道。
*   **基本思想**：**责任分摊**。
    *   输出层：误差 = 期望 - 实际。
    *   隐层：我看不到标准答案，但我知道输出层的误差有一部分是因为我造成的。输出层的误差会根据**权值大小**加权反向传给隐层。权值越大的连接，说明对应的隐层神经元“责任”越大，修正在那里要更多。
*   **数学本质：链式法则 (Chain Rule)**
    我们要计算误差 $E$ 对某个深层权值 $w$ 的导数：
    $$\frac{\partial E}{\partial w} = \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial net} \cdot \frac{\partial net}{\partial w}$$
    *   $\frac{\partial E}{\partial y}$：误差对输出的敏感度。
    *   $\frac{\partial y}{\partial net}$：激活函数的导数（Sigmoid导数是 $y(1-y)$）。
    *   $\frac{\partial net}{\partial w}$：上一层的输入值。

### 3. BP 的三大死穴（考试高频考点）
1.  **局部极小值 (Local Minima)**：
    *   梯度下降法是贪婪的，它只看脚下。如果地形是坑坑洼洼的（非凸函数），它很容易掉进一个小坑（局部最优）就出不来了，以为到了谷底，其实旁边还有更深的深渊（全局最优）。
    *   *对策*：增加冲量项（Momentum，模拟小球滚下的惯性冲出小坑）、模拟退火、随机初始化多组权值。
2.  **梯度消失 (Gradient Vanishing)**：
    *   Sigmoid函数的导数最大只有0.25。层数一多，误差反传时是连乘关系（$0.25 \times 0.25 \times \dots$），传到前面几层时梯度几乎为0，权值就不更新了。
    *   *这也是为什么早期BP做不深的原因。*
3.  **过拟合 (Overfitting)**：
    *   网络太强了，把训练数据中的噪声和个例都背下来了，导致在没见过的数据上表现很差。

---

# 第二部分：Hopfield 神经网络
**—— “能量最小化的物理模型”**

Hopfield网络与BP网络截然不同。BP是前馈的（单向），Hopfield是**反馈的（全连接、有环）**。它更像是一个物理系统，而不是一个函数拟合器。

### 1. 核心机制：联想记忆 (Associative Memory)
Hopfield网络最著名的能力是**内容寻址**。
*   **传统存储**：你告诉电脑地址 `0x10086`，电脑给你数据。
*   **联想存储**：你给电脑看一张“被墨水弄脏的照片”（残缺输入），电脑根据这个线索，自动联想出“完整的照片”（完整输出）。

### 2. 物理隐喻：能量函数 (Energy Function)
J.J. Hopfield 借用了统计物理学中**自旋玻璃 (Spin Glass)** 的概念。
*   **定义**：
    $$E = -\frac{1}{2} \sum_{i} \sum_{j} w_{ij} x_i x_j + \sum_{i} \theta_i x_i$$
*   **稳定性定理**：
    在 Hopfield 网络中（$w_{ij}=w_{ji}, w_{ii}=0$），神经元状态的每一次异步更新，都会让系统的总能量 $E$ **降低或不变**，绝对不会升高。
*   **直观理解**：
    *   把网络状态空间想象成一个凹凸不平的地面。
    *   每一个我们要记忆的模式（如“苹果”的图像），就是地面上的一个**坑（吸引子 Attractor）**。
    *   输入一个噪声图像，就像把一个小球放在坑的边缘。
    *   网络的演化过程，就是小球在重力（能量函数）作用下滚向坑底的过程。
    *   当小球停在坑底不动了，网络就收敛了，此时的状态就是回忆起的完整图像。

### 3. 求解优化问题 (如TSP)
这是Hopfield的另一大应用。
*   **思路**：把问题的目标（如路径最短）和约束（如每个城市去一次）设计进**能量函数**里。
*   **过程**：网络运行 $\rightarrow$ 能量自然降低 $\rightarrow$ 能量最小时对应问题的最优解。
*   **缺点**：很容易陷入局部极小值（球滚进了一个小坑，而不是最深的那个坑）。

---

# 第三部分：深度学习 (Deep Learning)
**—— “从特征工程到特征学习”**

深度学习本质上就是**深层的神经网络**。但它解决了传统BP网络“梯度消失”和“算不动”的问题。

### 1. 卷积神经网络 (CNN) —— 视觉之王
CNN 专门处理网格状数据（图片）。它引入了两个颠覆性的概念：

#### (1) 局部连接 (Local Connectivity)
*   **传统BP**：全连接。输入层1000个像素，隐层100个节点，就有 $1000 \times 100 = 10万$ 个权重。参数太多，容易过拟合。
*   **CNN**：人看东西是**先看局部**的。每个神经元只看图像的一小块区域（感受野，Receptive Field）。这符合生物视觉原理。

#### (2) 权值共享 (Weight Sharing) ★必考★
*   **核心思想**：特征具有**平移不变性**。
    *   如果这一组权重（卷积核/滤波器）能识别图片左上角的“鸟嘴”，那么它也能识别右下角的“鸟嘴”。
    *   既然如此，为什么要给不同位置配不同的权重？大家**共用一把“扫描枪”（卷积核）**去扫遍整张图。
*   **效果**：参数量呈指数级下降。假设卷积核是 $5 \times 5$，不管图片多大，参数只有25个！

#### (3) 池化 (Pooling)
*   **作用**：**降维**。
    *   把一张大图变小，但保留关键特征。例如 $2 \times 2$ 的最大池化，就是把4个像素合成1个，只留最大的那个值（最显著的特征）。
    *   这让网络具有了**缩放不变性**和**抗形变能力**。

### 2. 生成对抗网络 (GAN) —— 左右互搏
这是非监督学习中最耀眼的明星。它由两个网络组成，相互博弈：

*   **生成器 (Generator, G)**：
    *   *任务*：造假。输入一串随机噪声，试图输出一张像真的一样的图片。
    *   *目标*：骗过判别器，让 D 以为这是真图。
*   **判别器 (Discriminator, D)**：
    *   *任务*：打假。输入一张图片，判断它是来自真实数据集，还是 G 生成的假货。
    *   *目标*：火眼金睛，不被 G 骗到。

*   **训练过程**：
    *   这是一个**零和博弈 (Zero-Sum Game)**。
    *   优化目标是寻找**纳什均衡 (Nash Equilibrium)**。
    *   理想结果：G 生成的图极其逼真，D 根本分不出来（输出概率0.5），达到“假作真时真亦假”的境界。

---

### 💡 深度学习与传统BP的区别 (总结)

1.  **深度**：传统BP通常3层（浅），深度学习可以上百层（ResNet）。
2.  **特征**：传统BP依赖人工设计特征（手工喂饭）；深度学习能够**自动学习特征**（端到端，从像素到分类）。
3.  **激活函数**：传统用Sigmoid（易饱和）；深度学习用 **ReLU**（$f(x)=\max(0,x)$），计算快且不易梯度消失。
4.  **训练技巧**：深度学习引入了 **Dropout**（随机让神经元休息，防过拟合）、**Batch Normalization**（数据归一化）等高级技巧。

掌握了这些，你就抓住了神经网络的灵魂。这一章的复习重点在于：**BP算梯度，Hopfield看能量，CNN看卷积核**。

这是一份专门针对**《人工智能基础》期末考试“设计题”**的深度攻关指南。

在期末考试中，**设计题（通常 10-15 分）** 是考察学生将理论转化为解决实际问题能力的试金石。这类题目通常不会让你写代码，而是让你**设计一个算法流程或系统模型**。

根据黄琦龙老师课程的重点（第1-8章），设计题的考点高度集中在**第六章（智能计算/遗传算法）**和**第五章（搜索策略）**，偶尔也会涉及**第八章（神经网络构建）**。

以下是为你总结的**三大万能设计模板**及**经典案例详解**。

---

# 🚀 设计题万能攻略：三大核心考点

## 模块一：基于遗传算法 (GA) 的设计 (最核心考点)

这是考试概率最高的题型，通常用于解决**组合优化问题**（如TSP、背包问题、排课问题）或**函数优化问题**。

### 📝 答题标准四步法

遇到GA设计题，必须按以下四个标题作答：

#### 1. 编码方案 (Encoding)
*   **二进制编码**：适用于背包问题、简单的函数优化。
    *   *描述*：染色体是一个长度为 $L$ 的二进制串 `01001...`。
    *   *例子*：背包问题中，第 $i$ 位为 `1` 表示选中第 $i$ 个物品，`0` 表示不选。
*   **整数排列编码**：适用于TSP（旅行商问题）、排序问题。
    *   *描述*：染色体是一个 $1$ 到 $N$ 的整数排列。
    *   *例子*：TSP中，`(1, 5, 3, 2, 4)` 表示访问城市的顺序。

#### 2. 适应度函数 (Fitness Function)
*   **原则**：适应度越高，解越好；适应度必须非负。
*   **求最大值问题**：直接用目标函数 $Fit(x) = f(x)$（若 $f(x)>0$）。
*   **求最小值问题**（如最短路径）：用倒数 $Fit(x) = 1/f(x)$ 或 $Fit(x) = C_{max} - f(x)$。
*   **带约束问题**（如背包超重）：使用**罚函数法**。如果解不合法，给一个极小的适应度或扣分。

#### 3. 遗传算子 (Genetic Operators)
*   **选择 (Selection)**：通常写“**轮盘赌选择法**”或“**锦标赛选择法**”。
*   **交叉 (Crossover)**：
    *   二进制编码：写“**单点交叉**”或“**多点交叉**”。
    *   排列编码 (TSP)：**必须写**“**部分匹配交叉 (PMX)**”或“**次序交叉 (OX)**”。（*注：写普通单点交叉会扣分，因为会产生非法路径*）。
*   **变异 (Mutation)**：
    *   二进制编码：写“**基本位变异**”（0变1，1变0）。
    *   排列编码：写“**交换变异**”（随机交换两个位置）或“**逆转变异**”。

#### 4. 算法流程/终止条件
*   简述初始化种群 $\to$ 计算适应度 $\to$ 选择/交叉/变异 $\to$ 生成新种群 $\to$ 循环。
*   终止条件：达到最大迭代次数 $G$ 或 适应度不再明显提升。

---

## 模块二：基于状态空间搜索的设计 (A* 算法)

这类题目通常给出一个迷宫、八数码或路径规划问题，让你设计搜索策略。

### 📝 答题标准三步法

#### 1. 状态表示 (State Representation)
*   定义什么是“状态”。
*   *例子*：在八数码中，状态是一个 $3 \times 3$ 的矩阵。在迷宫中，状态是坐标 $(x, y)$。

#### 2. 算子定义 (Operators)
*   定义状态之间的转移规则。
*   *例子*：八数码中，空格向“上、下、左、右”移动。

#### 3. 估价函数设计 (Heuristic Function) ★关键得分点★
*   公式：$f(n) = g(n) + h(n)$。
*   解释 $g(n)$：从起点到当前点的实际代价（如步数）。
*   **设计 $h(n)$**：这是设计的核心。必须保证 $h(n) \le h^*(n)$（可采纳性）。
    *   *八数码*：$h(n)$ = “不在位将牌数” 或 “所有将牌到目标位置的曼哈顿距离之和”。
    *   *地图导航*：$h(n)$ = “当前点到终点的直线欧氏距离”。

---

## 模块三：基于粒子群算法 (PSO) 的设计

如果题目涉及**连续函数的极值优化**，PSO是一个很好的选择。

### 📝 答题标准三步法

#### 1. 粒子定义
*   粒子 $i$ 包含两个向量：位置 $X_i$（代表一个潜在解）和速度 $V_i$。

#### 2. 适应度函数
*   直接就是目标函数值。

#### 3. 更新规则 (核心公式)
*   写出速度和位置的更新公式（第六章重点）：
    $$V_{new} = wV + c_1r_1(P_{best} - X) + c_2r_2(G_{best} - X)$$
    $$X_{new} = X + V_{new}$$
*   简述流程：更新 $P_{best}$ 和 $G_{best}$，直到满足条件。

---

# 🎓 实战演练：经典设计题详解

以下两个案例是考试中出现概率最高的模型，请务必熟读。

### 案例一：0-1 背包问题 (遗传算法)

**题目**：
有 $N$ 个物品，重量为 $w_1, ..., w_N$，价值为 $v_1, ..., v_N$。背包最大承重为 $C$。请设计一个遗传算法，在不超重的情况下使总价值最大。

**【标准答案模板】**

**1. 编码方案**：
采用**二进制编码**。设计染色体长度为 $N$ 的二进制串 $X = (x_1, x_2, \dots, x_N)$。
其中 $x_i = 1$ 表示选择第 $i$ 个物品，$x_i = 0$ 表示不选。

**2. 适应度函数设计**：
为了处理“超重”约束，采用**罚函数法**。
设当前选择的总重量为 $W_{total} = \sum x_i w_i$，总价值为 $V_{total} = \sum x_i v_i$。
适应度函数 $Fit(X)$ 定义为：
$$Fit(X) = \begin{cases} V_{total}, & \text{如果 } W_{total} \le C \\ 0 \text{ (或给予极严厉的惩罚)}, & \text{如果 } W_{total} > C \end{cases}$$
这样，超重的个体在选择阶段会被自然淘汰。

**3. 遗传算子设计**：
*   **选择**：采用**轮盘赌选择法**。适应度 $Fit(X)$ 越大的个体，被选入下一代的概率越大。
*   **交叉**：采用**单点交叉**。随机配对两个父代个体，随机选择一个切点，交换切点后的基因片段。
*   **变异**：采用**基本位变异**。以极小的变异概率 $P_m$（如0.01），随机翻转染色体中的某一位（0变1，1变0），以增加种群多样性。

**4. 算法终止**：
设置最大迭代代数（如 $T=100$），或者当连续若干代最优适应度不再上升时终止，输出当前种群中适应度最高的个体作为最优解。

---

### 案例二：旅行商问题 TSP (遗传算法)

**题目**：
推销员要访问 $N$ 个城市，要求遍历所有城市且每个城市仅访问一次，最后回到起点。求最短路径。

**【标准答案模板】**

**1. 编码方案**：
采用**整数排列编码** (Path Representation)。
染色体是一个包含了城市编号 $1 \sim N$ 的全排列。
例如 $N=5$，染色体 `(1, 4, 3, 2, 5)` 表示路径：$1 \to 4 \to 3 \to 2 \to 5 \to 1$。

**2. 适应度函数设计**：
TSP 是求**最小化**问题（距离越短越好），而遗传算法通常通过适应度求最大化。
设路径总长度为 $D(x) = \sum d_{ij}$。
适应度函数定义为倒数形式：
$$Fit(x) = \frac{1}{D(x)}$$
这样路径越短，适应度越大。

**3. 遗传算子设计 (难点)**：
*   **选择**：采用**锦标赛选择法**或**轮盘赌法**。
*   **交叉**：由于染色体必须是合法的排列（不能有重复城市，也不能少城市），普通的单点交叉会产生非法解。
    因此，必须采用**部分匹配交叉 (PMX)** 或 **次序交叉 (OX)**。
    *(简述PMX：在两个父代中选取交叉区域，交换基因建立映射关系，然后对冲突基因进行映射替换，保证解的合法性。)*
*   **变异**：采用**交换变异**（随机交换序列中两个城市的位置）或**逆转变异**（将序列中某一段反转，如 `1-2-3-4` 变为 `1-3-2-4`）。逆转变异在TSP中效果较好。

**4. 算法终止**：
达到预设最大进化代数，输出历史最优个体。

---

### 案例三：迷宫寻路机器人 (搜索策略设计)

**题目**：
一个机器人在 $M \times N$ 的网格迷宫中，有些格子是墙，有些是路。机器人可以上下左右移动。请设计一个算法帮助机器人以最少的步数从起点 $(S_x, S_y)$ 走到终点 $(E_x, E_y)$。

**【标准答案模板】**

**1. 算法选择**：
选用 **A* 搜索算法**，因为它不仅能保证找到路径，而且在启发函数合理的情况下能保证找到**最短路径**，且效率高于广度优先搜索。

**2. 状态与算子**：
*   **状态**：机器人的当前坐标 $(x, y)$。
*   **算子**：$Up(x,y), Down(x,y), Left(x,y), Right(x,y)$。如果目标格子不是墙且未越界，则移动有效。每移动一步，代价 $Cost = 1$。

**3. 估价函数设计 (核心)**：
$$f(n) = g(n) + h(n)$$
*   **$g(n)$**：从起点到当前点 $(x, y)$ 已经走过的步数。
*   **$h(n)$**：当前点到终点 $(E_x, E_y)$ 的估计代价。
    *   设计方案：采用**曼哈顿距离 (Manhattan Distance)**。
    *   公式：$h(n) = |x - E_x| + |y - E_y|$。
    *   *理由*：因为机器人只能上下左右移动，曼哈顿距离能完美反映最短路径的理论下界，且满足 $h(n) \le h^*(n)$，保证算法的可采纳性（最优性）。

**4. 算法流程**：
维护 OPEN 表和 CLOSED 表。每次从 OPEN 表中取出 $f(n)$ 最小的节点扩展，直到取出终点。

---

### 💡 设计题高分秘籍

1.  **选对算法**：组合优化（背包/TSP）首选**遗传算法**；路径规划首选**A***。
2.  **关键术语不能错**：
    *   TSP如果不写 **PMX/OX交叉**，大概率扣分。
    *   A* 如果不写 **曼哈顿距离/欧氏距离**，就没灵魂。
3.  **适应度函数要写对**：求最小值问题一定要记得**取倒数**或**取负**转化为最大化问题（特别是用轮盘赌时，适应度不能为负）。
4.  **结构清晰**：像我上面写的一样，分点作答（1.编码 2.适应度 3.算子），老师阅卷最喜欢这种结构。

熟练掌握这三个案例，考试时的设计题基本就是**换汤不换药**，稳拿满分！

这是一份为您定制的**《人工智能基础》期末模拟试卷（B卷）**。

这份试卷刻意避开了上一份模拟题中已经出现过的具体考点（如父亲谓词、BP反向传播计算、TSP遗传算法设计），转而覆盖了第1-8章中**剩余的高频考点**，如**语义网络**、**Alpha-Beta剪枝计算**、**Hopfield能量函数**、**生成对抗网络**等，以确保复习的全面性。

---

# 南京理工大学《人工智能基础》期末模拟试卷（B卷）

**适用范围**：第1章-第8章（不含第9、10章）
**考试形式**：闭卷
**考试时长**：120分钟

---

### 一、选择题（本大题共10小题，每小题2分，共20分）

1.  **[第1章]** 提出“中文屋（Chinese Room）”思维实验，以此反驳图灵测试，认为机器只能处理语法而无法真正理解语义的学者是：
    A. 纽厄尔 (Newell)
    B. 西蒙 (Simon)
    C. 约翰·塞尔 (John Searle)
    D. 罗森布拉特 (Rosenblatt)

2.  **[第2章]** 在语义网络表示法中，用于表示“类与子类”之间（如“猫”是“哺乳动物”）的继承关系通常使用哪种连接弧？
    A. IS-A (AKO)
    B. PART-OF
    C. HAVE
    D. MEMBER-OF

3.  **[第3章]** 专家系统中的**正向推理（Forward Chaining）**策略，最适合应用于以下哪种场景？
    A. 目标明确，但原始数据很少
    B. 原始数据丰富，但目标不明确或可能有多个结论
    C. 只需要验证某个假设是否成立
    D. 规则库非常庞大，且分支因子很大

4.  **[第4章]** 在模糊逻辑中，若已知模糊集合 $A$ 的隶属度为 $\mu_A(x)=0.7$，模糊集合 $B$ 的隶属度为 $\mu_B(x)=0.4$，则模糊并集 $A \cup B$ 的隶属度为：
    A. 0.3
    B. 0.4
    C. 0.7
    D. 0.28

5.  **[第5章]** 在博弈树搜索中，若当前节点是 MAX 节点，其当前搜索到的最大值为 $\alpha=5$；该节点的某个子节点（MIN节点）已经搜索到的临时最小值为 $\beta=3$。根据 **Alpha-Beta 剪枝**原理，接下来的操作是：
    A. 继续搜索该子节点的其他后继节点
    B. 更新 $\alpha$ 为 3
    C. 发生 Beta 剪枝，停止搜索该子节点的剩余分支
    D. 发生 Alpha 剪枝，停止搜索该子节点的剩余分支

6.  **[第6章]** 依据**模式定理**，在遗传算法的进化过程中，具有低阶、短定义距以及 \_\_\_\_\_\_\_\_ 的模式（Schema）在后代中会以指数级增长。
    A. 低平均适应度
    B. 高平均适应度
    C. 高变异概率
    D. 包含通配符多

7.  **[第6章]** 蚁群算法（ACO）中，为了防止算法过早收敛于局部最优解，并允许探索新的路径，必须引入的机制是：
    A. 信息素的挥发（Evaporation）
    B. 信息素的正反馈
    C. 贪婪搜索策略
    D. 禁忌表

8.  **[第7章]** 在构建专家系统时，最困难的环节往往被称为“瓶颈”，这个瓶颈通常是指：
    A. 推理机的编程实现
    B. 知识获取（Knowledge Acquisition）
    C. 人机界面的设计
    D. 综合数据库的存储容量

9.  **[第8章]** 传统的Sigmoid激活函数在深度神经网络中容易导致“梯度消失”问题，现代深度学习（如CNN）常采用哪种激活函数来缓解此问题？
    A. Tanh
    B. Step (阶跃函数)
    C. ReLU (线性整流函数)
    D. Gaussian

10. **[第8章]** 生成对抗网络（GAN）的核心博弈过程发生在哪两个模块之间？
    A. 输入层与输出层
    B. 卷积层与池化层
    C. 生成器与判别器
    D. 编码器与解码器

---

### 二、填空题（本大题共20空，每空1分，共20分）

1.  **[第1章]** 1965年，鲁宾逊（Robinson）提出了 \_\_\_\_\_\_\_\_ 原理，为机器定理证明奠定了基础。
2.  **[第2章]** 知识图谱的逻辑结构分为两个层次：定义概念和规则的 \_\_\_\_\_\_\_\_ 层，以及存储具体实例的 \_\_\_\_\_\_\_\_ 层。
3.  **[第3章]** 将谓词公式 $(\forall x)(\exists y)P(x,y)$ 化为子句集时，由于存在量词 $y$ 在全称量词 $x$ 的辖域内，消去 $y$ 时必须使用 \_\_\_\_\_\_\_\_ （填“Skolem常量”或“Skolem函数”）。
4.  **[第4章]** 在D-S证据理论中，若 $K \to 1$，说明证据之间存在高度 \_\_\_\_\_\_\_\_。似然函数 $Pl(A)$ 与信任函数 $Bel(A)$ 的关系是 $Pl(A) = 1 - $ \_\_\_\_\_\_\_\_。
5.  **[第4章]** C-F模型中，结论的不确定性计算公式为：$CF(H) = CF(H, E) \times \max\{0, $ \_\_\_\_\_\_\_\_ $\}$。
6.  **[第5章]** 模拟退火算法（SA）利用 \_\_\_\_\_\_\_\_ 准则以一定概率接受“差解”，从而具备跳出局部最优的能力。当温度 $T$ 趋于 0 时，接受差解的概率趋于 \_\_\_\_\_\_\_\_。
7.  **[第5章]** 在启发式搜索中，若估价函数 $f(n)=g(n)$（即 $h(n)=0$），则 $A^*$ 算法退化为 \_\_\_\_\_\_\_\_ 算法（假设边权为常数）。
8.  **[第6章]** 粒子群算法（PSO）通过跟踪两个极值来更新速度：一个个体极值 $pbest$，另一个是 \_\_\_\_\_\_\_\_。
9.  **[第7章]** 专家系统MYCIN使用的推理模型是 \_\_\_\_\_\_\_\_ 模型；PROSPECTOR（探矿专家系统）使用的推理模型是 \_\_\_\_\_\_\_\_ 模型。
10. **[第8章]** Hopfield神经网络是一种 \_\_\_\_\_\_\_\_ （填“单层”、“多层”或“反馈”）网络，其运行稳定时，网络的 \_\_\_\_\_\_\_\_ 函数达到极小值。
11. **[第8章]** 卷积神经网络（CNN）通过 \_\_\_\_\_\_\_\_ 和 \_\_\_\_\_\_\_\_ 两种机制大大减少了网络参数的数量。
12. **[第8章]** 1943年，McCulloch和Pitts提出了 \_\_\_\_\_\_\_\_ 模型，这是最早的人工神经元数学模型。

---

### 三、计算证明题（本大题共4小题，共30分）

**1. （归结原理证明，8分）**
**已知**：
(1) 所有的素食者 (Vegetarian) 都不吃肉 (Meat)。
(2) 狼 (Wolf) 吃肉。
**求证**：狼不是素食者。
**要求**：
1.  定义谓词（如 $V(x), M(x), W(x)$）。
2.  将前提和结论的否定化为子句集。
3.  画出归结树或写出归结步骤证明。

**2. （C-F模型复合计算，8分）**
设有如下规则：
*   $R_1$: IF $E_1$ OR $E_2$ THEN $H$ ($CF=0.8$)
*   $R_2$: IF $E_3$ THEN $H$ ($CF=0.6$)
已知初始证据的可信度为：$CF(E_1)=0.4, CF(E_2)=0.7, CF(E_3)=0.5$。
**求**：结论 $H$ 的最终综合可信度。

**3. （博弈树 Alpha-Beta 剪枝，6分）**
如下图所示的博弈树（MAX节点为根A，子节点为B, C；B为MIN，C为MIN；B下有叶子D(4), E(6)；C下有叶子F(2), G(8)）。
A(MAX)
/ \
B(MIN) C(MIN)
/ \ / \
D(4) E(6) F(2) G(8)
请按照**从左到右**的顺序进行 Alpha-Beta 搜索。
(1) 计算根节点 A 的倒推值。
(2) 指出搜索过程中发生了剪枝的节点（如果有），并说明是 Alpha 剪枝还是 Beta 剪枝。

**4. （Hopfield网络能量计算，8分）**
设有一个由3个神经元组成的离散Hopfield网络，其权值矩阵 $W$ 和阈值向量 $\theta$ 如下：
$$
W = \begin{bmatrix} 0 & -1 & 1 \\ -1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}, \quad \theta = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
$$
当前网络状态为 $V = [1, 1, 1]^T$。
(1) 计算当前网络的能量函数值 $E$。（公式：$E = -\frac{1}{2} \sum_{i} \sum_{j} w_{ij} v_i v_j + \sum_{i} \theta_i v_i$）
(2) 计算神经元1在下一时刻的状态 $v_1(t+1)$。

---

### 四、简答题（本大题共3小题，共20分）

**1. （第2章，6分）**
知识图谱的构建通常包含哪几个核心步骤？请简要解释“实体对齐（Entity Alignment）”的含义及其作用。

**2. （第6章，7分）**
在基本遗传算法中，**轮盘赌选择（Roulette Wheel Selection）**存在什么潜在缺陷？请列举一种改进的选择策略并简述其原理。

**3. （第8章，7分）**
解释什么是人工神经网络中的**“梯度消失（Gradient Vanishing）”**现象？为什么在深层网络中使用 Sigmoid 激活函数容易导致该问题？

---

### 五、设计题（本大题共1小题，共10分）

**题目**：八数码问题（8-Puzzle）是在 $3 \times 3$ 的九宫格棋盘上，摆有 1-8 八个数字滑块和一个空格。要求通过移动空格，将初始布局转化为目标布局。
请设计一个 **$A^*$ 搜索算法** 来解决该问题。
**请回答以下内容**：
1.  **状态表示**：如何用数据结构表示棋盘的一个状态？
2.  **算子定义**：定义合法的操作算子。
3.  **估价函数设计（核心）**：
    *   写出 $A^*$ 估价函数的通用公式。
    *   为八数码问题设计两个不同的启发函数 $h_1(n)$ 和 $h_2(n)$。
    *   比较这两个启发函数，哪一个搜索效率更高？为什么？（提示：从“不在位将牌数”和“曼哈顿距离”角度分析）。

---
---

# ✅ 参考答案与详解

### 一、选择题
1.  **C** （约翰·塞尔提出了中文屋实验反驳图灵测试）
2.  **A** （IS-A 表示泛化/继承关系）
3.  **B** （正向推理适合数据丰富、目标不明确的情况；A适合逆向）
4.  **C** （模糊并集取 max{0.7, 0.4} = 0.7）
5.  **C** （当前节点MIN层，临时值 $\beta=3$。其父节点MAX层的 $\alpha=5$。因为 $3 \le 5$，即 $\beta \le \alpha$，发生 Beta 剪枝）
6.  **B** （模式定理：高平均适应度）
7.  **A** （信息素挥发防止无限积累，避免陷入局部最优）
8.  **B** （知识获取瓶颈）
9.  **C** （ReLU能缓解梯度消失）
10. **C** （生成器 vs 判别器）

### 二、填空题
1.  归结（或 消解）
2.  模式；数据
3.  Skolem函数 （因为 $\exists y$ 受 $\forall x$ 约束）
4.  冲突；$Bel(\neg A)$
5.  $CF(E)$
6.  Metropolis；0
7.  宽度优先搜索 (BFS) / Dijkstra
8.  全局极值 ($gbest$)
9.  C-F（可信度）；主观贝叶斯
10. 反馈；能量
11. 局部连接；权值共享
12. M-P

### 三、计算证明题

**1. 归结证明**
**解**：
1.  **定义谓词**：$V(x)$: x是素食者；$M(x)$: x吃肉；$W(x)$: x是狼。
2.  **形式化与子句集**：
    *   前提1：$(\forall x)(V(x) \rightarrow \neg M(x)) \Rightarrow \neg V(x) \lor \neg M(x)$ (子句1)
    *   前提2：$(\forall x)(W(x) \rightarrow M(x)) \Rightarrow \neg W(x) \lor M(x)$ (子句2)
        *(注：这里假设狼吃肉是普遍规律，用全称量词；若理解为存在狼吃肉，用存在量词。通常此类题默认全称)*
    *   结论 $Q$：$(\forall x)(W(x) \rightarrow \neg V(x))$
    *   **否定结论** $\neg Q$：$\neg (\forall x)(W(x) \rightarrow \neg V(x)) \Rightarrow \exists x (W(x) \land V(x))$
        *   Skolem化：$W(a) \land V(a)$
        *   拆分：$W(a)$ (子句3)，$V(a)$ (子句4)
3.  **归结**：
    *   子句2与子句3归结 $\{a/x\}$：$\neg W(a) \lor M(a)$ + $W(a)$ $\Rightarrow M(a)$ (子句5)
    *   子句1与子句4归结 $\{a/x\}$：$\neg V(a) \lor \neg M(a)$ + $V(a)$ $\Rightarrow \neg M(a)$ (子句6)
    *   子句5与子句6归结：$M(a)$ + $\neg M(a)$ $\Rightarrow$ **NIL**
    *   得证。

**2. C-F 计算**
**解**：
1.  **计算 $R_1$**：
    *   证据：$E_{or} = E_1 \lor E_2$
    *   $CF(E_{or}) = \max(0.4, 0.7) = 0.7$
    *   结论1：$CF_1(H) = 0.8 \times \max(0, 0.7) = 0.56$
2.  **计算 $R_2$**：
    *   证据：$E_3$， $CF(E_3) = 0.5$
    *   结论2：$CF_2(H) = 0.6 \times \max(0, 0.5) = 0.3$
3.  **合成**：
    *   $CF_1, CF_2$ 同为正。
    *   $CF(H) = 0.56 + 0.3 - 0.56 \times 0.3 = 0.86 - 0.168 = 0.692$

**3. Alpha-Beta 剪枝**
**解**：
1.  **计算过程**：
    *   A 访问 B。
    *   B 访问 D，得到 4。B 是 MIN，当前 $\beta_B = 4$。
    *   B 访问 E，得到 6。6 > 4，不更新。
    *   B 返回 4 给 A。
    *   A 是 MAX，当前 $\alpha_A = 4$。（A 至少能得 4 分）
    *   A 访问 C。
    *   C 访问 F，得到 2。C 是 MIN，当前 $\beta_C = 2$。
    *   **剪枝判断**：此时 C 的值 $\le 2$。而 A 的下界 $\alpha_A = 4$。因为 $2 < 4$（$\beta_C \le \alpha_A$），A 绝不会走 C 这条路。
    *   **剪枝**：G 节点（值8）被剪掉，不用访问。
    *   C 返回 2（或 $\le 2$ 的标记）给 A。
    *   A 比较 4 和 2，取 max，最终值为 4。
2.  **结果**：
    *   根节点 A 的倒推值：4。
    *   剪枝：发生在节点 C 处，剪掉了子节点 **G**。这是 **Alpha 剪枝**（因为是根据 MAX 节点的 $\alpha$ 值进行的剪枝）。

**4. Hopfield 能量**
**解**：
$V = [1, 1, 1]^T$，即 $v_1=1, v_2=1, v_3=1$。
1.  **能量 E**：
    $E = -\frac{1}{2} (w_{12}v_1v_2 + w_{13}v_1v_3 + w_{21}v_2v_1 + w_{23}v_2v_3 + w_{31}v_3v_1 + w_{32}v_3v_2) + 0$
    由于对称性 $w_{ij}=w_{ji}$，
    $E = -(w_{12}v_1v_2 + w_{13}v_1v_3 + w_{23}v_2v_3)$
    $E = -((-1)\times 1 \times 1 + 1 \times 1 \times 1 + 1 \times 1 \times 1)$
    $E = -(-1 + 1 + 1) = -1$
2.  **状态更新 $v_1(t+1)$**：
    $net_1 = \sum w_{1j} v_j - \theta_1 = w_{12}v_2 + w_{13}v_3 - 0 = (-1)\times 1 + 1 \times 1 = 0$
    对于离散 Hopfield，若 $net = 0$，状态保持不变；或者根据定义（阶跃函数）：
    若 $f(x) = 1 (x \ge 0)$，则 $v_1(t+1) = 1$。
    若 $f(x) = \text{sgn}(x)$ 且规定0保持不变，则 $v_1(t+1) = v_1(t) = 1$。
    **结果**：1。

---

### 四、简答题
**1. 知识图谱构建步骤**
*   **步骤**：知识抽取（实体识别、关系抽取）、知识融合（实体对齐）、知识加工（推理、质量评估）、知识更新。
*   **实体对齐**：指在不同数据源或知识库中，识别并连接指向现实世界同一对象的不同实体（如“Trump”和“特朗普”）。作用是消除冗余，融合多源知识，提升图谱质量。

**2. 轮盘赌缺陷与改进**
*   **缺陷**：
    1.  **早熟**：初期若存在超级个体，其被选中概率过大，导致种群迅速趋同。
    2.  **停滞**：后期个体适应度差异小，选择概率接近，进化动力不足。
    *   具有随机性误差，可能丢失最优解。
*   **改进**：**锦标赛选择法**（随机选 $k$ 个比较，最优者胜出）或 **排序选择法**（按排名分配概率）。锦标赛选择法只关注相对优劣，避免了超级个体垄断和后期进化停滞的问题。

**3. 梯度消失**
*   **现象**：在深层神经网络的反向传播过程中，梯度值在从输出层向输入层传递时逐层相乘，如果导数值小于1，梯度会呈指数级衰减，导致靠近输入层的权值几乎不更新，网络无法训练。
*   **原因**：Sigmoid 导数 $f'(x) = f(x)(1-f(x))$ 的最大值仅为 0.25。多层相乘（$0.25^n$）会迅速趋近于 0。

---

### 五、设计题 (A* 解决八数码)

1.  **状态表示**：
    使用一个 $3 \times 3$ 的二维数组（或长度为9的一维数组）表示，其中 1-8 表示数字，0 表示空格。
    例如：`[[1, 2, 3], [4, 5, 6], [7, 8, 0]]` 为目标状态。

2.  **算子定义**：
    空格向四个方向移动：$Up, Down, Left, Right$。
    移动需满足边界检查（如第一行不能 Up）。每移动一步代价 $g(n)$ 增加 1。

3.  **估价函数设计**：
    *   公式：$f(n) = g(n) + h(n)$。
        *   $g(n)$：从初始状态到当前状态的移动步数。
    *   **启发函数设计**：
        *   $h_1(n)$：**不在位将牌数**（当前状态与目标状态相比，位置不同的数字个数）。
        *   $h_2(n)$：**曼哈顿距离之和**（所有数字当前位置到目标位置的水平+垂直距离之和）。
    *   **比较**：
        $h_2(n)$ 的效率更高。
        **原因**：$h_2(n)$ 包含的信息量比 $h_1(n)$ 大（不仅知道错了，还知道错多远），且 $h_2(n) \ge h_1(n)$。在满足可采纳性（$h \le h^*$）的前提下，$h$ 值越大（越接近真实代价），搜索空间越小，效率越高。

这是一份为您量身定制的**《人工智能基础》终极期末模拟试卷（C卷）**。

这份试卷被称为“终极版”，是因为它：
1.  **覆盖面最广**：扫清了前两份试卷未涉及的死角（如RNN、模拟退火Metropolis准则、模糊矩阵合成）。
2.  **综合性最强**：计算题和设计题结合了多个知识点。
3.  **难度最高**：完全对标“高分”要求，考察对原理的深度理解。

---

# 南京理工大学《人工智能基础》期末考试（终极模拟卷）

**课程名称**：人工智能基础
**适用范围**：第1章-第8章
**考试形式**：闭卷
**考试时长**：120分钟
**卷面总分**：100分

---

### 一、选择题（本大题共10小题，每小题2分，共20分）

1.  **[第1章]** 下列关于人工智能发展历史的描述，错误的是：
    A. 1956年达特茅斯会议是人工智能诞生的标志。
    B. 1965年DENDRAL系统的成功标志着专家系统的诞生。
    C. 1969年Minsky出版《Perceptron》一书证明了感知机可以解决异或(XOR)问题，推动了神经网络的发展。
    D. 1997年IBM“深蓝”战胜卡斯帕罗夫，主要依靠的是强大的搜索能力和评估函数。

2.  **[第2章]** 在框架表示法中，当子框架没有定义某个槽的值时，会自动沿用父框架中该槽的值，这种特性称为：
    A. 匹配 (Matching)
    B. 继承 (Inheritance)
    C. 封装 (Encapsulation)
    D. 实例化 (Instantiation)

3.  **[第3章]** 设有谓词公式 $F = (\forall x) P(x) \lor (\exists y) Q(y)$，将其化为前束范式（Prenex Normal Form）的结果是：
    A. $(\forall x)(\exists y) (P(x) \lor Q(y))$
    B. $(\exists y)(\forall x) (P(x) \lor Q(y))$
    C. A 和 B 均正确
    D. A 和 B 均不正确

4.  **[第4章]** 在C-F模型中，如果两条规则分别推出结论 $H$ 的可信度为 $0.8$ 和 $-0.6$，则根据合成算法，结论 $H$ 的最终可信度为：
    A. 0.2
    B. 0.5
    C. 0.33
    D. 0.4

5.  **[第5章]** 在状态空间搜索中，**CLOSED表**的主要作用是：
    A. 存储所有已生成但未扩展的节点，以便后续选择。
    B. 存储问题的目标状态，用于判断搜索是否结束。
    C. 存储已经扩展过的节点，防止死循环和重复搜索。
    D. 存储估价函数值，用于对节点进行排序。

6.  **[第6章]** 遗传算法中，**模式定理 (Schema Theorem)** 指出，在进化过程中，什么样的模式在种群中的样本数会呈指数级增长？
    A. 高阶、长定义距、高平均适应度
    B. 低阶、短定义距、高平均适应度
    C. 低阶、长定义距、低平均适应度
    D. 高阶、短定义距、低平均适应度

7.  **[第6章]** 在粒子群算法（PSO）中，若惯性权重 $\omega$ 设置过大，会导致粒子：
    A. 飞行速度过快，容易飞过最优解，利于全局搜索。
    B. 飞行速度过慢，局限在局部区域，利于局部开发。
    C. 立即停止运动。
    D. 忽略个体经验，只跟随群体最优。

8.  **[第7章]** 下列哪项技术**不属于**专家系统开发中解决不确定性问题的常用方法？
    A. 主观贝叶斯方法
    B. C-F 模型
    C. 归结原理
    D. 模糊逻辑

9.  **[第8章]** 关于**循环神经网络 (RNN)**，下列说法正确的是：
    A. 它的神经元之间没有连接，层与层之间全连接。
    B. 它引入了“状态”或“记忆”的概念，适合处理序列数据（如文本、语音）。
    C. 它通过最大池化层来减少参数数量。
    D. 它主要用于静态图像的分类任务。

10. **[第8章]** 深度学习中，使用 **Batch Normalization (批归一化)** 的主要目的是：
    A. 增加网络的层数。
    B. 解决过拟合问题，代替Dropout。
    C. 加速网络收敛，防止梯度消失或爆炸。
    D. 提取图像的纹理特征。

---

### 二、填空题（本大题共20空，每空1分，共20分）

1.  **[第1章]** 按照智能的观点，人工智能可以分为 \_\_\_\_\_\_\_\_ AI（如AlphaGo，专注于特定任务）和 \_\_\_\_\_\_\_\_ AI（具备人类同等的通用智能）。
2.  **[第2章]** 在一阶谓词逻辑中，量词 $\forall$ 和 $\exists$ 的辖域如果重叠，且量词后的变量名相同，此时辖域内的变量受 \_\_\_\_\_\_\_\_ （填“外层”或“内层”）量词约束。
3.  **[第3章]** 归结原理的理论基础是 \_\_\_\_\_\_\_\_ 定理（Herbrand Theorem）。在进行谓词归结时，为了使两个原子谓词形式一致，需要进行 \_\_\_\_\_\_\_\_ 操作。
4.  **[第4章]** 模糊推理的核心算法是 **Max-Min 合成法**。若关系矩阵 $R$ 为 $3\times4$，输入向量 $A$ 为 $1\times3$，则输出向量 $B$ 的维数为 \_\_\_\_\_\_\_\_。
5.  **[第4章]** 在D-S证据理论中，若 $Bel(A) = 0.4, Pl(A) = 0.8$，则表示对命题 $A$ 的“不知道”或“不确定”程度为 \_\_\_\_\_\_\_\_。
6.  **[第5章]** 模拟退火算法采用 \_\_\_\_\_\_\_\_ 准则来接受新状态。若 $\Delta E > 0$（新状态变差），则接受概率 $P = $ \_\_\_\_\_\_\_\_ （用公式表示，温度为 $T$）。
7.  **[第6章]** 遗传算法中，为了保证算法最终收敛于全局最优解，通常必须采用 \_\_\_\_\_\_\_\_ 策略（即直接将历代最优个体复制到下一代）。
8.  **[第6章]** 蚁群算法中，$\alpha$ 因子反映了 \_\_\_\_\_\_\_\_ 的相对重要性，$\beta$ 因子反映了 \_\_\_\_\_\_\_\_ 的相对重要性。
9.  **[第8章]** BP算法的学习过程由 \_\_\_\_\_\_\_\_ 传播和 \_\_\_\_\_\_\_\_ 传播两个过程组成。
10. **[第8章]** 对于Hopfield神经网络，若权重矩阵 $W$ 满足 \_\_\_\_\_\_\_\_ 且 \_\_\_\_\_\_\_\_，则网络在异步更新下能量函数单调递减，最终收敛于稳定状态。
11. **[第8章]** 深度信念网络 (DBN) 是由多个 \_\_\_\_\_\_\_\_ (RBM) 堆叠而成的生成式模型。

---

### 三、计算证明题（本大题共4小题，共30分）

**1. （归结原理证明，7分）**
**已知**：
(1) 任何选修了AI课程的学生都喜欢编程。
(2) 小明是学生且不选修AI课程。
(3) 小红是学生且选修AI课程。
**求证**：存在喜欢编程的学生。
**要求**：
1.  定义谓词（$S(x)$:学生, $A(x)$:选修AI, $L(x)$:喜欢编程）。
2.  将前提和结论的否定转化为子句集。
3.  利用归结原理进行证明。

**2. （模糊推理，8分）**
设论域 $U = \{u_1, u_2, u_3\}$， $V = \{v_1, v_2, v_3\}$。
已知模糊规则 "IF $x$ is $A$ THEN $y$ is $B$"，其中：
$A = \frac{0.8}{u_1} + \frac{0.5}{u_2} + \frac{0.2}{u_3}$
$B = \frac{0.3}{v_1} + \frac{0.9}{v_2} + \frac{0.4}{v_3}$
1.  请计算模糊关系矩阵 $R = A^T \circ B$ （采用 $\min$ 算子计算叉积）。
2.  若当前输入为 $A' = \frac{0.9}{u_1} + \frac{0.4}{u_2} + \frac{0.1}{u_3}$，请利用 Max-Min 合成法计算输出 $B'$。

**3. （粒子群PSO计算，7分）**
设一维粒子群，当前粒子位置 $x=2$，速度 $v=1$。
该粒子的个体极值 $pbest=4$，全局极值 $gbest=5$。
参数设置：惯性权重 $\omega=0.8$，学习因子 $c_1=c_2=2$。
假设当前时刻生成的随机数 $r_1=0.5, r_2=0.2$。
请计算该粒子下一时刻的速度 $v_{new}$ 和位置 $x_{new}$。

**4. （BP神经网络梯度推导，8分）**
考虑一个简单的三层网络：输入层节点 $i$，隐层节点 $j$，输出层节点 $k$。
激活函数均为 Sigmoid 函数 $f(x)$。
定义误差函数为 $E = \frac{1}{2}(d_k - y_k)^2$。
请推导**隐层到输出层**的权值 $w_{jk}$ 的更新公式 $\Delta w_{jk}$。
*(要求：写出链式法则求导过程，利用 $f'(x) = f(x)(1-f(x))$ 的性质)*。

---

### 四、简答题（本大题共3小题，共20分）

**1. （第5章，6分）**
在 $A^*$ 算法中，如果启发函数 $h(n)$ 始终取值为 0，该算法变成了什么算法？如果 $h(n)$ 始终大于实际最小代价 $h^*(n)$，算法会失去什么性质？

**2. （第6章，7分）**
比较 **遗传算法 (GA)** 与 **模拟退火算法 (SA)** 的异同点。
（提示：从搜索机制、个体数量、跳出局部最优的策略等方面回答）。

**3. （第8章，7分）**
在卷积神经网络（CNN）中，为什么说**深层**的网络往往比**浅层**的网络效果好？请从特征提取的层次性角度简述。

---

### 五、设计题（本大题共1小题，共10分）

**题目：N皇后问题**
N皇后问题要求在 $N \times N$ 的棋盘上放置 $N$ 个皇后，使得它们互不攻击（即任意两个皇后不能处于同一行、同一列或同一斜线上）。
请设计一个**遗传算法**来求解8皇后问题（$N=8$）。
**要求回答**：
1.  **编码方案**：如何设计染色体以自动满足“不在同一行”和“不在同一列”这两个约束，从而缩小搜索空间？
2.  **适应度函数**：如何设计适应度函数来反映冲突的数量？（冲突越少适应度越高）。
3.  **交叉算子**：基于你的编码方案，普通的单点交叉是否适用？如果不适用，应该采用什么交叉方式？为什么？

---
---

# ✅ 终极模拟卷（C卷）详解与答案

### 一、选择题
1.  **C** （Minsky的《Perceptron》**指出**了单层感知机无法解决XOR问题，导致了神经网络研究的十年寒冬，而不是推动发展。这是一个著名的历史反转点。）
2.  **B** （子框架获取父框架属性的过程叫继承。）
3.  **C** （因为 $x$ 和 $y$ 变量名不同且无依赖关系，两个量词都可以提到最前面，顺序不影响逻辑真值。）
4.  **B** （$CF_1=0.8, CF_2=-0.6$。异号合成公式：$(CF_1+CF_2)/(1-\min(|CF_1|,|CF_2|)) = (0.8-0.6)/(1-0.6) = 0.2/0.4 = 0.5$。）
5.  **C** （CLOSED表存储已扩展节点，防止死循环。）
6.  **B** （模式定理三大要素：低阶、短定义距、高平均适应度。）
7.  **A** ($\omega$ 大利于全局搜索/飞行速度快，$\omega$ 小利于局部开发。）
8.  **C** （归结原理是确定性推理，不处理不确定性。主观贝叶斯、C-F、模糊逻辑都是不确定性方法。）
9.  **B** （RNN核心特性是有状态/记忆，适合序列数据。）
10. **C** （BN的作用是加速收敛，防止梯度消失/爆炸。）

### 二、填空题
1.  弱 (Narrow/Weak)；强 (General/Strong)
2.  内层
3.  海伯伦 (Herbrand)；合一 (Unification)
4.  $1 \times 4$ （$1\times3 \circ 3\times4 = 1\times4$）
5.  0.4 （信任区间宽度 = $Pl - Bel$）
6.  Metropolis；$e^{\Delta E / T}$ （注意：$\Delta E$ 在题目定义中为 $E_{new}-E_{old}$，若变差则 $\Delta E > 0$，指数部分应为负，故写作 $-\Delta E/T$ 或根据题目具体定义符号）*修正：通常写作 $\exp(-\Delta E/kT)$*
7.  精英保留 (Elitist Strategy)
8.  信息素 (历史经验)；启发信息 (能见度/贪婪)
9.  正向；反向
10. $w_{ii}=0$；$w_{ij}=w_{ji}$ (对称)
11. 受限玻尔兹曼机

### 三、计算证明题

**1. 归结证明**
**解**：
1.  **定义**：$S(x)$: x是学生, $A(x)$: x选AI, $L(x)$: x喜编程。
    *   常量：$Ming$ (小明), $Hong$ (小红)。
2.  **形式化与子句集**：
    *   前提1：$\forall x ((S(x) \land A(x)) \rightarrow L(x))$
        $\Rightarrow \neg S(x) \lor \neg A(x) \lor L(x)$ (子句1)
    *   前提2：$S(Ming) \land \neg A(Ming)$
        $\Rightarrow S(Ming)$ (子句2), $\neg A(Ming)$ (子句3)
    *   前提3：$S(Hong) \land A(Hong)$
        $\Rightarrow S(Hong)$ (子句4), $A(Hong)$ (子句5)
    *   结论 $Q$：$\exists x (S(x) \land L(x))$
    *   **否定结论** $\neg Q$：$\forall x (\neg S(x) \lor \neg L(x))$ (子句6)
3.  **归结过程**：
    *   子句1 与 子句4 归结 $\{Hong/x\}$：$\neg A(Hong) \lor L(Hong)$ (子句7)
    *   子句7 与 子句5 归结：$L(Hong)$ (子句8)
    *   子句6 与 子句4 归结 $\{Hong/x\}$：$\neg L(Hong)$ (子句9)
    *   子句8 与 子句9 归结：**NIL**
    *   得证。

**2. 模糊推理**
**解**：
1.  **计算 R ($A^T \times B$)**：
    $$R_{ij} = \min(A_i, B_j)$$
    $A = [0.8, 0.5, 0.2]^T, B = [0.3, 0.9, 0.4]$
    $$R = \begin{bmatrix}
    0.8\land0.3 & 0.8\land0.9 & 0.8\land0.4 \\
    0.5\land0.3 & 0.5\land0.9 & 0.5\land0.4 \\
    0.2\land0.3 & 0.2\land0.9 & 0.2\land0.4
    \end{bmatrix} = \begin{bmatrix}
    0.3 & 0.8 & 0.4 \\
    0.3 & 0.5 & 0.4 \\
    0.2 & 0.2 & 0.2
    \end{bmatrix}$$

2.  **计算 $B'$ ($A' \circ R$)**：
    $A' = [0.9, 0.4, 0.1]$
    *   $b'_1 = (0.9\land0.3) \lor (0.4\land0.3) \lor (0.1\land0.2) = 0.3 \lor 0.3 \lor 0.1 = 0.3$
    *   $b'_2 = (0.9\land0.8) \lor (0.4\land0.5) \lor (0.1\land0.2) = 0.8 \lor 0.4 \lor 0.1 = 0.8$
    *   $b'_3 = (0.9\land0.4) \lor (0.4\land0.4) \lor (0.1\land0.2) = 0.4 \lor 0.4 \lor 0.1 = 0.4$
    *   **结果**：$B' = \frac{0.3}{v_1} + \frac{0.8}{v_2} + \frac{0.4}{v_3}$

**3. PSO计算**
**解**：
1.  **速度更新**：
    $$v_{new} = \omega v + c_1 r_1 (p - x) + c_2 r_2 (g - x)$$
    $$v_{new} = 0.8 \times 1 + 2 \times 0.5 \times (4 - 2) + 2 \times 0.2 \times (5 - 2)$$
    $$v_{new} = 0.8 + 1 \times 2 + 0.4 \times 3$$
    $$v_{new} = 0.8 + 2 + 1.2 = 4$$
2.  **位置更新**：
    $$x_{new} = x + v_{new} = 2 + 4 = 6$$
    **答案**：速度为 4，位置为 6。

**4. BP梯度推导**
**解**：
1.  **误差定义**：$E = \frac{1}{2}(d_k - y_k)^2$。
2.  **链式法则**：
    $$\frac{\partial E}{\partial w_{jk}} = \frac{\partial E}{\partial y_k} \cdot \frac{\partial y_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial w_{jk}}$$
3.  **分项计算**：
    *   $\frac{\partial E}{\partial y_k} = -(d_k - y_k)$
    *   $\frac{\partial y_k}{\partial net_k} = f'(net_k) = y_k(1-y_k)$
    *   由于 $net_k = \sum w_{jk} y_j$，故 $\frac{\partial net_k}{\partial w_{jk}} = y_j$ (隐层输出)
4.  **汇总**：
    $$\frac{\partial E}{\partial w_{jk}} = -(d_k - y_k) y_k(1-y_k) y_j$$
5.  **权值更新公式**：
    $$\Delta w_{jk} = -\eta \frac{\partial E}{\partial w_{jk}} = \eta (d_k - y_k) y_k(1-y_k) y_j$$

---

### 四、简答题

**1. A* 算法退化与失效**
*   若 $h(n)=0$，A* 退化为 **Dijkstra算法**（或BFS，若边权为1）。此时变为盲目搜索，保证最优但效率最低。
*   若 $h(n) > h^*(n)$，即启发函数过大（过度乐观估计距离），算法**失去了可采纳性 (Admissibility)**。此时算法可能还没找到最优路径就过早终止，**不能保证找到最优解**。

**2. GA vs SA**
*   **相同点**：都是随机优化算法，都能跳出局部最优。
*   **不同点**：
    1.  **搜索机制**：GA是**群体搜索**（并行），通过个体间的交叉变异交换信息；SA是**单点搜索**（串行），通过状态转移寻找新解。
    2.  **跳出局部最优**：GA靠**变异**算子；SA靠**Metropolis准则**（以概率接受差解）。
    3.  **效率**：GA通常并行效率高，搜索范围广；SA如果降温慢，能逼近全局最优，但耗时极长。

**3. CNN特征提取**
*   **深层优于浅层的原因**：
    深度神经网络提取特征具有**层次性**。
    *   **底层**网络提取低级特征（如边缘、线条、颜色）。
    *   **中层**网络组合低级特征，形成中级形状（如眼睛、轮胎）。
    *   **高层**网络将形状组合成高级语义特征（如人脸、汽车）。
    这种分层抽象的表达能力使得深层网络能更好地理解复杂数据，而浅层网络难以通过单层变换完成这种复杂的抽象。

---

### 五、设计题 (N皇后 - GA)

**1. 编码方案**：
采用**整数排列编码**。
染色体是一个长度为 $N=8$ 的数组 $P = [p_1, p_2, ..., p_8]$。
*   **含义**：$p_i$ 表示第 $i$ 行的皇后放在第 $p_i$ 列。
*   **优势**：这种编码天然满足了“每行有一个皇后”和“每列有一个皇后”（因为是 $1-8$ 的排列，数字不重复）这两个约束。只需要解决“斜线冲突”问题。

**2. 适应度函数**：
目标是冲突最少。
*   计算染色体中互相攻击的皇后对数，记为 $Collision$。
*   在排列编码下，只有斜线攻击。两个皇后 $(i, p_i)$ 和 $(j, p_j)$ 冲突，当且仅当 $|i-j| = |p_i - p_j|$。
*   **适应度**：$Fit = \frac{1}{1 + Collision}$ （或者 $C_{max} - Collision$）。冲突越少，适应度越高。

**3. 交叉算子**：
*   **普通单点交叉不适用**：因为普通交叉会破坏排列特性（例如父代1是 `12345678`，父代2是 `87654321`，若在中间切开组合，可能得到 `12344321`，出现了两个4，少了5678，不合法）。
*   **解决方案**：采用**部分匹配交叉 (PMX)** 或 **次序交叉 (OX)**。这些交叉算子专门用于排列编码，能够保证产生的子代仍然是一个 $1-8$ 的合法排列（无重复数字）。

这份**《人工智能基础》终极临考复习要点**是根据前几次的模拟题和课程核心内容提炼而成的**“干货版”**。

请在考前最后时间，拿着这份清单，在脑海中过一遍。如果某个点卡住了，立刻回去翻看之前的详细讲解。

---

# 🚀 考前 10 分钟·极速记忆清单

## 第一部分：必背公式与计算规则 (拿分核心)

这部分是**硬通货**，记错了就是送分。

### 1. 不确定性推理 (第4章)
*   **C-F 模型**：
    *   **组合证据**：AND取最小 ($\min$)，OR取最大 ($\max$)。
    *   **传递公式**：$CF(H) = CF(H, E) \times \max\{0, CF(E)\}$。（注意：证据可信度 $<0$ 时归零）。
    *   **合成公式**（同号相加减乘积，异号相加除以归一）：
        *   异号时：$CF = (CF_1 + CF_2) / (1 - \min(|CF_1|, |CF_2|))$。
*   **D-S 证据理论**：
    *   **冲突因子 $K$**：所有**空集 ($\Phi$)** 的交叉积之和。
    *   **归一化**：所有非空集合的交叉积之和，必须除以 **$(1-K)$**。
*   **模糊推理**：
    *   **矩阵合成 (Max-Min)**：先列与行取小，再结果取大。
    *   **补集**：$1 - \mu$。

### 2. 智能计算 (第6章)
*   **粒子群 (PSO) 速度更新**（背下来！）：
    $$v_{new} = \omega \cdot v + c_1 r_1 (pbest - x) + c_2 r_2 (gbest - x)$$
    *(记忆口诀：惯性 + 认知(自我) + 社会(群体))*
*   **遗传算法 (GA) 轮盘赌概率**：$P_i = f_i / \sum f$。
*   **蚁群算法 (ACO)**：
    *   转移概率取决于：信息素 $\tau$ (历史) $\times$ 启发信息 $\eta$ (贪婪/距离倒数)。
    *   信息素更新：$\tau_{new} = (1-\rho)\tau_{old} + \Delta \tau$ （$\rho$ 是挥发系数）。

### 3. 神经网络 (第8章)
*   **BP 权值更新**：$\Delta w = -\eta \frac{\partial E}{\partial w}$ （梯度下降，负梯度方向）。
*   **Hopfield 能量函数**：$E = -\frac{1}{2} \sum \sum w_{ij} x_i x_j$ （能量总是单调递减）。
*   **CNN 特征图大小**：$(Input - Kernel + 2Padding) / Stride + 1$。

---

## 第二部分：必考算法流程 (大题/设计题)

### 1. 归结原理 (第3章)
*   **口诀**：否定结论 $\to$ 并入前提 $\to$ 化子句集 $\to$ 归结 $\to$ 空子句 (NIL)。
*   **易错点**：
    *   消去 $\exists$ (Skolem化) 时，如果在 $\forall$ 后面，必须用函数 $f(x)$，不能用常量 $a$。
    *   不同子句的变量要换名。

### 2. A* 搜索 (第5章)
*   **估价函数**：$f(n) = g(n) + h(n)$。
*   **可采纳性条件**：$h(n) \le h^*(n)$ （估计代价 $\le$ 实际最小代价）。
*   **流程**：OPEN表找最小 $f$ $\to$ 放进 CLOSED $\to$ 扩展邻居 $\to$ 更新 $f$。

### 3. Alpha-Beta 剪枝 (第5章)
*   **MAX 节点**：更新 $\alpha$ (下界)。如果发现子节点值 $v \ge \beta$ (父节点的上界)，剪枝。
*   **MIN 节点**：更新 $\beta$ (上界)。如果发现子节点值 $v \le \alpha$ (父节点的下界)，剪枝。

---

## 第三部分：核心概念辨析 (选择/填空/简答)

### 1. 人工智能流派 (第1章)
*   **符号主义**：逻辑、推理、知识图谱、专家系统。（达特茅斯会议主流）
*   **连接主义**：神经网络、深度学习。（仿生学）
*   **行为主义**：感知-行动、进化计算、强化学习。

### 2. 知识表示 (第2章)
*   **一阶谓词**：$\forall$ 配 $\rightarrow$，$\exists$ 配 $\land$。
*   **框架**：核心特性是**继承**。
*   **知识图谱**：本质是语义网络，基本单元是**三元组**。

### 3. 神经网络特性 (第8章)
*   **M-P模型**：最早的神经元模型。
*   **BP网络**：前馈、有监督、梯度消失、局部极小值。
*   **Hopfield**：反馈、联想记忆、优化计算(TSP)。
*   **CNN**：**局部连接** + **权值共享** $\to$ 减少参数，平移不变性。

### 4. 遗传算法细节 (第6章)
*   **交叉**：产生新个体，全局搜索。
*   **变异**：维持多样性，防止**早熟收敛**。
*   **TSP编码**：必须用**部分匹配交叉 (PMX)** 或次序交叉，防止城市重复。

### 5. 专家系统 (第7章)
*   **核心**：**知识库**与**推理机**的分离。
*   **瓶颈**：知识获取。

---

## 🛡️ 考试避坑指南

1.  **看清题目**：是求“最大值”还是“最小值”？A*算法和遗传算法的适应度函数方向往往相反（A*求最小代价，GA求最大适应度，记得取倒数）。
2.  **计算归一化**：做D-S证据理论题，算完 $K$ 后，**千万别忘了**用 $1/(1-K)$ 进行归一化，否则全错。
3.  **符号正负**：C-F模型中，证据不可信 ($CF < 0$) 时，规则是不触发的（贡献为0），不要乘出一个负数去抵消别人的正数。
4.  **术语准确**：简答题尽量用专业术语，如“泛化能力”、“鲁棒性”、“梯度消失”、“完备性”、“可采纳性”。

---

**最后祝愿：**
你现在的知识储备已经足以应对这张试卷。保持冷静，细心计算，逻辑清晰。
**祝考试顺利，高分拿下！** 💯